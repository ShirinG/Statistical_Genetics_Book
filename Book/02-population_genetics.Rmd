
# Evolutionary and Population Genetics

Evolutionary and population genetics study processes like adapation and speciation events from a genetic perspective. This means that they are interested in how genes and alleles are transmitted between parents and offspring (or in some cases even horizontally), how they change and adapt over time , how dominance, epistasis and epigenetics influence phenotypes and how this relate to population structure. 

The basic concept of evolutionary biology is that in order for selection to lead to change in populations, there needs to be sufficient genetic and phenotypic variation in that population to select from. From a genetics perspective, this means that selection will favor certain alleles among the pool of genes in the population. Favorable alleles will therefore increase in frequency within the population over time, while disadvantageous alleles will eventually disappear. Depending on how stable the environment is, this change in allele frequencies can be subject to severe fluctuation.

Additional aspects like migration, mutation, recombination, inbreeding, drift, etc. make modeling of evolutionary and population genetic processes non-trivial.

https://en.wikipedia.org/wiki/Population_genetics

- Mendelian and population genetics;

This primer provides a concise introduction to conducting applied analyses of population genetic data in R, with a special emphasis on non-model populations including clonal or partially clonal organisms. It is not meant to be a textbook on population genetics. Quite to the contrary, we refer the reader to several textbooks on population genetics (Templeton, 2006; Hartl & Clark, 2007; Nielsen & Slatkin, 2013). Likewise, this book will not replace books on theory and statistics of population genetics (Weir, 1996). The reader is thus expected to have a basic understanding of population genetic theory and applications. Finally, this primer is focused on traditional population genetics based on allele frequencies, rather than more sophisticated coalescent approaches (Hein, Schierup & Wiuf, 2004; Wakeley, 2009; Nielsen & Slatkin, 2013), although some of the material covered here will apply. In a nutshell, this primer provides a valuable resource for tackling the nitty-gritty analysis of populations that do not necessarily conform to textbook genetics and might or might not be in Hardy-Weinberg equilibrium and panmixia.

This primer is geared towards biologists interested in analyzing their populations. This primer does not require extensive knowledge of programming in R, but the user is expected to install R and all packages required for this primer.

Why use R?
Until recently, one of the more tedious aspects of conducting a population genetic analysis was the need for repeated reformatting data to conduct different, complimentary analyses in different programs. Often, these programs only ran on one platform. Now, R provides a toolbox with its packages that allows analysis of most data conveniently without tedious reformatting on all major computing platforms including Microsoft Windows, Linux, and Apple’s OS X. R is an open source statistical programming and graphing language that includes tools for statistical, population genetic, genomic, phylogenetic, and comparative genomic analyses. This primer is for those of us that want to conduct applied analyses of populations and make full use of the palette of tools available in R including for example the R markdown code for this primer.

Note that the R user community is very active and that both R and its packages are regularly updated, critically modified, and noted as deprecated (no longer updated) as appropriate.

Any R user needs to make sure all components are up-to-date and that versions are compatible.
Population genetics
Traditional population genetics is based on analysis of observed allele frequencies compared to frequencies expected, assuming a population genetic model. For example, under a Wright-Fisher model you might expect to see populations of diploid individuals that reproduce sexually, with non-overlapping generations. This model ignores effects such as mutation, recombination, selection or changes in population size or structure. More complex models can incorporate different aspects of effects observed in real populations. However, most of these models assume that populations reproduce sexually.

Here, we briefly review some terminology but assume that the reader has a basic understanding of theory and applications of population genetics.

A locus is a position in the genome where we can observe one or several alleles in different individuals. Loci used in population genetics are assumed to be selectively neutral and can be an anonymous or non-coding region such as a microsatellite locus (SSR), a single nucleotide polymorphism (SNP) or the presence/absence of a band on a gel. A genotype is the combination of alleles carried by a given individual at a particular set of loci. Individuals carrying the same set of alleles are considered to have the same multilocus genotype MLGMLG.

Basic metrics of interest in populations are polymorphisms, allele frequencies and genotype frequencies. Polymorphism can be estimated in several ways, such as the total number of loci observed that have more than one allele. The frequency of an allele is calculated as the number of allele copies observed in a population divided by the total number of individuals, times the ploidy (NN in haploids or 2N2N in diploids) in the population. For diploids we would observe frequency fAfA and fafa for alleles AA and aa, respectively:

fA=NA2NfA=NA2N and fa=Na2Nfa=Na2N

where NANA are the numbers of allele AA segregating in the population genotyped.

By definition, at a biallelic locus frequencies sum to 1:

fA+fa=1fA+fa=1.

Genotype frequencies are the relative frequencies of each MLGMLG observed in a population. Thus, for diploids at a biallelic locus we can observe three genotypes: AAAA, AaAa, and aaaa and their respective frequencies:

fAA=NAA2NfAA=NAA2N and fAa=NAa2NfAa=NAa2N and faa=Naa2Nfaa=Naa2N

These frequencies again add up to 1. In diploid organisms we can also calculate the frequency of homozygotes (AAAA or aaaa) or heterozygotes (AaAa) as the proportion of individuals falling into each class. The proportion of individuals that are heterozygous is given by fAafAa while the homozygous proportion is given by 1−fAa=fAA+faa1−fAa=fAA+faa.

An important aspect of population structure is the heterozygosity observed within subdivided populations HSHS and across total populations HTHT. If we assume presence of two populations in Hardy-Weinberg equilibrium then frequencies of Allele AA in populations 1 and 2 pooled over both populations would be:

fA=2N1fA1+2N2fA22N1+2N2fA=2N1fA1+2N2fA22N1+2N2

where each allele frequency is multiplied by population size N1N1 and N2N2 and divided by total number of alleles 2N1+2N22N1+2N2. Remember, that in a diploid population there are 2 alleles per locus per individual and hence every frequency is multiplied by 2.

Genetic differentiation of two populations (e.g. subdivision of populations) occurs if we observe differences in polymorphism, allele frequencies, genotype frequencies, and heterozygosity. Wright’s FstFst is a commonly used measure of differences in heterozygosity observed in the overall population relative to the subdivided populations and is defined as the differences between HTHT and HSHS relative to HTHT:

FST=HT−HSHTFST=HT−HSHT

If allele frequencies are identical in both subpopulations then HT=HSHT=HS. If, on the other hand, allele frequencies vary between subdivided populations, then HT>HSHT>HS and populations are considered to be genetically differentiated.

Besides calculation of FstFst other approaches can be used to assess population structure including clustering. These approaches will be explored in subsequent chapters.

The special case of clonal organisms
Plants and microorganisms such as fungi, oomycetes, and bacteria often reproduce clonally or follow a mixed reproductive system, including various degrees of clonality and sexuality (Halkett, Simon & Balloux, 2005). Traditional population genetic theory does not apply and these populations violate basic assumptions in the corresponding analyses. Thus, a different set of tools are required for analyses of clonal populations (Anderson & Kohn, 1995; Milgroom, 1996; Balloux, Lehmann & Meeûs, 2003; Halkett et al., 2005; De Meeûs, Lehmann & Balloux, 2006; Grünwald & Goss, 2011). Typically these focus on metrics that are model free and do not assume random mating or random sampling of alleles. Model free metrics suitable for analyzing these populations include measures of genotypic diversity or evenness, as well as cluster analysis based on genetic distance. Additional useful measures include indices of linkage disequiblibrium among markers that are used to infer if populations are reproducing sexually or clonally. We will explore these analyses bit by bit throughout this primer. The poppr R package was specifically developed to facilitate analyses of clonal populations (Kamvar, Tabima & Grünwald, 2014; Kamvar, Brooks & Grünwald, 2015).

Marker systems, ploidy and other kinds of data
Population genetic data come in many shapes and forms and a good analysis needs to be tailored to the marker system, ploidy used and corresponding genetic models assumed (Grünwald & Goss, 2011). Organisms can be haploid, diploid, with or without known phase, or polyploid. In the case of diploid species, marker systems can be dominant (AFLP, RFLP or RAPD data) or co- dominant (SSR, SNPs, allozymes) (Grünwald et al., 2003). A locus can have two alleles (0/1 for AFLP; C/T for SNPs) or many alleles per locus (A/B/C…N for SSRs or allozymes). In each case the data has to be coded and analyzed differently. Mutation rates can also differ for different markers systems (SSR >> mitochondrial SNPs > nuclear SNPs) (Grünwald & Goss, 2011). All these aspects of a particular data set need to be considered in the data analyses.

Applications of population genetic analyses
Population genetic analyses are tremendously valuable for answering questions ranging from applied to basic evolutionary questions (Grünwald & Goss, 2011). For example, a typical concern when finding a new, invasive organism is whether it is introduced to the area or emerged from a resident population. Other questions of interest might include:

Where is the center of origin?
Does this organism reproduce sexually?
Has the population gone through a genetic bottleneck?
Are populations structured by region, geography, micro-environment?
What are source and sink populations and what are the rates of migration?
This primer will provide some of the basic tools needed to answer many of these questions for populations that are clonal or partially clonal. Note, however, that more powerful, complementary coalescent methods will not be covered here. We hope you enjoy following along.

References
Anderson JB., Kohn LM. 1995. Clonality in soilborne, plant-pathogenic fungi. Annual Review of Phytopathology 33:369–391. Available at: http://www.annualreviews.org/doi/abs/10.1146/annurev.py.33.090195.002101

Balloux F., Lehmann L., Meeûs T de. 2003. The population genetics of clonal and partially clonal diploids. Genetics 164:1635–1644. Available at: http://www.genetics.org/content/164/4/1635.full

De Meeûs T., Lehmann L., Balloux F. 2006. Molecular epidemiology of clonal diploids: A quick overview and a short dIY (do it yourself) notice. Infection, Genetics and Evolution 6:163–170. Available at: http://www.ncbi.nlm.nih.gov/pubmed/16290062

Grünwald NJ., Goodwin SB., Milgroom MG., Fry WE. 2003. Analysis of genotypic diversity data for populations of microorganisms. Phytopathology 93:738–746. Available at: http://apsjournals.apsnet.org/doi/abs/10.1094/PHYTO.2003.93.6.738

Grünwald NJ., Goss EM. 2011. Evolution and population genetics of exotic and re-emerging pathogens: Novel tools and approaches. Annual Review of Phytopathology 49:249–267. Available at: http://www.annualreviews.org/doi/abs/10.1146/annurev-phyto-072910-095246?journalCode=phyto

Halkett F., Simon J-C., Balloux F. 2005. Tackling the population genetics of clonal and partially clonal organisms. Trends in Ecology & Evolution 20:194–201. Available at: http://www.ncbi.nlm.nih.gov/pubmed/16701368

Hartl D., Clark A. 2007. Principles of population genetics. Sinauer Associates, Incorporated. Available at: http://books.google.com/books?id=SB1vQgAACAAJ

Hein J., Schierup M., Wiuf C. 2004. Gene genealogies, variation and evolution: A primer in coalescent theory. Oxford University Press, USA. Available at: http://books.google.com/books?id=QBC\_SFOamksC

Kamvar ZN., Brooks JC., Grünwald NJ. 2015. Novel R tools for analysis of genome-wide population genetic data with emphasis on clonality. Name: Frontiers in Genetics 6:208. Available at: http://dx.doi.org/10.3389/fgene.2015.00208

Kamvar ZN., Tabima JF., Grünwald NJ. 2014. PopprPoppr: An R package for genetic analysis of populations with clonal, partially clonal, and/or sexual reproduction. PeerJ 2:e281. Available at: http://dx.doi.org/10.7717/peerj.281

Milgroom MG. 1996. Recombination and the multilocus structure of fungal populations. Annual Review of Phytopathology 34:457–477. Available at: http://www.annualreviews.org/doi/abs/10.1146/annurev.phyto.34.1.457

Nielsen R., Slatkin M. 2013. An introduction to population genetics: Theory and applications. Sinauer Associates, Incorporated. Available at: http://books.google.com/books?id=Iy08kgEACAAJ

Templeton A. 2006. Population genetics and microevolutionary theory. Wiley. Available at: http://books.google.com/books?id=tJVreQjInt0C

Wakeley J. 2009. Coalescent theory: An introduction. Roberts & Company Publishers. Available at: http://books.google.com/books?id=x30RAgAACAAJ

Weir B. 1996. Genetic data analysis 2. Sinauer Associates. Available at: http://books.google.com/books?id=e9QPAQAAMAAJ

Data sets used in this primer
AFLP (diploid, clonal): Aeut
This data is for a population of the diploid plant pathogen Aphanomyces euteiches using AFLP as a marker system (Grünwald & Hoheisel, 2006). A total of 187 pathogen isolates were sampled hierarchically in two regions in Oregon (N = 97) and Washington (N = 90). The alleles observed were treated as dominant markers with presence or absence. Analysis included clone-correction, genotypic diversity, linkage disequilibrium using the index of association, dendrograms based on Nei’s genetic distance, and AMOVA.

Microsatellite (haploid, clonal): monpop
This is microsatellite data for a population of the haploid plant pathogen Monilinia fructicola that causes disease within peach tree canopies (Everhart & Scherm, 2015). Entire populations within trees were sampled across 3 years (2009, 2010, and 2011) in a total of four trees, where one tree was sampled in all three years, for a total of 6 within-tree populations. Within each year, samples in the spring were taken from affected blossoms (termed “BB” for blossom blight) and in late summer from affected fruits (termed “FR” for fruit rot). There are a total of 694 isolates with 65 to 173 isolates within each canopy population that were characterized using a set of 13 microsatellite markers.

Microsatellite (diploid, sexual): microbov
This data set contains 704 bovine samples over 30 microsatellite loci for 15 breeds, 2 species and 2 countries. From (Laloe et al., 2007).

Microsatellite (diploid, sexual): nancycats
This is a data set of 217 cats Felis catus L. from Nancy, France genotyped over 9 microsatellite loci. They have been divided into 17 populations with spatial coordinates in the @other slot. It comes from the adegenet package without a published reference. Likely, the source paper is here.

Microsatellite (triploid, sexual/clonal): Pinf
This microsatellite data comes from a larger data set of populations of Phytophthora infestans (Goss et al., 2014). It contains 86 individuals representing 72 multilocus genotypes that have been genotyped over 11 loci. They are grouped by continent and country. This data set is used to demonstrate clone correction and linkage disequilibrium.

Microsatellite (diploid, clonal): Pram
This data set contains populations of Phytophthora ramorum from Nurseries in California and Oregon (Goss et al., 2009) and Forests in Curry County, Oregon from 2001 to 2014 (Kamvar et al., 2015). There are 729 samples representing 98 multilocus genotypes genotyped over 5 microsatellite loci.

SNP (haploid, clonal): H3N2
This is a SNP data set from the hemaglutinin segment of the H3N2 strain of seasonal influenza contained within the adegenet package. It contains 1902 samples genotyped over 125 SNP markers. It contains a wealth of information in the @other slot including year isolated, month isolated, country of origin, and more. This data set is utilized to demonstrate the DAPC function in adegenet.

GBS (diploid): Prubi_gbs
We will use a data set of 94 samples of the red raspberry pathogen Phytophthora rubi (Tabima et al., In Prep). This pathogen is diploid and a fungal like Oomycete. Populations were obtained by sampling individual pathogen strains from roots of infected red raspberry in the states of California (CA), Oregon (OR), and Washington (WA). A total of 94 samples of P. rubi were sequenced using the Illumina HiSeq 3000 technology with 150 bp paired end reads and a target insert size of 500 bp. Currently, there is little information about the population structure of P. rubi in the western USA. We are interested in studying the population structure of P. rubi populations in the western US. The VCF data for this population can be downloaded from: prubi_gbs.VCF.gz.

Genomic (diploid): pinfsc50
For genomics examples we’ll use the pinfsc50 dataset. The pinfsc50 dataset is from a number of published P. infestans genomics projects where the data has been subset here to supercontig_1.50. This dataset is available as a stand alone R package (Knaus & Grünwald, 2017). By subsetting the data to one supercontig it creates a dataset of a size that can be conveniently used for examples. This dataset illustrates some important strengths and weaknesses of these studies. A strength is the amount of data we have for each individual. Among the weaknesses are that the samples are ‘opportunistic’ in that we have no control over the design of the experiment. Also, because of the large investment in data per sample, there is a relatively small number of samples.

References
Everhart S., Scherm H. 2015. Fine-scale genetic structure of Monilinia fructicola during brown rot epidemics within individual peach tree canopies. Phytopathology 105:542–549. Available at: https://doi.org/10.1094/PHYTO-03-14-0088-R

Goss EM., Larsen M., Chastagner GA., Givens DR., Grünwald NJ. 2009. Population genetic analysis infers migration pathways of phytophthora ramorum in uS nurseries. PLoS Pathog 5:e1000583. Available at: http://dx.doi.org/10.1371/journal.ppat.1000583

Goss EM., Tabima JF., Cooke DEL., Restrepo S., Fry WE., Forbes GA., Fieland VJ., Cardenas M., Grünwald NJ. 2014. The Irish potato famine pathogen Phytophthora infestans originated in central mexico rather than the andes. Proceedings of the National Academy of Sciences 111:8791–8796. Available at: http://www.pnas.org/content/early/2014/05/29/1401884111.abstract

Grünwald NJ., Hoheisel G-A. 2006. Hierarchical analysis of diversity, selfing, and genetic differentiation in populations of the oomycete Aphanomyces euteiches. Phytopathology 96:1134–1141. Available at: http://apsjournals.apsnet.org/doi/abs/10.1094/PHYTO-96-1134

Kamvar Z., Larsen M., Kanaskie A., Hansen E., Grünwald N. 2015. Spatial and temporal analysis of populations of the sudden oak death pathogen in oregon forests. Phytopathology 105:982–989. Available at: http://dx.doi.org/10.1094/PHYTO-12-14-0350-FI

Knaus BJ., Grünwald NJ. 2017. VcfrVcfr: A package to manipulate and visualize variant call format data in R. Molecular Ecology Resources 17:44–53. Available at: http://dx.doi.org/10.1111/1755-0998.12549

Laloe D., Jombart T., Dufour A-B., Moazami-Goudarzi K. 2007. Consensus genetic structuring and typological value of markers using multiple co-inertia analysis. Genetics Selection Evolution 39:545–567. Available at: http://dx.doi.org/10.1051/gse:2007021

Tabima JF., Zasada I., Coffey M., Grünwald NJ. In Prep. Population dynamics of Phytopthora rubi indicate high rates of migration between states and nurseries in the pacific north western United States. In prep.

```{r}
pkg = c("poppr", "treemap", "magrittr", "pegas", "lattice")
sapply(pkg, function(x) {if (system.file(package = x) == '') install.packages(x)})
```

You can then load the package:

```{r message=FALSE, warning=FALSE}
library(poppr)
```

This section will briefly go over the basics of data import into poppr. For this section, we will focus on the GenAlEx format. Other formats are supported and details are given in the R help page for the adegenet function import2genind. We will show examples of haploid, diploid, and polyploid data sets and show you how you can format your data if it’s grouped into multiple stratifications.

To import GenAlEx formatted data into poppr, you should use the function read.genalex. Below is an example using monpop.csv.

```{r}
monpop <- read.genalex("datasets/monpop.csv")
monpop
```

Other data formats
Given dependence of poppr on adegenet, users can import data from the following formats

FSTAT (file.dat)
GENETIX (file.gtx)
GENEPOP (file.gen)
STRUCTURE (file.str)
The adegenet function import2genind will import all of these formats. If you have sequence data, you can use the read.FASTA function from the ape package. If your data is in any other format, type help("df2genind") for guidance.

GenAlEx data format
GenAlEx is a very popular add-on for Microsoft Excel. It is relatively easy to use because of its familiar, menu-driven interface. It also gives the user the option to include information on population groupings, regional groupings, and xy coordinates. The flexibility of this format made it a clear choice for import into poppr.

The data format is standard in that individuals are defined in the rows and loci are defined in the columns. The first two rows are reserved for metadata and the first two columns are reserved for the individual names and population names, respectively. The examples we will be using include haploid, diploid and polyploid data.

Basic Format
Below is what the monpop (haploid) data looks like. Highlighted in red is how missing data should be coded for SSR markers. Highlighted in blue are the parts of the metadata rows used by poppr. These three numbers represent:

The columns of the metadata beyond those three rows define the number of individuals contained within each population. Since this data is redundant with the second column, it is not necessary. Notice, also, that the second column, reserved for the population assignments, has a pattern of underscores in the populations. This will be important at the end of this section. Below is a modified version of the input format that should make it easier to format.

Highlighted in blue is the cell that defines the number of columns highlighted in red. If we set this number to 1, then we do not have to enter in any information in those columns. Try it for yourself.

Diploids
Diploid data is only different in the fact that you will have two alleles per locus. This is coded such that each allele is in a separate column. Below is an example of the nancycats data set (from the adegenet package), exported like above. Highlighted in blue and red are the first and second loci, respectively.

Polyploids
GenAlEx does not handle polyploids, but since poppr can do it, we have set up a scheme to allow import of polyploids via this format. The limitation is that all of your loci have to have the same observed ploidy. Below is the example of Phytophthora infestans in the data set Pinf where some genotypes had observed tetraploid loci (Goss et al., 2014).

Highlighted in blue is the first locus and highlighted in red are two samples at that locus, an observed diploid and observed triploid. Note the extra zeroes needed to make the genotype tetraploid.

Population strata
A hierarchical sampling approach is necessary to infer structure of populations in space or time. Poppr facilitates definition of stratified data by concatenating the different stratifications into a single column by a common separator (“_” by default). Here’s an example of the three stratifications of the monpop data set introduced above:

```{r}
splitStrata(monpop) <- ~Tree/Year/Symptom
monpop # After (Three distinct levels)
```

References
Everhart S., Scherm H. 2015. Fine-scale genetic structure of Monilinia fructicola during brown rot epidemics within individual peach tree canopies. Phytopathology 105:542–549. Available at: https://doi.org/10.1094/PHYTO-03-14-0088-R

Goss EM., Tabima JF., Cooke DEL., Restrepo S., Fry WE., Forbes GA., Fieland VJ., Cardenas M., Grünwald NJ. 2014. The Irish potato famine pathogen Phytophthora infestans originated in central mexico rather than the andes. Proceedings of the National Academy of Sciences 111:8791–8796. Available at: http://www.pnas.org/content/early/2014/05/29/1401884111.abstract

Populations are best sampled hierarchically on a range of scales from subpopulations (e.g. fields, valleys, ranges) to regions (e.g. valleys, states, countries or continents) or across time (years or decades). This approach is useful because population structure and evolutionary processes may not be discernible a priori. Most of the times we do not know if population are differentiated spatially or temporally. Thus, a combination of targeted local sampling with sampling over larger spatial or temporal scales is necessary to detect population structure over different scales, without using intense sampling throughout the entire range.

The methods implemented in poppr allow specification of which strata you want to analyze. This is a rapid way of working with subsets of your data without having to perform any data manipulation or changing the input file. In this tutorial, we will show you how to define the hierarchical structure of your data and how to specify specific levels that you might want to analyze.

Data used in this example
For this example, we will use the monpop data set (Everhart & Scherm, 2015). This microsatellite data consists of 13 loci for 694 individuals of the haploid fungal pathogen Monilinia fructicola that infects peach flowers and fruits in commercial orchards. The monpop population came from four trees within a single orchard (trees 26, 45, and 7). Each tree was sampled in 2009, 2010, and/or 2011. Additionally, each sample was noted as to whether it came from a blossom or a fruit. This example data set is included with the poppr package.

Working with stratified data
The steps for working with stratified data include:

Import data set with samples labeled according to strata
Define the stratifications for the data
Setting the stratification(s) that you want to have analyzed
Importing data labeled according to a stratum
The easiest way to work with stratified data is to label each sample using an underscore “_” to separate each level. This was already done for the monpop data, where each sample was coded hierarchically by tree, year, and symptom in the following format: “Tree_Year_Symptom”.

Let’s load the hierarchically labeled example data:

Genotype information shows us that the data contains 264 multilocus genotypes among 694 haploid individuals with 13 loci. Population information has two items, the stratifications and the populations defined. You can think of stratifications as the index names for each of the hierarchical levels within your data (so for our data it should be Tree, Year, and Symptom). By default, however, no stratifications are defined and so this is “Pop”, which is the entire dataset of 694 individuals. Because we labeled each sample according to stratification, populations defined shows us our data has 12 groups defined: 7_09_BB, 26_09_BB, 26_09_FR, 7_09_FR, 26_10_BB, 45_10_BB, 79_10_BB, 79_10_FR, 26_10_FR, 45_10_FR, 26_11_BB, and 26_11_FR.

Assigning stratifications
We imported the data that has three stratifications “Tree_Year_Symptom”. In order to analyze our data according to any combination of those three stratifications, we need to tell poppr that the 12 groups should be split by tree, year, and/or symptom. Thus, the first step is to split our data according to strata so that we can access each of the three hierarchical levels in the data. The splitStrata command is used to index the three stratifications:

After splitting the data populations are specified by stratification: “Tree Year Symptom”.

We can look at how the stratifications are distributed by using a treemap plot. This is a plot that allows us to visualize hierarchical stratifications. The function we’ll use is called treemap() from the treemap package.

The treemap() function needs only a data frame containing the strata and their respective counts. This can easily be done with the dplyr package where we will:

Group our strata by Tree, Year, and Symptom
Summarize the data by counting how many times we see each specific combination

```{r}
library(dplyr)
monstrata <- strata(monpop) %>%     
  group_by(Tree, Year, Symptom) %>%
  summarize(Count = n())

monstrata
```

Now we can use the treemap() function to plot the data. Note that it has a lot of arguments to allow you to properly manipulate the graphic, but we will only use the very necessary components to visualize the distribution of the strata:

dtf - The data frame containing the stratifications and counts
index - The variables used for nesting (in order)
vSize - The variable to use to compute the size of the blocks
All the other arguments we are using here give various aesthetics. If you want to know more about how they work, you can peruse the manual for the treemap function by typing help("treemap", "treemap").

```{r}
library(treemap)
nameStrata(monpop) # The order of our variables
monstrata$Count    # The variable used for the block size
```

```{r}
# Adjusting the aesthetics for the labels
label_position <- list(c("center", "top"), c("center", "center"), c("center", "bottom"))
label_size     <- c(Tree = 0, Year = 15, Symptom = 15)

# Plotting, First three arguments are necessary.
treemap(dtf = monstrata, index = nameStrata(monpop), vSize = "Count",
        type = "categorical", vColor = "Tree", title = "M. fructicola",
        align.labels = label_position, fontsize.labels = label_size)
```

```{r eval=FALSE}
itreemap(dtf = monstrata, index = nameStrata(monpop), vSize = "Count",
        type = "categorical", vColor = "Tree")
```

Next, we analyze the data according to Tree and Year:

```{r}
setPop(monpop) <- ~Tree/Year
monpop
```

To analyze the data according to Symptom:

```{r}
setPop(monpop) <- ~Symptom
monpop
```

Order of the levels that you define is important, so if we wanted to define the symptoms according to tree, we would use the following:

```{r}
setPop(monpop) <- ~Symptom/Tree
monpop
```

Now that we have laid out the basics of manipulating data by strata, we will now apply strata for clone correction.

Clone correction
When dealing with clonal populations, analyses are typically conducted with and without clone correction. Clone correction is a method of censoring a data set such that only one individual per MLG is represented per population (Milgroom, 1996; Grünwald et al., 2003; Grünwald & Hoheisel, 2006). This technique is commonly used with the index of association and genotypic diversity measures since clone corrected populations approximate behavior of sexual populations. Since we want to only observe unique genotypes per population, clone correction requires specification of the stratifications at which clones should be censored. This section will show how to clone correct at a specific stratification and also compare the results with uncorrected data.

Question: Will allelic diversity increase or decrease with clone-censored data?
Using monpop as an example, if we wanted to know the diversity of alleles within each tree per year, how should we go about correcting for the clones? We use the function clonecorrect specifying the “Tree/Year” strata:

```{r}
mcc_TY <- clonecorrect(monpop, strata = ~Tree/Year, keep = 1:2)
mcc_TY
```

Notice that the number of samples reduced from 694 to 278, but is still more than the number of MLGs. This indicates that there are duplicated genotypes that cross trees and years, but that’s okay because of our definition of a clone-corrected data set as having one representative genotype per population. Before we continue, we should set the original data to the same strata:

```{r}
setPop(monpop) <- ~Tree/Year
```

Now we can compare the diversity of alleles at each locus using Simpson’s index (1−D1−D) as implemented in the function locus_table (Detailed in our chapter on locus based statistics). We will do this in three steps:

Calculate diversity of the clone corrected data
Calculate diversity of the uncorrected data
Take the difference of step 1 from step 2.

```{r}
cc <- locus_table(mcc_TY, info = FALSE)
mp <- locus_table(monpop, info = FALSE)
mp - cc
```

```{r}
locus_diff <- mp - cc

# Note that I need to select the column containing Simpson's Index. That's
# labeled as "1-D".
barplot(locus_diff[, "1-D"], ylab = "Change in Simpson's Index", xlab = "Locus",
        main = "Comparison of clone-corrected vs. uncorrected data")
```

We can see quite a difference in some loci after clone correcting based on tree in the overall data set showing that, while some loci show a decrease, many loci show an increase in allelic diversity after clone-correction.

Advanced R: writing your own functions for analysis by stratum
Of course, we still want to analyze each tree/year combination separately. Instead of typing those commands above for each combination, we can write a function to do it for us.

A function can be thought of as a set of instructions that tells the computer what to do. You have been using functions before such as poppr(). A function is written like this:

Writing a function for comparing clone corrected and uncorrected data

We will simply take the steps from above and turn them into a function to calculate the difference in Simpson’s diversity for a given population. In order to do that, we will need to compute three steps:

the name of the population
the clone-corrected data set
the uncorrected data set
From there, we can construct the function.

```{r}
plot_simp_diff <- function(pop_name, clone_corrected, un_corrected){

  # Step 1: calculate diversity for clone-corrected data
  cc <- locus_table(clone_corrected, pop = pop_name, info = FALSE)

  # Step 2: calculate diversity for uncorrected data
  uc <- locus_table(un_corrected, pop = pop_name, info = FALSE)

  # Step 3: Take the difference
  res <- uc - cc

  # Step 4: Plot Simpson's index.
  barplot(res[, "1-D"], main = pop_name, ylab = "Change in Simpson's Index", xlab = "Locus")
}
```

```{r}
par(mfrow = c(2, 3)) # Set up the graphics to have two rows and three columns

for (i in popNames(monpop)){
  plot_simp_diff(i, mcc_TY, monpop)
}
par(mfrow = c(1, 1)) # Next we reset the graphics to have one row and one column
```

These barplots show the difference in Simpson’s index of original minus clone corrected data for each population per locus. We can see that allelic diversity generally is lower in the total data set (containing some repeated MLGs) relative to clone corrected data.

Conclusions
This was a brief introduction to the easiest way to create stratifications and apply them in poppr to more rapidly analyze your data. By indexing the stratifications of your data, you can set the stratification(s) you want to have analyzed in a single command. This approach avoids having to create new sub-sets of the data for each analysis and simultaneously reduces the chance of error when manipulating data sets by hand.

References
Everhart S., Scherm H. 2015. Fine-scale genetic structure of Monilinia fructicola during brown rot epidemics within individual peach tree canopies. Phytopathology 105:542–549. Available at: https://doi.org/10.1094/PHYTO-03-14-0088-R

Grünwald NJ., Goodwin SB., Milgroom MG., Fry WE. 2003. Analysis of genotypic diversity data for populations of microorganisms. Phytopathology 93:738–746. Available at: http://apsjournals.apsnet.org/doi/abs/10.1094/PHYTO.2003.93.6.738

Grünwald NJ., Hoheisel G-A. 2006. Hierarchical analysis of diversity, selfing, and genetic differentiation in populations of the oomycete Aphanomyces euteiches. Phytopathology 96:1134–1141. Available at: http://apsjournals.apsnet.org/doi/abs/10.1094/PHYTO-96-1134

Milgroom MG. 1996. Recombination and the multilocus structure of fungal populations. Annual Review of Phytopathology 34:457–477. Available at: http://www.annualreviews.org/doi/abs/10.1146/annurev.phyto.34.1.457

Nielsen R., Slatkin M. 2013. An introduction to population genetics: Theory and applications. Sinauer Associates, Incorporated. Available at: http://books.google.com/books?id=Iy08kgEACAAJ

Locus stats, heterozygosity, HWE
ZN Kamvar, SE Everhart, and NJ Grünwald
A rigorous population genetic analysis looks closely at the data to assess quality and identify outliers or problems in the data such as erroneous allele calls. This chapter focuses on analysis on a per-locus level. While there are statistics that analyze populations across loci, it is important to analyze each locus independently to make sure that one locus is not introducing bias or spurious errors into the analysis.

Note: Many of these statistics are specific to co-dominant data.
Locus summary statistics
A quick way to assess quality of the data is to determine the number, diversity, expected heterozygosity, and evenness of the alleles at each locus. As an example, we will use data for the fungal-like protist Phytophthora infestans from (Goss et al., 2014). First, we’ll use the function  locus_table to get all of the statistics mentioned above. For documentation on this function type ?locus_table. Here is a first look at each locus:

```{r}
library(magrittr)  # We will also use magrittr for part of this chapter
data(Pinf)         # P. infestans data set from Mexico and South America
locus_table(Pinf)
```

We can see here that we have a widely variable number of alleles per locus and that we actually have a single locus that only has two alleles, Pi33. This locus also has low diversity, low expected heterozygosity and is very uneven in allele distribution. This is a sign that this might be a phylogenetically uninformative locus, where we have two alleles and one is occurring at a minor frequency. We will explore analysis with and without this locus. Let’s first see if both of these alleles exist in both populations of this data set.

```{r}
locus_table(Pinf, pop = "North America")
locus_table(Pinf, pop = "South America")
```

Phylogenetically uninformative loci
We can see that the South American populations is fixed for one allele, thus it would not be a bad idea to remove that locus from downstream analyses. We can do this using the function informloci. This will remove loci that contain less than a given percentage of divergent individuals (the default is 2/N2/N, where NN equals the number of individuals in the data set).

```{r}
nLoc(Pinf)  # Let's look at our data set, note how many loci we have.
iPinf <- informloci(Pinf)
nLoc(iPinf) # Note that we have 1 less locus
poppr(Pinf)
poppr(iPinf)
```

We can see that it increased ever so slightly for the “North America” and “Total” populations, but not the “South America” population as expected given the fixed alleles at locus P33.

Missing data
It is often important to asses the percentage of missing data. The poppr function info_table will help you visualize missing data so that you can assess how to treat these further using the function missingno. For this example, we will use the nancycats data set as it contains a wide variety of possibilities for missing data:

```{r}
data(nancycats)
info_table(nancycats, plot = TRUE)
```

Here we see a few things. The data set has an average of 2.34% missing data overall. More alarming, perhaps is the fact that population 17 has not been genotyped at locus fca45 at all and that locus fca8 shows missing data across many populations. Many analyses in poppr can be performed with missing data in place as it will be either considered an extra allele in the case of MLG calculations or will be interpolated to not contribute to the distance measure used for the index of association. If you want to specifically treat missing data, you can use the function missingno to remove loci or individuals, or replace missing data with zeroes or the average values of the locus.

Removing loci and genotypes
When removing loci or genotypes, you can specify a cutoff representing the percent missing to be removed. The default is 0.05 (5%).

```{r}
nancycats %>% missingno("loci") %>% info_table(plot = TRUE, scale = FALSE)
```

Advanced Users: when scale = TRUE, the color scale will be set so that the warmest color corresponds to the highest value.
We only removed two loci. If we wanted to make sure we removed everything, we could set cutoff = 0.

```{r}
miss <- nancycats %>% missingno("loci", cutoff = 0) %>% info_table(plot = TRUE)
```

Again, removing individuals is also relatively easy:

```{r}
miss <- nancycats %>% missingno("geno") %>% info_table(plot = TRUE)
```

```{r}
miss <- nancycats %>% missingno("geno", cutoff = 0) %>% info_table(plot = TRUE)
```

The function missingno removes individuals based on the percent of missing data relative to the number of loci. Let’s remove all individuals with 2 missing loci:

```{r}
miss <- nancycats %>%
  missingno("geno", cutoff = 2/nLoc(nancycats)) %>%
  info_table(plot = TRUE)
```

We only found one individual in population 11.

Hardy-Weinberg equilibrium
Next, let’s determine if our populations are in Hardy-Weinberg equilibrium. We will again use the nancycats data to test for HWE using the function  hw.test() from the pegas package. This will compute the χ2χ2 statistic over the entire data set and compute two P-values, one analytical and one derived from permutations:

```{r}
library("pegas")
(nanhwe.full <- hw.test(nancycats, B = 1000)) # performs 1000 permuatations
```

We can see here that both the analytical p-value and permuted p-value show that we have confidence that all loci are not under the null expectation of HWE. This makes sense given that these data represent 17 populations of cats. If we wanted to check what the HWE statistic for each population is, we should first separate the populations with the function seppop(). For this exercise, we will only focus on the analytical p-value by setting B = 0.

```{r}
(nanhwe.pop <- seppop(nancycats) %>% lapply(hw.test, B = 0))
```

Advanced: Visualization of population-wise p-values
Now we have one matrix per sample, but all we care about are the p-values, which are in the third column. We can use the functions sapply and  [ to loop to create a matrix that only contains populations in columns and loci in rows.

[ is used to subset data, and it’s also a function! It provides an easy way to subset a list of data.

```{r}
(nanhwe.mat <- sapply(nanhwe.pop, "[", i = TRUE, j = 3)) # Take the third column with all rows
```

This output is still hard to sift through. An easy way to analyze this is by visualizing this as a heatmap. Since we only care whether or not a given locus is in or out of HWE, we will thus define an αα value and set everything above that value to 1 so that we can visually detect candidate loci where we might reject the HoHo of HWE.

```{r}
alpha  <- 0.05
newmat <- nanhwe.mat
newmat[newmat > alpha] <- 1
```

```{r}
library(lattice)
levelplot(t(newmat))
```

Heatmap showing significant departures from HWE

This simple plot shows us loci in rows and populations in columns. Note, that all loci shown in pink are loci suspected of not being in HWE with p≤0.05p≤0.05.

References
Goss EM., Tabima JF., Cooke DEL., Restrepo S., Fry WE., Forbes GA., Fieland VJ., Cardenas M., Grünwald NJ. 2014. The Irish potato famine pathogen Phytophthora infestans originated in central mexico rather than the andes. Proceedings of the National Academy of Sciences 111:8791–8796. Available at: http://www.pnas.org/content/early/2014/05/29/1401884111.abstract

Population genetics and genomics in R
TABLE OF CONTENTS
PART I 
PART II 
PART III 
WORKSHOP 
ABOUT 
APPENDICES 
Genotypic richness, diversity, and evenness
SE Everhart, ZN Kamvar, and NJ Grünwald
In the previous chapter, we introduced basic summary statistics that can be calculated using poppr. For this chapter, we want to specifically focus on how to evaluate genotypic richness, diversity, and evenness in your data. In this example, we’ll examine the monpop microsatellite data for 13 loci of 694 individuals of the haploid fungal pathogen Monilinia fructicola that infects peach flowers and fruits in commercial orchards.

For this example, we can explore the hypothesis that the population that infects flowers and yields blighted blossoms (BB) is from a more genetically diverse pool (being a product of overwintering, sexual recombinants), than the population that infects and causes fruit rots (FR), which is likely a product of asexual production from existing infections in the orchard. As such, we can ask the following questions:

Is genotypic richness of BB populations higher than for FR populations?
Is genotypic diversity of BB populations higher for FR populations?
Is genotypic evenness higher for BB populations than for FR populations?
For the analysis, we need to read in the data, specify the stratifications in the data, and then set the stratification to symptom so that we can calculate genotypic richness, diversity, and evenness for BB as compared to FR for the entire data set:

library("poppr")
data(monpop)
splitStrata(monpop) <- ~Tree/Year/Symptom
setPop(monpop) <- ~Symptom
monpop
## 
## This is a genclone object
## -------------------------
## Genotype information:
## 
##    264 multilocus genotypes 
##    694 haploid individuals
##     13 codominant loci
## 
## Population information:
## 
##      3 strata - Tree, Year, Symptom
##      2 populations defined - BB, FR
To calculate genotypic richness, diversity, and evenness, we can use the poppr function:

(monpop_diversity <- poppr(monpop))
##     Pop   N MLG eMLG   SE    H    G lambda   E.5  Hexp    Ia  rbarD   File
## 1    BB 113  94 94.0 0.00 4.40 61.7  0.984 0.755 0.584 0.591 0.0493 monpop
## 2    FR 581 191 66.6 4.17 4.58 53.4  0.981 0.543 0.588 0.809 0.0679 monpop
## 3 Total 694 264 73.6 4.33 4.89 65.0  0.985 0.486 0.589 0.729 0.0611 monpop
This shows us several summary statistics:

Abbreviation	Statistic
Pop	Population name.
N	Number of individuals observed.
MLG	Number of multilocus genotypes (MLG) observed.
eMLG	The number of expected MLG at the smallest sample size ≥ 10 based on rarefaction
SE	Standard error based on eMLG.
H	Shannon-Wiener Index of MLG diversity (Shannon, 2001).
G	Stoddart and Taylor’s Index of MLG diversity (Stoddart & Taylor, 1988).
lambda	Simpson’s Index (Simpson, 1949).
E.5	Evenness, E5E5 (Pielou, 1975; Ludwig & Reynolds, 1988; Grünwald et al., 2003).
Hexp	Nei’s unbiased gene diversity (Nei, 1978).
Ia	The index of association, IAIA (Brown, Feldman & Nevo, 1980; Smith et al., 1993).
rbarD	The standardized index of association, r¯dr¯d [@].
Genotypic richness
The number of observed MLGsMLGs is equivalent to genotypic richness. We expect that the BB population would have a higher genotypic richness than the FR population. However, looking at the raw number of MLGs for each symptom type, it shows us the opposite: there are 94 MLGs for BB and 191 MLGs for FR. This discrepancy has to do with the sample size differences, namely N=113N=113 for BB and N=581N=581 for FR. A more appropriate comparison is the eMLGeMLG value, which is an approximation of the number of genotypes that would be expected at the largest, shared sample size (N=113N=113) based on rarefaction. For BB (N=113N=113) the eMLG=94eMLG=94 and for FR (where NN is set to 113) the eMLGeMLG = 66.6. Thus, genotypic richness is indeed higher in the BB populations than the FR population when considering equal sample sizes.

library("vegan")
mon.tab <- mlg.table(monpop, plot = FALSE)
min_sample <- min(rowSums(mon.tab))
rarecurve(mon.tab, sample = min_sample, xlab = "Sample Size", ylab = "Expected MLGs")
title("Rarefaction of Fruit Rot and Blossom Blight")


Genotypic diversity
Diversity measures incorporate both genotypic richness and abundance. There are three measures of genotypic diversity employed by poppr, the Shannon-Wiener index (H), Stoddart and Taylor’s index (G), and Simpson’s index (lambda). In our example, comparing the diversity of BB to FR shows that H is greater for FR (4.58 vs. 4.4), but G is lower (53.4 vs. 61.7). Thus, our expectation that diversity is lower for FR than BB is rejected in the case of H, which is likely due to the sensitivity of the Shannon-Wiener index to genotypic richness in the uneven sample sizes, and accepted in the case of G. To be fair, the sample size used to calculate these diversity measures is different and is therefore not an appropriate comparison.

For an easier statistic to grasp, we have included the Simpson index, which is simply one minus the sum of squared genotype frequencies. This measure provides an estimation of the probability that two randomly selected genotypes are different and scales from 0 (no genotypes are different) to 1 (all genotypes are different). In the data above, we can see that lambda is just barely higher in BB than FR (0.984 vs. 0.981). Since this might be an artifact of sample size, we can explore a correction of Simpson’s index for sample size by multiplying lambda by N/(N−1)N/(N−1). Since R is vectorized, we can do this for all of our populations at once:

N      <- monpop_diversity$N      # number of samples
lambda <- monpop_diversity$lambda # Simpson's index
(N/(N - 1)) * lambda              # Corrected Simpson's index
## [1] 0.9925727 0.9829604 0.9860399
Now we can see that, even after correction, Simpson’s index is still higher for BB.

You try it! Can you calculate the clonal fraction for each population (1 - (MLG/N))?
Genotypic evenness
Evenness is a measure of the distribution of genotype abundances, wherein a population with equally abundant genotypes yields a value equal to 1 and a population dominated by a single genotype is closer to zero. In the example above, the BB population has E.5=0.755E.5=0.755 and the FR population has E.5=0.543E.5=0.543 . This indicates that the MLGs observed in the BB population are closer to equal abundance than those in the FR population. Indeed, when we look at a distribution of the MLGsMLGs for each symptom type it shows us there are many more unique BB symptoms as compared to the FR symptoms.

mon.tab <- mlg.table(monpop)


Conclusions
Calculating measures of genotypic richness, diversity, and evenness is straightforward to do in poppr. In our example, we were able to perform these calculations with one command. However, the ease of calculating these measures is not an indication of the ease of interpretation, particularly when it comes to measures of diversity. There are a large number of diversity measures available and the measures provided here are those we found most useful.

References
Brown AHD., Feldman MW., Nevo E. 1980. Multilocus structure of natural populations of Hordeum spontaneum. Genetics 96:523–536. Available at: http://www.genetics.org/content/96/2/523

Grünwald NJ., Goodwin SB., Milgroom MG., Fry WE. 2003. Analysis of genotypic diversity data for populations of microorganisms. Phytopathology 93:738–746. Available at: http://apsjournals.apsnet.org/doi/abs/10.1094/PHYTO.2003.93.6.738

Hurlbert SH. 1971. The nonconcept of species diversity: A critique and alternative parameters. Ecology 52:577–586. Available at: http://www.jstor.org/discover/10.2307/1934145?uid=3739856&uid=2&uid=4&uid=3739256&sid=21103760010461

Ludwig JA., Reynolds JF. 1988. Statistical ecology: A primer in methods and computing. Wiley.com.

Nei M. 1978. Estimation of average heterozygosity and genetic distance from a small number of individuals. Genetics 89:583–590. Available at: http://www.genetics.org/content/89/3/583.abstract

Pielou EC. 1975. Ecological diversity. Wiley New York.

Shannon CE. 2001. A mathematical theory of communication. ACM SIGMOBILE Mobile Computing and Communications Review 5:3–55. Available at: http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf

Simpson EH. 1949. Measurement of diversity. Nature 163:688. Available at: http://dx.doi.org/10.1038/163688a0

Smith JM., Smith NH., O’Rourke M., Spratt BG. 1993. How clonal are bacteria. Proceedings of the National Academy of Sciences 90:4384–4388. Available at: http://www.pnas.org/content/90/10/4384

Stoddart JA., Taylor JF. 1988. Genotypic diversity: Estimation and prediction in samples. Genetics 118:705–711. Available at: http://www.genetics.org/content/118/4/705

Population genetics and genomics in R
TABLE OF CONTENTS
PART I 
PART II 
PART III 
WORKSHOP 
ABOUT 
APPENDICES 
Linkage disequilibrium
NJ Grünwald, ZN Kamvar and SE Everhart
In this chapter we will formally test if populations are in linkage disequilibrium or not. This test is useful to determine if populations are clonal (where significant disequilibrium is expected due to linkage among loci) or sexual (where linkage among loci is not expected). The null hypothesis tested is that alleles observed at different loci are not linked if populations are sexual while alleles recombine freely into new genotypes during the process of sexual reproduction. In molecular ecology we typically use the index of association or related indices to test this phenomenon.

The index of association
The index of association (IAIA) was originally proposed by Brown et al. (Brown, Feldman & Nevo, 1980) and implemented in the poppr R package (Kamvar, Tabima & Grünwald, 2014) using a permutation approach to assess if loci are linked as described previously by Agapow and Burt [@]. Agapow and Burt also described the index r¯dr¯d that accounts for the number of loci sampled that is less biased and will be used here. The data we will use in this chapter are populations of Phytophthora infestans from North and South America (Goss et al., 2014). We will use the index of association to test the hypothesis that Mexico is the putative origin of P. infestans where populations are expected to be sexual while populations in South America are expected to be clonal.

First, we need to load the packages needed for this analysis.

library("poppr")
library("magrittr")
data(Pinf)
Next, we will analyze the North American population with the index of association and use 999 permutations of the data in order to give us a p-value. Note that the p-value is calculated with the original observation included.

MX <- popsub(Pinf, "North America")
ia(MX, sample = 999)
## |================================================================| 100%


##         Ia       p.Ia      rbarD       p.rD 
## 0.22260850 0.02300000 0.02395687 0.01500000
For advanced users: For reproducibility, use set.seed() before invoking ia().
We observe 48 individuals and see that P=0.015P=0.015 for r¯d=0.024r¯d=0.024. We thus reject the null hypothesis of no linkage among markers. Notice, however, that the observed r¯dr¯d falls on the right tail of the re-sampled distribution and the P value is close to P=0.01P=0.01. Could this population have clones? We can find out by displaying the data.

MX
## 
## This is a genclone object
## -------------------------
## Genotype information:
## 
##    43 multilocus genotypes 
##    48 tetraploid individuals
##    11 codominant loci
## 
## Population information:
## 
##     2 strata - Continent, Country
##     1 populations defined - North America
Clone correction
Indeed we observe 43 multilocus genotypes out of 48 samples. We are looking at partial clonality and thus need to use clone-corrected (also called clone- censored) data:

MX %>% clonecorrect(strata= ~Continent/Country) %>% ia(sample = 999)
## |================================================================| 100%


##          Ia        p.Ia       rbarD        p.rD 
## 0.079811141 0.225000000 0.008568857 0.213000000
Now r¯dr¯d is located more centrally in the distribution expected from unlinked loci. Note that PP has improved and we fail to reject the null hypothesis of no linkage among markers. Thus it appears that populations in Mexico are sexual.

Next let’s use the same process to evaluate the South American population:

SA <- popsub(Pinf, "South America")
ia(SA, sample = 999)
## |================================================================| 100%


##        Ia      p.Ia     rbarD      p.rD 
## 2.8733344 0.0010000 0.3446431 0.0010000
Here we find significant support for the hypothesis that alleles are linked across loci with P<0.001P<0.001. The observed r¯d=0.345r¯d=0.345 and falls outside of the distribution expected under no linkage. Let’s look at the clone-corrected data and make sure this is not an artifact of clonality:

SA %>% clonecorrect(strata= ~Continent/Country) %>% ia(sample=999)
## |================================================================| 100%


##        Ia      p.Ia     rbarD      p.rD 
## 2.6335025 0.0010000 0.3145711 0.0010000
Both clone-corrected (N=29N=29) and uncorrected data (N=38N=38) reject the hypothesis of no linkage among markers. We thus have support for populations in Mexico being sexual while those in South America are clonal.

This approach has been applied to provide support for Mexico as the putative center of origin of the potato late blight pathogen P. infestans (Goss et al., 2014). At the center of origin this organism is expected to reproduce sexually, while South American populations are clonal.

Pairwise r¯dr¯d over all loci
To ensure that the pattern of linkage disequilibrium seen is not due to a single pair of loci, you can calculate IAIA and r¯dr¯d over all pairs of loci. We’ll perform this on the clone-corrected samples as above.

Pairwise for the Mexican population:

mxpair <- MX %>% clonecorrect(strata = ~Continent/Country) %>% pair.ia
## |================================================================| 100%


Pairwise for the South American population:

sapair <- SA %>% clonecorrect(strata = ~Continent/Country) %>% pair.ia
## |================================================================| 100%


The heatmaps produced make it look like there is more linkage in the Mexican population! But this is where looks can be deceiving. The color palettes are scaled to the data. We can confirm it by looking at the values:

head(mxpair, 10) # Mexico
##                    Ia       rbarD
## Pi02:D13   0.03952145  0.04195430
## Pi02:Pi33  0.05386977  0.09014200
## Pi02:Pi04  0.06845658  0.06944477
## Pi02:Pi4B -0.08388353 -0.08457969
## Pi02:Pi16  0.13698795  0.13710471
## Pi02:G11   0.11013984  0.11014617
## Pi02:Pi56  0.11255045  0.11365168
## Pi02:Pi63 -0.06903465 -0.06918173
## Pi02:Pi70  0.05049544  0.05049764
## Pi02:Pi89  0.03529987  0.03621175
head(sapair, 10) # South America
##                     Ia        rbarD
## Pi02:D13   0.006586122  0.006730583
## Pi02:Pi33  0.000000000          NaN
## Pi02:Pi04 -0.017633090 -0.017647437
## Pi02:Pi4B  0.288949585  0.301335905
## Pi02:Pi16  0.126278265  0.142859861
## Pi02:G11   0.600576689  0.609929970
## Pi02:Pi56  0.190590008  0.215322486
## Pi02:Pi63  0.673519987  0.684987213
## Pi02:Pi70  0.349111187  0.397116239
## Pi02:Pi89  0.355279336  0.368027168
We can see that most of the values from South America are indeed higher than in Mexico. Notice the value that says “NaN” in the South American data? That represents missing data. If you recall from the chapter on Locus Stats, the number of alleles at locus Pi33 for the South American population was 1. If you try to analyze the index of association on a locus with only one allele, you will get an undefined value. This is why the heatmap for the South American population has grey squares in it.

Plotting the output of pair.ia
The output of pair.ia is a matrix that also has a class of “pairia”. It has a specific plot method that we can use to plot the output again and set a standard limit to the plot by specifying a range.

plotrange <- range(c(mxpair, sapair), na.rm = TRUE)
plot(mxpair, limits = plotrange)


plot(sapair, limits = plotrange)


References
Brown AHD., Feldman MW., Nevo E. 1980. Multilocus structure of natural populations of Hordeum spontaneum. Genetics 96:523–536. Available at: http://www.genetics.org/content/96/2/523

Goss EM., Tabima JF., Cooke DEL., Restrepo S., Fry WE., Forbes GA., Fieland VJ., Cardenas M., Grünwald NJ. 2014. The Irish potato famine pathogen Phytophthora infestans originated in central mexico rather than the andes. Proceedings of the National Academy of Sciences 111:8791–8796. Available at: http://www.pnas.org/content/early/2014/05/29/1401884111.abstract

Kamvar ZN., Tabima JF., Grünwald NJ. 2014. PopprPoppr: An R package for genetic analysis of populations with clonal, partially clonal, and/or sexual reproduction. PeerJ 2:e281. Available at: http://dx.doi.org/10.7717/peerj.281

Population genetics and genomics in R
TABLE OF CONTENTS
PART I 
PART II 
PART III 
WORKSHOP 
ABOUT 
APPENDICES 
Population structure: GSTGST, genetic distance, and clustering
ZN Kamvar, SE Everhart and NJ Grünwald
In this chapter we explore various ways of assessing if population are structured (e.g., differentiated). You can think of population structure as identifying clusters or groups of more closely related individuals resulting from reduced gene flow among these groups. Populations can be studied to determine if they are structured by using, for example, population differentiation summary statistics (e.g. GSTGST), clustering or minimum spanning networks. Note, that this chapter will utilize many data sets due to the unique features offered by each. Let’s first look at an example of population differentiation based on GSTGST.

GSTGST an example with Felis catus data.
Assessing genetic diversity almost always starts with an analysis of a parameter such as GSTGST. There are lengthy debates as to what measure of differentiation is better (Meirmans & Hedrick, 2011). Instead of going into that lengthy debate, it would be more worthwhile to point you into the direction of a package dedicated to Modern Methods of Differentiation called mmod. We will use the data set nancycats containing 17 colonies of cats collected from Nancy, France. As cats tend to stay within small groups, we expect to see some population differentiation. In terms of these diversity measures, an index of GST=0GST=0 indicates no differentiation, whereas GST=1GST=1 indicates that populations are segregating for differing alleles.

Let’s load the package and the example data set:

library("mmod")
data("nancycats")
nancycats
## /// GENIND OBJECT /////////
## 
##  // 237 individuals; 9 loci; 108 alleles; size: 145.3 Kb
## 
##  // Basic content
##    @tab:  237 x 108 matrix of allele counts
##    @loc.n.all: number of alleles per locus (range: 8-18)
##    @loc.fac: locus factor for the 108 columns of @tab
##    @all.names: list of allele names for each locus
##    @ploidy: ploidy of each individual  (range: 2-2)
##    @type:  codom
##    @call: genind(tab = truenames(nancycats)$tab, pop = truenames(nancycats)$pop)
## 
##  // Optional content
##    @pop: population of each individual (group size range: 9-23)
##    @other: a list containing: xy
Now we will use Hendrick’s standardized GSTGST to assess population structure among these populations (Hedrick, 2005).

Gst_Hedrick(nancycats)
## $per.locus
##      fca8     fca23     fca43     fca45     fca77     fca78     fca90 
## 0.4750445 0.2956688 0.2675766 0.2653163 0.4855829 0.1933327 0.3807578 
##     fca96     fca37 
## 0.3913924 0.1609576 
## 
## $global
## [1] 0.3084895
What does this output tell us?

Next we will look at genetic distance to find related groups of individuals.

Genetic Distance
If we wanted to analyze the relationship between individuals or populations, we would use genetic distance measures which calculate the “distance” between samples based on their genetic profile. These distances can be visualized with heatmaps, dendrograms, or minimum spanning networks. In the package poppr, there are several distances available:

Distance	Function	Marker type	Can handle missing data
Bruvo’s distance	bruvo.dist	microsatellite	yes
Edwards’ distance	edwards.dist	any	no
Nei’s distance	nei.dist	any	no
Provesti’s distance	provesti.dist	any	yes
Reynolds’ distance	reynolds.dist	any	no
Rogers’ distance	rogers.dist	any	no
Provesti’s distance	bitwise.dist	SNP	yes
One common way to visualize a genetic distance is with a dendrogram. For this example, we will use the microbov data set (Laloe et al., 2007). This contains information on 704 cattle from both Africa and France over several different breeds. We can create a dendrogram over all 704 samples, but that would be difficult to visualize. For our purposes, let’s take ten random samples and calculate Provesti’s distance, which will return the fraction of the number of differences between samples:

library("poppr")
library("ape") # To visualize the tree using the "nj" function
library("magrittr")
data(microbov)
set.seed(10)
ten_samples <- sample(nInd(microbov), 10)
mic10       <- microbov[ten_samples]
(micdist    <- provesti.dist(mic10))
##              FRBTBDA35243 AFBTSOM9386 FRBTBAZ26396 FRBTGAS9052 AFBIZEB9462
## AFBTSOM9386     0.7500000                                                 
## FRBTBAZ26396    0.6000000   0.6833333                                     
## FRBTGAS9052     0.6333333   0.8500000    0.5333333                        
## AFBIZEB9462     0.7166667   0.6666667    0.7833333   0.8500000            
## AFBTND211       0.6333333   0.5666667    0.6833333   0.7333333   0.8000000
## AFBTSOM9362     0.6000000   0.5333333    0.6666667   0.8166667   0.7000000
## AFBTSOM9360     0.7000000   0.5166667    0.6000000   0.7166667   0.7333333
## FRBTCHA25069    0.6166667   0.7000000    0.5833333   0.6833333   0.8000000
## FRBTBAZ26388    0.5333333   0.7000000    0.5500000   0.6333333   0.7500000
##              AFBTND211 AFBTSOM9362 AFBTSOM9360 FRBTCHA25069
## AFBTSOM9386                                                
## FRBTBAZ26396                                               
## FRBTGAS9052                                                
## AFBIZEB9462                                                
## AFBTND211                                                  
## AFBTSOM9362  0.5333333                                     
## AFBTSOM9360  0.5500000   0.5666667                         
## FRBTCHA25069 0.6500000   0.7333333   0.7166667             
## FRBTBAZ26388 0.6000000   0.6666667   0.6833333    0.6166667
The above represents the pairwise distances between these 10 samples. We will use this distance matrix to create a neighbor-joining tree.

theTree <- micdist %>%
  nj() %>%    # calculate neighbor-joining tree
  ladderize() # organize branches by clade
plot(theTree)
add.scale.bar(length = 0.05) # add a scale bar showing 5% difference.


Notice that the sample names start with either “AF” or “FR”. This indicates their country of origin and we are seeing that the populations cluster correspondingly. Of course, a tree is a hypothesis and one way of generating support is to bootstrap loci. This can be achieved with the poppr function aboot.

set.seed(999)
aboot(mic10, dist = provesti.dist, sample = 200, tree = "nj", cutoff = 50, quiet = TRUE)


## 
## Phylogenetic tree with 10 tips and 8 internal nodes.
## 
## Tip labels:
##  FRBTBDA35243, AFBTSOM9386, FRBTBAZ26396, FRBTGAS9052, AFBIZEB9462, AFBTND211, ...
## Node labels:
##  100, NA, NA, 71.5, NA, NA, ...
## 
## Unrooted; includes branch lengths.
The bootstrap value of 100 on the node separating the French and African samples gives support that the country of origin is a factor in how these breeds are structured. If we wanted to analyze all of the breeds against one another, it would be better to create a bootstrapped dendrogram based on a genetic distance. To do this, we will add 3 stratifications to the microbov data set: Country, Breed, and Species. We will then set the population to Country by Breed, convert the data to a genpop object and then create a tree using aboot with Nei’s genetic distance.

# Setting up the data
strata(microbov) <- data.frame(other(microbov))
microbov
## /// GENIND OBJECT /////////
## 
##  // 704 individuals; 30 loci; 373 alleles; size: 1.1 Mb
## 
##  // Basic content
##    @tab:  704 x 373 matrix of allele counts
##    @loc.n.all: number of alleles per locus (range: 5-22)
##    @loc.fac: locus factor for the 373 columns of @tab
##    @all.names: list of allele names for each locus
##    @ploidy: ploidy of each individual  (range: 2-2)
##    @type:  codom
##    @call: genind(tab = truenames(microbov)$tab, pop = truenames(microbov)$pop)
## 
##  // Optional content
##    @pop: population of each individual (group size range: 30-61)
##    @strata: a data frame with 3 columns ( coun, breed, spe )
##    @other: a list containing: coun  breed  spe
nameStrata(microbov) <- ~Country/Breed/Species

# Analysis
set.seed(999)
microbov %>%
  genind2genpop(pop = ~Country/Breed) %>%
  aboot(cutoff = 50, quiet = TRUE, sample = 1000, distance = nei.dist)
## 
##  Converting data from a genind to a genpop object... 
## 
## ...done.


## 
## Phylogenetic tree with 15 tips and 14 internal nodes.
## 
## Tip labels:
##  AF_Borgou, AF_Zebu, AF_Lagunaire, AF_NDama, AF_Somba, FR_Aubrac, ...
## Node labels:
##  100, 100, 99.8, 93.1, 92.9, 63.8, ...
## 
## Rooted; includes branch lengths.
Now we can see that, in all 1,000 bootstrapped trees, the African and French samples were each in separate clades. Of course, dendrograms are only one type of analysis you can use genetic distances for. Below is a table describing some of the different analyses for which you can utilize genetic distance:

Analysis	Function	Package	Note
Bootstrapped dendrograms	aboot	poppr	
Mantel Test	mantel.randtest	ade4	To be used with geographic distance matrix
Principle Coordinates Analysis	cmdscale	stats	
DAPC	dapc	adegenet	Convert to matrix with  as.matrix
Minimum Spanning Networks	poppr.msn	poppr	requires a distance matrix; cannot handle genpop
K-means hierarchical clustering
A recent study reported that the origin of the potato late blight pathogen Phytophthora infestans lies in Mexico as opposed to South America (Goss et al., 2014). We saw in the previous chapter that South American populations showed signatures of clonal reproduction while Mexican populations showed no evidence rejecting the null hypothesis of random mating. In this section, we will use K-means clustering in combination with bootstrapped dendrograms to see how well this pattern holds up. Clonal populations should have short terminal branch lengths and should cluster according to those branches. Panmictic populations will show no clear pattern. Let’s look at the data:

library("poppr")
data("Pinf")
Pinf
## 
## This is a genclone object
## -------------------------
## Genotype information:
## 
##    72 multilocus genotypes 
##    86 tetraploid individuals
##    11 codominant loci
## 
## Population information:
## 
##     2 strata - Continent, Country
##     2 populations defined - South America, North America
First, we will perform a cluster analysis:

MX <- popsub(Pinf, "North America")
MXclust <- find.clusters(MX)
MX_PCA
MX_PCA

## Choose the number PCs to retain (>=1):
> 50
PC stands for principal components, which are unit-less transformations of your data that explain the variance observed. For the purposes of  find.clusters, we can keep as many as we want.

MX_CLUSTER
MX_CLUSTER

## Choose the number PCs to retain (>=2:
> 3
BIC stands for “Bayesian Information Criterion”. The lower the BIC value, the better. On the x axis are the number of clusters. We see that there is a bend at 3 clusters, indicating that the data clusters optimally into three groups.

And now we can see the cluster assignments:

MXclust
## $Kstat
## NULL
## 
## $stat
## NULL
## 
## $grp
##  PiMX01  PiMX02  PiMX03  PiMX04  PiMX05  PiMX06  PiMX07  PiMX10  PiMX11 
##       1       1       3       2       2       2       2       3       3 
##  PiMX12  PiMX13  PiMX14  PiMX15  PiMX16  PiMX17  PiMX18  PiMX19  PiMX20 
##       2       3       3       1       3       1       2       2       2 
##  PiMX21  PiMX22  PiMX23  PiMX24  PiMX25  PiMX26  PiMX27  PiMX28  PiMX29 
##       1       3       1       3       2       1       3       3       2 
##  PiMX30  PiMX40  PiMX41  PiMX42  PiMX43  PiMX44  PiMX45  PiMX46  PiMX47 
##       3       3       1       1       1       1       1       1       1 
##  PiMX48  PiMX49  PiMX50 PiMXT01 PiMXT02 PiMXT03 PiMXT04 PiMXT05 PiMXT06 
##       3       3       3       1       1       2       2       1       2 
## PiMXT07 PiMXt48 PiMXt68 
##       2       2       2 
## Levels: 1 2 3
## 
## $size
## [1] 17 16 15
We will go through the same procedure for the South American population.

SA <- popsub(Pinf, "South America")
SAclust <- find.clusters(SA)
SA_PCA
SA_PCA

## Choose the number PCs to retain (>=1):
> 30
SA_CLUSTER
SA_CLUSTER

## Choose the number PCs to retain (>=2):
> 4
Notice here that there is no local minimum in the curve. This indicates that there might not be enough information in the data set to properly cluster. We will go ahead by choosing the highest number of clusters:

Trees
Now we will build trees. We are using Bruvo’s distance since polyploids bias calculation of other distances:

pinfreps <- c(2, 2, 6, 2, 2, 2, 2, 2, 3, 3, 2)
MXtree <- bruvo.boot(MX, replen = pinfreps, cutoff = 50, quiet = TRUE)


SAtree <- bruvo.boot(SA, replen = pinfreps, cutoff = 50, quiet = TRUE)


We see very long terminal branches in the MX tree. Let’s see how the groups we found with the clustering algorithm match up:

library("ape")
cols <- rainbow(4)
plot.phylo(MXtree, cex = 0.8, font = 2, adj = 0, tip.color = cols[MXclust$grp],
           label.offset = 0.0125)
nodelabels(MXtree$node.label, adj = c(1.3, -0.5), frame = "n", cex = 0.8,
           font = 3, xpd = TRUE)
axisPhylo(3)


You can see that the assigned clusters don’t necessarily group with the dendrogram clusters. Let’s see what happens when we view this with the South American population:

plot.phylo(SAtree, cex = 0.8, font = 2, adj = 0, tip.color = cols[SAclust$grp],
           label.offset = 0.0125)
nodelabels(SAtree$node.label, adj = c(1.3, -0.5), frame = "n", cex = 0.8,
           font = 3, xpd = TRUE)
axisPhylo(3)


Everything clusters together nicely, further supporting a non-panmictic population.

References
Goss EM., Tabima JF., Cooke DEL., Restrepo S., Fry WE., Forbes GA., Fieland VJ., Cardenas M., Grünwald NJ. 2014. The Irish potato famine pathogen Phytophthora infestans originated in central mexico rather than the andes. Proceedings of the National Academy of Sciences 111:8791–8796. Available at: http://www.pnas.org/content/early/2014/05/29/1401884111.abstract

Hedrick PW. 2005. A standardized genetic differentiation measure. Evolution 59:1633–1638. Available at: http://dx.doi.org/10.1111/j.0014-3820.2005.tb01814.x

Laloe D., Jombart T., Dufour A-B., Moazami-Goudarzi K. 2007. Consensus genetic structuring and typological value of markers using multiple co-inertia analysis. Genetics Selection Evolution 39:545–567. Available at: http://dx.doi.org/10.1051/gse:2007021

Meirmans PG., Hedrick PW. 2011. Assessing population structure: FSTFST and related measures. Molecular Ecology Resources 11:5–18. Available at: http://onlinelibrary.wiley.com/doi/10.1111/j.1755-0998.2010.02927.x/full

Population genetics and genomics in R
TABLE OF CONTENTS
PART I 
PART II 
PART III 
WORKSHOP 
ABOUT 
APPENDICES 
Minimum spanning networks
ZN Kamvar, SE Everhart and NJ Grünwald
Minimum spanning networks (MSN) are a great way to visualize relationships among individuals in your data. Particularly for clonal organisms it can be a more powerful visualization tool than trees. In this chapter, we will show you how to construct and view minimum spanning networks on the command line and in an interactive viewer.

From the command line
Note:
This section will utilize Bruvo’s distance (Bruvo et al., 2004). Calculating Bruvo’s distance is computationally different to other distances; hence, this requires specialized functions for minimum spanning networks and bootstrapping that do not require a distance matrix:

General function	Bruvo specific
poppr.msn	bruvo.msn
aboot	bruvo.boot
Minimum spanning network
For this section, we will use the monpop data set from (Everhart & Scherm, 2015). See Chapter 5 for more details. We will be focusing on sources of multilocus genotypes. The seasonal epidemic of the pathogen Monilinia fructicola begins with an ascospore (sexual propagule) released from a mummified peach fruit that had overwintered on the ground. It infects an emerging blossom that, in turn, asexually infects fruit, which proceed with cyclical, asexual infections. Two obvious questions are:

Are the major genotypes of Fruit Rot (FR) samples closely related?
To what degree do the Blossom Blight (BB) samples contribute to the FR?
Let’s load the data:

library("poppr")
library("magrittr")
data(monpop)
splitStrata(monpop) <- ~Tree/Year/Symptom
summary(monpop)
## 
## // Number of individuals: 694
## // Group sizes: 23 41 132 73 5 13 1 64 85 130 30 97
## // Number of alleles per locus: 3 6 3 11 9 5 8 7 5 9 8 11 10
## // Number of alleles per group: 48 53 58 48 37 40 13 44 60 64 60 63
## // Percentage of missing data: 0.51 %
## // Observed heterozygosity: 0
We notice that tree number 26 is the only one to have been sampled for all three years. Let’s use it as an example.

t26 <- monpop %>% setPop(~Tree) %>% popsub("26") %>% setPop(~Year/Symptom)
t26
## 
## This is a genclone object
## -------------------------
## Genotype information:
## 
##    155 multilocus genotypes 
##    390 haploid individuals
##     13 codominant loci
## 
## Population information:
## 
##      3 strata - Tree, Year, Symptom
##      6 populations defined - 9_BB, 9_FR, 10_BB, 10_FR, 11_BB, 11_FR
Now that we have our tree, let’s calculate a MSN using Bruvo’s distance (Bruvo et al., 2004). Remember that this distance is based on a stepwise mutation model, so we have to first specify what kind of repeats units we have in our data (eg. dinucleotide = 2, trinucleotide = 3, etc.):

# Set up our repeat lengths and populations to analyze
reps <- c(CHMFc4 = 7, CHMFc5 = 2, CHMFc12 = 4,
          SEA = 4, SED = 4, SEE = 2, SEG = 6,
          SEI = 3, SEL = 4, SEN = 2,
          SEP = 4, SEQ = 2, SER = 4)

sub9 <- c("9_BB", "9_FR")

# Calculate the MSN
t26.9msn <- bruvo.msn(t26, replen = reps, sublist = sub9, showplot = FALSE)
The minimum spanning network is calculated via bruvo.msn. We have set the argument showplot = FALSE because we want to use the more powerful function plot_poppr_msn to view the MSN. I am telling it to label none of the samples, color populations using the “cm.colors” palette and scale the size of the nodes to log1.25log1.25. If you want to know what other things this function can do, simply type help("plot_poppr_msn")

# Visualize the network
set.seed(120)
plot_poppr_msn(t26, t26.9msn, inds = "none", palette = cm.colors, nodebase = 1.25)


We can see that the Blossom Blight in the tree (pink pie slices) heavily contributed to the major groups of MLGs found in the Fruit Rot (blue pie pieces).

Try it for yourself! See if you can produce similar graphs with the 2010 and 2011 populations.
Interactive viewer
Creating a customized, publishable MSN is somewhat daunting as the plot_poppr_msn function has a whole lot arguments:

args(plot_poppr_msn)
## function (x, poppr_msn, gscale = TRUE, gadj = 3, mlg.compute = "original", 
##     glim = c(0, 0.8), gweight = 1, wscale = TRUE, nodebase = 1.15, 
##     nodelab = 2, inds = "ALL", mlg = FALSE, quantiles = TRUE, 
##     cutoff = NULL, palette = NULL, layfun = layout.auto, beforecut = FALSE, 
##     pop.leg = TRUE, scale.leg = TRUE, ...) 
## NULL
We created the interactive tool imsn() to facilitate interactive plotting. This function will be able to create minimum spanning networks from all genind objects in your current R session. In this section, we will recreate the plot above. First we will make sure that we have all the data we need and then we will run imsn().

ls() # Show all the data we have in our workspace
##  [1] "Aeutamova"           "Aeutamovacc"         "Aeutccsignif"       
##  [4] "Aeut.new"            "Aeut.new.amova"      "Aeut.new.amova.test"
##  [7] "Aeutsignif"          "alpha"               "bib"                
## [10] "chrom"               "cols"                "contrib"            
## [13] "dapc.H3N2"           "dapc.results"        "dapc.x"             
## [16] "dna"                 "dna_file"            "dp"                 
## [19] "dp2"                 "dpf"                 "freq399"            
## [22] "freq906"             "gac"                 "gff"                
## [25] "gff_file"            "gi.rubi"             "gl.rubi"            
## [28] "grp"                 "gt"                  "H3N2"               
## [31] "html"                "i"                   "iPinf"              
## [34] "lambda"              "mat"                 "min_sample"         
## [37] "miss"                "monpop"              "monpop_diversity"   
## [40] "mon.tab"             "MX"                  "MXia"               
## [43] "mxpair"              "myIndex"             "myLevels"           
## [46] "myList"              "myPlots"             "myRegex"            
## [49] "myRows"              "N"                   "nan1"               
## [52] "nancycats"           "nanhwe.full"         "nanhwe.mat"         
## [55] "nanhwe.pop"          "nanmean"             "nanzero"            
## [58] "newmat"              "node.size"           "p"                  
## [61] "Pinf"                "pinflt"              "Pinf.ploidy"        
## [64] "plotrange"           "pnw.dapc"            "pop.data"           
## [67] "Pram"                "pramx"               "P.tab"              
## [70] "quants"              "reps"                "rubi.dist"          
## [73] "rubi.msn"            "rubi.pca"            "rubi.pca.scores"    
## [76] "rubi.VCF"            "SA"                  "SAia"               
## [79] "samps_per_row"       "sapair"              "snp399"             
## [82] "snp906"              "sub9"                "t26"                
## [85] "t26.9msn"            "temp"                "tree"               
## [88] "vcf"                 "vcf_file"            "x"                  
## [91] "X"                   "x.dist"              "x.msn"              
## [94] "x.msn2"              "x.tree"              "xval"
Note: you will not be able to access your R console after running this function until you close the pop-up app.
imsn()
(Note: you probably will not see the whole screen like this, but you can still scroll down)

Intial view of imsn
Intial view of imsn

We can see that there are five tabs and a sidebar. For now, we’ll explain what is on the sidebar.

status view
status view

The first thing you see is a green bar that says “ready”. This means that there are no processes going on in the background and that it’s waiting for input. Below that are three buttons: Go!, reData, and reGraph. These tell the program that you are ready render your graphs.

Parameters (sidebar)
Notice that there are two main sections, Data Parameters and Graphical Parameters.

Data Parameters
As you can expect, these options represent parameters that will manipulate your data. Changing any of these choices means that the distance matrix will be recalculated. When you modify things in this section and are satisfied with your choices, you can use the reData button to update the graph. You see here four choices presented:

Choose dataset - This dropdown menu allows you to choose your data set.
choose dataset
Choose populations - this will change based on your data set, but it allows you to select different populations with which to subset your data
choose populations
Convert to genclone? - If your data is in genind format, you can convert it to genclone (minimally reduces time for clonal organisms)
convert to genclone?
Choose distance calculation - The distance measure to calculate from your data (notice that there’s “Custom” at the bottom)
choose distance calculation
Distance arguments - every distance has different arguments, this is where you can modify them
Distance arguments
Include reticulations? - This is an important parameter that will include reticulations in the minimum spanning network. We’ll explore this with the Pram data set later.
include reticulations?
Graphical Parameters
The parameters listed here allow you to modify the look and feel of the graph without having to recalculate the data. We encourage the user to play around with these to see what the different sliders and buttons do.

Output (tabs)
Next let’s explore the different tabs on the MSN popup window:

Plot
This is the tab you see immediately. It has nothing in it because we haven’t told imsn() to do anything. Let’s hit Go! with our default parameters and see what happens.

minimum spanning network of partial_clone
minimum spanning network of partial_clone

We now have a minimum spanning network of partial clone based a dissimilarity distance (“diss.dist”) where the distance is represented as counts of dissimilar alleles.

Data
partial_clone output
partial_clone output

This is the output of your data set. You can use this to confirm that your data is what you expect.

Command
partial_clone command
partial_clone command

If you copy and paste this command into your R console, you can recreate the minimum spanning network. This is important to make sure that your figure is reproducible.

data("partial_clone") # Don't forget to load the data
partial_clone_sub <- popsub(partial_clone, blacklist = character(0))
partial_clone_dist <- diss.dist(partial_clone_sub, percent = FALSE, mat = FALSE)
min_span_net <- poppr.msn(partial_clone_sub, partial_clone_dist, showplot = FALSE, include.ties = TRUE)

set.seed(69)
plot_poppr_msn(partial_clone,
           min_span_net,
           inds = "ALL",
           mlg = FALSE,
           gadj = 3,
           nodebase = 1.15,
           palette = rainbow,
           cutoff = NULL,
           quantiles = FALSE,
           beforecut = TRUE)


Save Plot

This allows you to save the plot to your computer in pdf or png format.

Session Information

This shows you what package versions were used to create the graph.

Re-creating MSNs
We will now recreate the minimum spanning network that was displayed for the monpop data set.

First, choose the t26 data set that we defined above:


Next, make sure you only select the “9_BB” and “9_FR” populations.


Now, Select “Bruvo’s Distance”:


Notice how the options have now changed. This is because bruvo’s distance is a special distance that is parametric. Don’t worry about the model for missing data, but in the SSR repeat lengths section, type “reps” (we defined this earlier):


reps # our previous definition of "reps"
##  CHMFc4  CHMFc5 CHMFc12     SEA     SED     SEE     SEG     SEI     SEL 
##       7       2       4       4       4       2       6       3       4 
##     SEN     SEP     SEQ     SER 
##       2       4       2       4
Now, modify the following graphical parameters:

Node Size Scale: 1.25 (this is the option nodebase)
Random seed: 120
Labels: none (this is the option inds)
Palette: cm.colors
Once everthing is set, hit reData and the graph will reset just like this:


Your turn (use the data for Phytophthora ramorum)
Now we will use data from Kamvar et al. (2015). This is the Sudden Oak Death pathogen collected from forests in Curry County, Oregon and nurseries in California and Oregon (Goss et al., 2009). We will use this data set to see why reticulations are important for highly clonal organisms.

Load the data
Exit the imsn() graphical window and execute the following statements:

data("Pram")
Pram
## 
## This is a genclone object
## -------------------------
## Genotype information:
## 
##     98 multilocus genotypes 
##    729 diploid individuals
##      5 codominant loci
## 
## Population information:
## 
##      3 strata - SOURCE, YEAR, STATE
##      9 populations defined - 
## Nursery_CA, Nursery_OR, JHallCr_OR, ..., Winchuck_OR, ChetcoMain_OR, PistolRSF_OR
This data has a few things in the “other” slot that we need. First is the repeat lengths called “REPLEN” and the other is a vector of hexadecimal colors for each population called “comparePal”.

other(Pram)$REPLEN
##  PrMS6A1  Pr9C3A1 PrMS39A1 PrMS45A1 PrMS43A1 
##  3.00000  2.00000  4.00001  4.00000  4.00000
other(Pram)$comparePal
##    Nursery_CA    Nursery_OR    JHallCr_OR NFChetHigh_OR      Coast_OR 
##     "#000000"     "#808080"     "#A6CEE3"     "#99CD91"     "#B89B74" 
##   HunterCr_OR   Winchuck_OR ChetcoMain_OR  PistolRSF_OR 
##     "#F06C45"     "#ED8F47"     "#825D99"     "#B15928"
We will use these to create the following minimum spanning network:



Here are the parameters you should use:

Data:
Dataset: Pram
Distance: Bruvo
SSR Repeat Lengths: other(Pram)$REPLEN (Paste code into the pop-up app.)
Graphical:
Grey Scale: 9
Node Size Scale: 1.95
Random Seed: 78
Labels: none
Palette: Custom: other(Pram)$comparePal (Paste code into the pop-up app.)
When you finish your session, you can exit the app by closing the window.

References
Bruvo R., Michiels NK., D’Souza TG., Schulenburg H. 2004. A simple method for the calculation of microsatellite genotype distances irrespective of ploidy level. Molecular Ecology 13:2101–2106. Available at: http://dx.doi.org/10.1111/j.1365-294X.2004.02209.x

Everhart S., Scherm H. 2015. Fine-scale genetic structure of Monilinia fructicola during brown rot epidemics within individual peach tree canopies. Phytopathology 105:542–549. Available at: https://doi.org/10.1094/PHYTO-03-14-0088-R

Goss EM., Larsen M., Chastagner GA., Givens DR., Grünwald NJ. 2009. Population genetic analysis infers migration pathways of phytophthora ramorum in uS nurseries. PLoS Pathog 5:e1000583. Available at: http://dx.doi.org/10.1371/journal.ppat.1000583

Kamvar Z., Larsen M., Kanaskie A., Hansen E., Grünwald N. 2015. Spatial and temporal analysis of populations of the sudden oak death pathogen in oregon forests. Phytopathology 105:982–989. Available at: http://dx.doi.org/10.1094/PHYTO-12-14-0350-FI

Population genetics and genomics in R
TABLE OF CONTENTS
PART I 
PART II 
PART III 
WORKSHOP 
ABOUT 
APPENDICES 
AMOVA
ZN Kamvar, SE Everhart and NJ Grünwald
In this chapter, we will utilize AMOVA to analyze our populations. AMOVA stands for Analysis of MOlecular VAriance and is a method to detect population differentiation utilizing molecular markers (Excoffier, Smouse & Quattro, 1992). This procedure was initially implemented for DNA haplotypes, but applies to any marker system. The implementation of AMOVA in poppr requires two very basic components: (1) A distance matrix derived from the data and (2) a separate table used to partition the data into different stratifications.

The distance matrix can be calculated using any distance as long as it is euclidean. The distance that is used in the program Arlequin is the opposite of the Kronecker Delta function that counts the number of differences summed over LL loci:

δl,m={1 if m=l,0 if m≠l
δl,m={1 if m=l,0 if m≠l
di,j=∑L=1L1−δi,j
di,j=∑L=1L1−δi,j

Data set
To calculate AMOVA in poppr, one simply needs to supply a data set with stratifications. We will use the Aphanomyces euteiches data set from (Grünwald & Hoheisel, 2006).

library("poppr")
data("Aeut")
strata(Aeut) <- data.frame(other(Aeut)$population_hierarchy)
Aeut <- as.genclone(Aeut)
Aeut
## 
## This is a genclone object
## -------------------------
## Genotype information:
## 
##    119 original multilocus genotypes 
##    187 diploid individuals
##     56 dominant loci
## 
## Population information:
## 
##      3 strata - Pop_Subpop, Pop, Subpop
##      2 populations defined - Athena, Mt. Vernon
We can see that this data set contains clonal data and has three stratifications where the first is really a combination of the other levels. We can take a look at the different stratifications, populations or subpopulations:

table(strata(Aeut, ~Pop))  # Populations
## 
##     Athena Mt. Vernon 
##         97         90
table(strata(Aeut, ~Pop/Subpop, combine = FALSE))  # Subpopulations
##             Subpop
## Pop           1  2  3  4  5  6  7  8  9 10
##   Athena      9 12 10 13 10  5 11  8 10  9
##   Mt. Vernon 10  6  8 12 17 12 12 13  0  0
In this example, we have a data set of 187 individuals sampled from two fields located in Athena or Mt. Vernon over 8 or 10 different soil samples within each field. We want to see if most of the variance is observed at the sample, field, or regional level.

Analysis
In panmictic populations, we would expect to see most of the variance arise from within samples. If we see that the most of the variance occurs among samples within populations or among populations, then there is evidence that we have some sort of population structure. In the case of clonal organisms, this would help support a hypothesis of clonal reproduction. Since Aphanomyces eutieches is known to be clonal, we would not expect most of the variance to come from within samples.

Let’s invoke the AMOVA functions with and without clone correction:

Aeutamova <- poppr.amova(Aeut, ~Pop/Subpop)
Aeutamovacc <- poppr.amova(Aeut, ~Pop/Subpop, clonecorrect = TRUE)
We’ll look at the AMOVA results for both analyses.

Aeutamova
## $call
## ade4::amova(samples = xtab, distances = xdist, structures = xstruct)
## 
## $results
##                             Df    Sum Sq     Mean Sq
## Between Pop                  1 1051.2345 1051.234516
## Between samples Within Pop  16  273.4575   17.091091
## Within samples             169  576.5059    3.411277
## Total                      186 1901.1979   10.221494
## 
## $componentsofcovariance
##                                            Sigma          %
## Variations  Between Pop                11.063446  70.006786
## Variations  Between samples Within Pop  1.328667   8.407483
## Variations  Within samples              3.411277  21.585732
## Total variations                       15.803391 100.000000
## 
## $statphi
##                         Phi
## Phi-samples-total 0.7841427
## Phi-samples-Pop   0.2803128
## Phi-Pop-total     0.7000679
Aeutamovacc
## $call
## ade4::amova(samples = xtab, distances = xdist, structures = xstruct)
## 
## $results
##                             Df    Sum Sq    Mean Sq
## Between Pop                  1  741.9872 741.987234
## Between samples Within Pop  16  185.6877  11.605483
## Within samples             123  520.1123   4.228555
## Total                      140 1447.7872  10.341337
## 
## $componentsofcovariance
##                                             Sigma          %
## Variations  Between Pop                10.4131525  66.777680
## Variations  Between samples Within Pop  0.9520545   6.105355
## Variations  Within samples              4.2285550  27.116965
## Total variations                       15.5937620 100.000000
## 
## $statphi
##                         Phi
## Phi-samples-total 0.7288303
## Phi-samples-Pop   0.1837727
## Phi-Pop-total     0.6677768
The first thing to look at are the $results element. The degrees of freedom (the Df column) should match what we expect from our (not clone-corrected) data. The number in the Total row should equal 186 or N−1N−1, where values are calculated from pooled data. Note that here, “samples” actually refers to subpopulations since we cannot asses within-sample variance of dominant data.

The $componentsofcovariance table shows how much variance is detected at each stratification. We expect variations within samples to give the greatest amount of variation for populations that are not significantly differentiated. Sigma represents the variance, σσ, for each hierarchical level and to the right is the percent of the total.

Finally, $statphi provides the ϕϕ population differentiation statistics. These are used to test hypotheses about population differentiation. We would expect a higher ϕϕ statistic to represent a higher amount of differentiation.

Note, if you want to make a table of any of these components, you can isolate them by using the $ operator and then export it to a table with  write.table. Here’s an example with the components of covariance:

write.table(Aeutamova$componentsofcovariance, sep = ",", file = "~/Documents/AeutiechesAMOVA.csv")
Significance testing
To test if populations are significantly different, we perform a randomization test using the function randtest() from the ade4 package. This will randomly permute the sample matrices as described in (Excoffier et al., 1992).

set.seed(1999)
Aeutsignif   <- randtest(Aeutamova, nrepet = 999)
Aeutccsignif <- randtest(Aeutamovacc, nrepet = 999)
plot(Aeutsignif)


Aeutsignif
## class: krandtest lightkrandtest 
## Monte-Carlo tests
## Call: randtest.amova(xtest = Aeutamova, nrepet = 999)
## 
## Number of tests:   3 
## 
## Adjustment method for multiple comparisons:   none 
## Permutation number:   999 
##                         Test       Obs    Std.Obs   Alter Pvalue
## 1  Variations within samples  3.411277 -31.902575    less  0.001
## 2 Variations between samples  1.328667  20.986193 greater  0.001
## 3     Variations between Pop 11.063446   9.120263 greater  0.001
From this output, you can see three histograms representing the distribution of the randomized strata. The black line represents the observed data. You can see a table of observed results in the output showing that there is significant population structure considering all levels of the population strata. Of course, this could be due to the presence of clones, so let’s visualize the results from the clone corrected data set below:

plot(Aeutccsignif)


Aeutccsignif
## class: krandtest lightkrandtest 
## Monte-Carlo tests
## Call: randtest.amova(xtest = Aeutamovacc, nrepet = 999)
## 
## Number of tests:   3 
## 
## Adjustment method for multiple comparisons:   none 
## Permutation number:   999 
##                         Test        Obs    Std.Obs   Alter Pvalue
## 1  Variations within samples  4.2285550 -22.250873    less  0.001
## 2 Variations between samples  0.9520545   9.821081 greater  0.001
## 3     Variations between Pop 10.4131525   9.983940 greater  0.001
The above graphs show significant population differentiation at all levels given that the observed ϕϕ does not fall within the distribution expected from the permutation. Compare the results of our AMOVA analysis to those published in (Grünwald & Hoheisel, 2006). They should be identical.

Randomized population structure
Since AMOVA is used to detect whether or not there is significant population structure, we can see what happens when we randomly shuffle the population assignments in our data. Here we will show what the populations look like before and after shuffling:

Aeut.new <- Aeut
head(strata(Aeut)[, -1])
##        Pop Subpop
## 001 Athena      1
## 002 Athena      1
## 003 Athena      1
## 004 Athena      1
## 005 Athena      1
## 006 Athena      1
set.seed(9001)
head(strata(Aeut)[sample(nInd(Aeut)), -1])
##            Pop Subpop
## 044     Athena      4
## 177 Mt. Vernon      8
## 036     Athena      4
## 127 Mt. Vernon      4
## 008     Athena      1
## 039     Athena      4
Here we see that the populations are completely shuffled, so in the next step, we will reassign the strata with these newly shuffled populations and rerun the AMOVA analysis.

set.seed(9001)
strata(Aeut.new) <- strata(Aeut)[sample(nInd(Aeut)), -1]
Aeut.new.amova         <- poppr.amova(Aeut.new, ~Pop/Subpop)
Aeut.new.amova
## $call
## ade4::amova(samples = xtab, distances = xdist, structures = xstruct)
## 
## $results
##                             Df     Sum Sq   Mean Sq
## Between Pop                  1   23.20576 23.205765
## Between samples Within Pop  16  136.33757  8.521098
## Within samples             169 1741.65452 10.305648
## Total                      186 1901.19786 10.221494
## 
## $componentsofcovariance
##                                             Sigma          %
## Variations  Between Pop                 0.1588974   1.544009
## Variations  Between samples Within Pop -0.1733264  -1.684217
## Variations  Within samples             10.3056481 100.140207
## Total variations                       10.2912190 100.000000
## 
## $statphi
##                            Phi
## Phi-samples-total -0.001402074
## Phi-samples-Pop   -0.017106288
## Phi-Pop-total      0.015440091
Aeut.new.amova.test    <- randtest(Aeut.new.amova, nrepet = 999)
Aeut.new.amova.test
## class: krandtest lightkrandtest 
## Monte-Carlo tests
## Call: randtest.amova(xtest = Aeut.new.amova, nrepet = 999)
## 
## Number of tests:   3 
## 
## Adjustment method for multiple comparisons:   none 
## Permutation number:   999 
##                         Test        Obs    Std.Obs   Alter Pvalue
## 1  Variations within samples 10.3056481  0.3775001    less  0.605
## 2 Variations between samples -0.1733264 -0.7375843 greater  0.746
## 3     Variations between Pop  0.1588974  1.2656780 greater  0.123
plot(Aeut.new.amova.test)


We see that there now is no significant population structure.

Conclusions
AMOVA is a powerful tool that can help support hypotheses of population structure due to clonal reproduction or isolation without making assumptions about Hardy-Weinberg equilibrium. We have shown that we can reject the HoHo of random mating between the two populations and have strong evidence that these populations are significantly differentiated at all stratifications (Grünwald & Hoheisel, 2006). From these results, we can investigate hypotheses as to why these populations are significantly differentiated.

This example was performed with a data set of dominant (AFLP) markers, but it can also be performed on codominant markers such as SNPs or SSRs. These provide more information because within sample (individual) variance is also assessed. If one wants to utilize a genetic distance that has biological relevance, a different distance matrix can be chosen. See help('amova', package = 'poppr') for more details.

References
Excoffier L., Smouse PE., Quattro JM. 1992. Analysis of molecular variance inferred from metric distances among dNA haplotypes: Application to human mitochondrial dNA restriction data. Genetics 131:479–491. Available at: http://www.genetics.org/content/131/2/479.abstract

Grünwald NJ., Hoheisel G-A. 2006. Hierarchical analysis of diversity, selfing, and genetic differentiation in populations of the oomycete Aphanomyces euteiches. Phytopathology 96:1134–1141. Available at: http://apsjournals.apsnet.org/doi/abs/10.1094/PHYTO-96-1134

Population genetics and genomics in R
TABLE OF CONTENTS
PART I 
PART II 
PART III 
WORKSHOP 
ABOUT 
APPENDICES 
Discriminant analysis of principal components (DAPC)
NJ Grünwald, ZN Kamvar, and SE Everhart
Introduction
Often we want to infer population structure by determining the number of clusters (groups) observed without prior knowledge. Several approaches can be used to infer groups such as for example K-means clustering, Bayesian clustering using STRUCTURE, and multivariate methods such as Discriminant Analysis of Principal Components (DAPC) (Pritchard, Stephens & Donnelly, 2000; Jombart, Devillard & Balloux, 2010; Grünwald & Goss, 2011). A STRUCTURE-like approach assumes that markers are not linked and that populations are panmictic (Pritchard et al., 2000). To use model-free methods K-means clustering based on genetic distance or DAPC are more convenient approaches for populations that are clonal or partially clonal. Here we explore DAPC further.

DAPC analysis of the H3N2 influenza strains
DAPC was pioneered by Jombart and colleagues (Jombart et al., 2010) and can be used to infer the number of clusters of genetically related individuals. In this multivariate statistical approach variance in the sample is partitioned into a between-group and within- group component, in an effort to maximize discrimination between groups. In DAPC, data is first transformed using a principal components analysis (PCA) and subsequently clusters are identified using discriminant analysis (DA). This tutorial is based on the vignette written by Thibaut Jombart. We encourage the user to explore this vignette further. The vignette can also be opened within R by executing adegenetTutorial("dapc").

We will use the seasonal influenza dataset H3N2 data containing 1903 isolates genotyped for 125 SNPs located in the hemagglutinin segment. This dataset as well as the dapc() function is part of the adegenet package.

# DAPC requires the adegenet package. Let's load this package:
library("adegenet")
data(H3N2) # load the H3N2 influenza data. Type ?H3N2 for more info.
pop(H3N2) <- H3N2$other$epid
dapc.H3N2 <- dapc(H3N2, var.contrib = TRUE, scale = FALSE, n.pca = 30, n.da = nPop(H3N2) - 1)
scatter(dapc.H3N2, cell = 0, pch = 18:23, cstar = 0, mstree = TRUE, lwd = 2, lty = 2)


The dapc() arguments we used refer to:

the dataset H3N2
var.contrib this is set to TRUE, meaning that we want to retain the variables contributing to the analysis in our output. We will use this later to see which loci are responsible for separating populations.
center this is set to FALSE, indicating that we do not want the data to be rescaled so the mean = 0.
n.pca is the number of axes retained in the Principal Component Analysis (PCA). By default, it is set to NULL.
n.da is the number of axes retained in the Discriminant Analysis (DA). By default, it is set to NULL.
It is important to set n.pca = NULL when you analyze your data because the number of principal components retained has a large effect on the outcome of the data. See the section below for a statistical method called cross- validation as an aid for choosing n.pca
The scatter() function is part of the ade4 package and plots results of a DAPC analysis.

As you can see, each year between 2001 to 2005 is a cluster of H3N2 strains separated by axis 1. In contrast, axis 2 separates the strains observed in the 2006 cluster from the clusters observed during 2001-5, indicating that the strains observed in 2006 are genetically distinct.

Next, let’s assess if there are alleles that most differentiate the 2006 cluster from those in other years.

set.seed(4)
contrib <- loadingplot(dapc.H3N2$var.contr, axis = 2, thres = 0.07, lab.jitter = 1)


It looks like SNPs at position 384 and 906 are involved. Let’s check this further by looking at allele frequencies by year:

temp    <- seploc(H3N2)       # seploc {adegenet} creates a list of individual loci.
snp906  <- tab(temp[["906"]]) # tab {adegenet} returns a matrix of genotypes
snp399  <- tab(temp[["399"]])

# The following two commands find the average allele frequencies per population
(freq906 <- apply(snp906, 2, function(e) tapply(e, pop(H3N2), mean, na.rm = TRUE)))
##            906.c     906.t
## 2001 0.000000000 1.0000000
## 2002 0.000000000 1.0000000
## 2003 0.000000000 1.0000000
## 2004 0.000000000 1.0000000
## 2005 0.002155172 0.9978448
## 2006 0.616071429 0.3839286
(freq399 <- apply(snp399, 2, function(e) tapply(e, pop(H3N2), mean, na.rm = TRUE)))
##            399.c     399.t
## 2001 0.000000000 1.0000000
## 2002 0.000000000 1.0000000
## 2003 0.000000000 1.0000000
## 2004 0.001848429 0.9981516
## 2005 0.002079002 0.9979210
## 2006 0.357142857 0.6428571
Note that a new allele appeared in 2005 for SNP locus 906 and 2004 for locus 399 separating populations along axis 2.

# First, set the plotting parameters
# mfrow = number of columns, rows
# mar   = plot margin size
# las   = axis label style (3: always vertical)
par(mfrow = c(1, 2), mar = c(5, 4, 4, 0) + 0.1, las = 3)

matplot(freq906,  pch = c("c", "t"), type = "b",
        xlab = "year", ylab = "allele frequency", main = "SNP # 906",
        xaxt = "n", cex = 1.5)
axis(side = 1, at = 1:6, lab = 2001:2006)

matplot(freq399, pch = c("c", "t"), type = "b",
        xlab = "year", ylab = "allele frequency", main = "SNP #399",
        xaxt = "n", cex = 1.5)
axis(side = 1, at = 1:6, lab = 2001:2006)


# Now we reset the plotting parameters to default
par(mfrow = c(1, 1), mar = c(5, 4, 4, 2) + 0.1, las = 0)
This plot nicely illustrates the effect of mutation, followed by selection or drift in the seasonal H3N2 influenza virus.

Cross validation: DAPC analysis of Phytophthora ramorum from forests and nurseries
Above we showed a nice example of a story that shows how two loci can drastically influence an epidemic. Next we will spend some time establishing what the appropriate number of principal components (PC) is for the analysis. It is important to carefully choose the correct number of PCs so as to include most sources of variation explained by an appropriate number of PCs retained. One way of ensuring that you have selected the correct number of PCs is to do cross-validation. This is a procedure in which you leave out a certain percentage of your data, run DAPC, and then see if the data that was left out is correctly placed into the population.

Since the H3N2 data set is quite large, we will use Phytophthora ramorum data from nurseries in California and Oregon (Goss et al., 2009) and forests in Curry County, Oregon (Kamvar et al., 2015). These data represent the Sudden Oak Death epidemic in Curry County, OR from 2001-2014 separated into different watershed regions. In Kamvar et al. (2015), the “Hunter Creek (HunterCr)” population was shown to be the result of a second introduction, likely from nurseries. Part of the evidence to support this conclusion came from DAPC results. Here, we will recreate the process of cross validation and reporting.

If we run the function xvalDapc() with default parameters, it will run 30 replicates of cross-validation for a number of PCs less than the total number of alleles in the data. This is a good way to get an idea of where to focus more intense cross-validation runs:

By default xvalDapc() needs two parameters: 1. The genotype matrix, 2. The population factors.
library("poppr")
data("Pram", package = "poppr")
Pram
## 
## This is a genclone object
## -------------------------
## Genotype information:
## 
##     98 multilocus genotypes 
##    729 diploid individuals
##      5 codominant loci
## 
## Population information:
## 
##      3 strata - SOURCE, YEAR, STATE
##      9 populations defined - 
## Nursery_CA, Nursery_OR, JHallCr_OR, ..., Winchuck_OR, ChetcoMain_OR, PistolRSF_OR
set.seed(999)
pramx <- xvalDapc(tab(Pram, NA.method = "mean"), pop(Pram))


You can see that we have a peak around 15 PC. From here, we can narrow the search by specifying the number of PC to try with n.pca and centering it around 15, and doing 1000 replicates each (Note, this will take a long time).

Windows users: change parallel to “snow”.
set.seed(999)
system.time(pramx <- xvalDapc(tab(Pram, NA.method = "mean"), pop(Pram),
                             n.pca = 10:20, n.rep = 1000,
                             parallel = "multicore", ncpus = 4L))


##    user  system elapsed 
## 109.964   4.516  33.861
We can see that it’s basically a flat line all the way. If we take a look at the object, we see that 16 PCs give us the highest percent of correctly predicted subsamples with the lowest error.

names(pramx) # The first element are all the samples
## [1] "Cross-Validation Results"                          
## [2] "Median and Confidence Interval for Random Chance"  
## [3] "Mean Successful Assignment by Number of PCs of PCA"
## [4] "Number of PCs Achieving Highest Mean Success"      
## [5] "Root Mean Squared Error by Number of PCs of PCA"   
## [6] "Number of PCs Achieving Lowest MSE"                
## [7] "DAPC"
pramx[-1]
## $`Median and Confidence Interval for Random Chance`
##       2.5%        50%      97.5% 
## 0.08877822 0.11112549 0.13205190 
## 
## $`Mean Successful Assignment by Number of PCs of PCA`
##        10        11        12        13        14        15        16 
## 0.6476968 0.6461921 0.6444716 0.6471279 0.6575571 0.6681401 0.6721343 
##        17        18        19        20 
## 0.6716012 0.6713644 0.6725124 0.6674652 
## 
## $`Number of PCs Achieving Highest Mean Success`
## [1] "19"
## 
## $`Root Mean Squared Error by Number of PCs of PCA`
##        10        11        12        13        14        15        16 
## 0.3541510 0.3556648 0.3575714 0.3547137 0.3443211 0.3337825 0.3297427 
##        17        18        19        20 
## 0.3304602 0.3304979 0.3294658 0.3344430 
## 
## $`Number of PCs Achieving Lowest MSE`
## [1] "19"
## 
## $DAPC
##  #################################################
##  # Discriminant Analysis of Principal Components #
##  #################################################
## class: dapc
## $call: dapc.data.frame(x = as.data.frame(x), grp = ..1, n.pca = ..2, 
##     n.da = ..3)
## 
## $n.pca: 19 first PCs of PCA used
## $n.da: 8 discriminant functions saved
## $var (proportion of conserved variance): 0.989
## 
## $eig (eigenvalues): 890.7 214.8 99.55 58.37 49.21 ...
## 
##   vector    length content                   
## 1 $eig      8      eigenvalues               
## 2 $grp      729    prior group assignment    
## 3 $prior    9      prior group probabilities 
## 4 $assign   729    posterior group assignment
## 5 $pca.cent 38     centring vector of PCA    
## 6 $pca.norm 38     scaling vector of PCA     
## 7 $pca.eig  33     eigenvalues of PCA        
## 
##   data.frame    nrow ncol
## 1 $tab          729  19  
## 2 $means        9    19  
## 3 $loadings     19   8   
## 4 $ind.coord    729  8   
## 5 $grp.coord    9    8   
## 6 $posterior    729  9   
## 7 $pca.loadings 38   19  
## 8 $var.contr    38   8   
##   content                                          
## 1 retained PCs of PCA                              
## 2 group means                                      
## 3 loadings of variables                            
## 4 coordinates of individuals (principal components)
## 5 coordinates of groups                            
## 6 posterior membership probabilities               
## 7 PCA loadings of original variables               
## 8 contribution of original variables
We also have a DAPC object that we can plot comparable to figure 4 in Kamvar et al. (2015):

scatter(pramx$DAPC, col = other(Pram)$comparePal, cex = 2, legend = TRUE,
        clabel = FALSE, posi.leg = "bottomleft", scree.pca = TRUE,
        posi.pca = "topleft", cleg = 0.75, xax = 1, yax = 2, inset.solid = 1)


We can see that this shows the clear separation of Hunter Creek from the rest of the epidemic, providing evidence that this population arose from a separate introduction.

Conclusions
DAPC is a wonderful tool for exploring structure of populations based on PCA and DA without making assumptions of panmixia. Thus, this technique provides a robust alternative to Bayesian clustering methods like STRUCTURE (Pritchard et al., 2000) that should not be used for clonal or partially clonal populations.

DAPC analysis is inherently interactive and cannot be scripted a priori. Please refer to the vignette written by Thibaut Jombart for a more interactive analysis.

References
Goss EM., Larsen M., Chastagner GA., Givens DR., Grünwald NJ. 2009. Population genetic analysis infers migration pathways of phytophthora ramorum in uS nurseries. PLoS Pathog 5:e1000583. Available at: http://dx.doi.org/10.1371/journal.ppat.1000583

Grünwald NJ., Goss EM. 2011. Evolution and population genetics of exotic and re-emerging pathogens: Novel tools and approaches. Annual Review of Phytopathology 49:249–267. Available at: http://www.annualreviews.org/doi/abs/10.1146/annurev-phyto-072910-095246?journalCode=phyto

Jombart T., Devillard S., Balloux F. 2010. Discriminant analysis of principal components: A new method for the analysis of genetically structured populations. BMC genetics 11:94. Available at: http://www.biomedcentral.com/1471-2156/11/94

Kamvar Z., Larsen M., Kanaskie A., Hansen E., Grünwald N. 2015. Spatial and temporal analysis of populations of the sudden oak death pathogen in oregon forests. Phytopathology 105:982–989. Available at: http://dx.doi.org/10.1094/PHYTO-12-14-0350-FI

Pritchard JK., Stephens M., Donnelly P. 2000. Inference of population structure using multilocus genotype data. Genetics 155:945–959. Available at: http://www.genetics.org/content/155/2/945.abstract

Population genetics and genomics in R
TABLE OF CONTENTS
PART I 
PART II 
PART III 
WORKSHOP 
ABOUT 
APPENDICES 
Population genomics based on high throughut sequencing (HTS)
NJ Grünwald, BJ Knaus, and JF Tabima
We now live in the fast growing era of high throughput sequencing (HTS) that is revolutionizing our ability to understand genetic variation (Luikart et al., 2003; Grünwald, McDonald & Milgroom, 2016). Two factors are contributing to a need for new methods of analyzing data: 1. the data is now often in a genome-wide context where location within a genome is part of the analysis and 2. the number of variants are large.

Novel tools in R for population genomic analyses
The R computing language has become a great tool for analyzing population genomic data. A recent special issue in Molecular Ecology Resources provides a nice overview of the arsenal of tools available in R (Paradis et al., 2017). New tools have become available in R for analyzing HTS data including adegenet (Jombart, 2008), ape (Paradis, Claude & Strimmer, 2004), vcfR (Knaus & Grünwald, 2017), and poppr (Kamvar, Tabima & Grünwald, 2014; Kamvar, Brooks & Grünwald, 2015) among others. Section III of this primer is geared towards analyzing whole genome or reduced representation genomic data for populations using the variant call format (VCF). The next three chapters will focus on introducing the VCF file format, reading SNP data into R from high throughput sequencing projects, performing quality control, and conducting selected analyses using population genomic data.

References
Grünwald NJ., McDonald BA., Milgroom MG. 2016. Population genomics of fungal and oomycete pathogens. Annual Review of Phytopathology 54:323–346. Available at: http://arjournals.annualreviews.org/doi/full/10.1146/annurev-phyto-080614-115913

Jombart T. 2008. adegenetadegenet: A R package for the multivariate analysis of genetic markers. Bioinformatics 24:1403–1405. Available at: https://doi.org/10.1093/bioinformatics/btn129

Kamvar ZN., Brooks JC., Grünwald NJ. 2015. Novel R tools for analysis of genome-wide population genetic data with emphasis on clonality. Name: Frontiers in Genetics 6:208. Available at: http://dx.doi.org/10.3389/fgene.2015.00208

Kamvar ZN., Tabima JF., Grünwald NJ. 2014. PopprPoppr: An R package for genetic analysis of populations with clonal, partially clonal, and/or sexual reproduction. PeerJ 2:e281. Available at: http://dx.doi.org/10.7717/peerj.281

Knaus BJ., Grünwald NJ. 2017. VcfrVcfr: A package to manipulate and visualize variant call format data in R. Molecular Ecology Resources 17:44–53. Available at: http://dx.doi.org/10.1111/1755-0998.12549

Luikart G., England PR., Tallmon D., Jordan S., Taberlet P. 2003. The power and promise of population genomics: From genotyping to genome typing. Nature reviews genetics 4:981–994. Available at: http://www.nature.com/nrg/journal/v4/n12/full/nrg1226.html

Paradis E., Gosselin T., Grünwald NJ., Jombart T., Manel S., Lapp H. 2017. Towards an integrated ecosystem of r packages for the analysis of population genetic data. Molecular Ecology Resources 17:1–4. Available at: http://dx.doi.org/10.1111/1755-0998.12636

Paradis E., Claude J., Strimmer K. 2004. APE: Analyses of phylogenetics and evolution in r language. Bioinformatics 20:289–290. Available at: https://doi.org/10.1093/bioinformatics/btg412

Population genetics and genomics in R
TABLE OF CONTENTS
PART I 
PART II 
PART III 
WORKSHOP 
ABOUT 
APPENDICES 
Reading VCF data
BJ Knaus, JF Tabima and NJ Grünwald
Genetic variation data is typically stored in variant call format (VCF) files (Danecek et al., 2011). This format is the preferred file format obtained from genome sequencing or high throughput genotyping. One advantage of using VCF files is that only variants (e.g., SNPs, indels, etc.) are reported which economizes files size relative to a format that may included invariant sites. Variant callers typically attempt to aggressively call variants with the perspective that a downstream quality control step will remove low quality variants. Note that VCF files come in different flavors and that each variant caller may report a slightly different information. A first step in working with this data is to understand their contents.

VCF file structure
A VCF file can be thought of as having three sections: a vcf header, a fix region and a gt region. The VCF meta region is located at the top of the file and contains meta-data describing the body of the file. Each VCF meta line begins with a ‘##’. The information in the meta region defines the abbreviations used elsewhere in the file. It may also document software used to create the file as well as parameters used by this software. Below the metadata region, the data are tabular. The first eight columns of this table contain information about each variant. This data may be common over all variants, such as its chromosomal position, or a summary over all samples, such as quality metrics. These data are fixed, or the same, over all samples. The fix region is required in a VCF file, subsequent columns are optional but are common in our experience. Beginning at column ten is a column for every sample. The values in these columns are information for each sample and each variant. The organization of each cell containing a genotype and associated information is specified in column nine, the FORMAT column. The location of these three regions within a file can be represented by this cartoon:

Cartoon representation of VCF file organization
Cartoon representation of VCF file organization

The VCF file specification is flexible. This means that there are slots for certain types of data, but any particular software which creates a VCF file does not necessarily use them all. Similarly, authors have the opportunity to include new forms of data, forms which may not have been foreseen by the authors of the VCF specification. The result is that all VCF files do not contain the same information.

For this example, we will use example data provided with the R package vcfR (Knaus & Grünwald, 2017).

library(vcfR)
data(vcfR_example)
vcf
## ***** Object of Class vcfR *****
## 18 samples
## 1 CHROMs
## 2,533 variants
## Object size: 2.9 Mb
## 8.497 percent missing data
## *****        *****         *****
The function library() loads libraries, in this case the package vcfR. The function data() loads datasets that were included with R and its packages. Our usage of data() loads the objects ‘gff’, ‘dna’ and ‘vcf’ from the ‘vcfR_example’ dataset. Here we’re only interested in the object ‘vcf’ which contains example VCF data. When we call the object name with no function it invokes the ‘show’ method which prints some summary information to the console.

The meta region
The meta region contains information about the file, its creation, as well as information to interpret abbreviations used elsewhere in the file. Each line of the meta region begins with a double pound sign (‘##’). The example which comes with vcfR is shown below. (Only the first seven lines are shown for brevity.)

strwrap(vcf@meta[1:7])
##  [1] "##fileformat=VCFv4.1"                                              
##  [2] "##source=\"GATK haplotype Caller, phased with beagle4\""           
##  [3] "##FILTER=<ID=LowQual,Description=\"Low quality\">"                 
##  [4] "##FORMAT=<ID=AD,Number=.,Type=Integer,Description=\"Allelic depths"
##  [5] "for the ref and alt alleles in the order listed\">"                
##  [6] "##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Approximate"   
##  [7] "read depth (reads with MQ=255 or with bad mates are filtered)\">"  
##  [8] "##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Genotype"      
##  [9] "Quality\">"                                                        
## [10] "##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">"
The first line contains the version of the VCF format used in the file. This line is required. The second line specifies the software which created the VCF file. This is not required, so not all VCF files include it. When they do, the file becomes self documenting. Note that the alignment software is not included here because it was used upstream of the VCF file’s creation (aligners typically create *.SAM or *.BAM format files). Because the file can only include information about the software that created it, the entire pipeline does not get documented. Some VCF files may contain a line for every chromosome (or supercontig or contig depending on your genome), so they may become rather long. Here, the remaining lines contain INFO and FORMAT specifications which define abbreviations used in the fix and gt portions of the file.

The meta region may include long lines that may not be easy to view. In vcfR we’ve created a function to help press this data.

queryMETA(vcf)
##  [1] "FILTER=ID=LowQual"                   
##  [2] "FORMAT=ID=AD"                        
##  [3] "FORMAT=ID=DP"                        
##  [4] "FORMAT=ID=GQ"                        
##  [5] "FORMAT=ID=GT"                        
##  [6] "FORMAT=ID=PL"                        
##  [7] "GATKCommandLine=ID=HaplotypeCaller"  
##  [8] "INFO=ID=AC"                          
##  [9] "INFO=ID=AF"                          
## [10] "INFO=ID=AN"                          
## [11] "INFO=ID=BaseQRankSum"                
## [12] "INFO=ID=ClippingRankSum"             
## [13] "INFO=ID=DP"                          
## [14] "INFO=ID=DS"                          
## [15] "INFO=ID=FS"                          
## [16] "INFO=ID=HaplotypeScore"              
## [17] "INFO=ID=InbreedingCoeff"             
## [18] "INFO=ID=MLEAC"                       
## [19] "INFO=ID=MLEAF"                       
## [20] "INFO=ID=MQ"                          
## [21] "INFO=ID=MQ0"                         
## [22] "INFO=ID=MQRankSum"                   
## [23] "INFO=ID=QD"                          
## [24] "INFO=ID=ReadPosRankSum"              
## [25] "INFO=ID=SOR"                         
## [26] "1 contig=<IDs omitted from queryMETA"
When the function queryMETA() is called with only a vcfR object as a parameter, it attempts to summarize the meta information. Not all of the information is returned. For example, ‘contig’ elements are not returned. This is an attempt to summarize information that may be most useful for comprehension of the file’s contents.

queryMETA(vcf, element = 'DP')
## [[1]]
## [1] "FORMAT=ID=DP"                                                                         
## [2] "Number=1"                                                                             
## [3] "Type=Integer"                                                                         
## [4] "Description=Approximate read depth (reads with MQ=255 or with bad mates are filtered)"
## 
## [[2]]
## [1] "INFO=ID=DP"                                                           
## [2] "Number=1"                                                             
## [3] "Type=Integer"                                                         
## [4] "Description=Approximate read depth; some reads may have been filtered"
When an element parameter is included, only the information about that element is returned. In this example the element ‘DP’ is returned. We see that this acronym is defined as both a ‘FORMAT’ and ‘INFO’ acronym. We can narrow down our query by including more information in the element parameter.

queryMETA(vcf, element = 'FORMAT=<ID=DP')
## [[1]]
## [1] "FORMAT=ID=DP"                                                                         
## [2] "Number=1"                                                                             
## [3] "Type=Integer"                                                                         
## [4] "Description=Approximate read depth (reads with MQ=255 or with bad mates are filtered)"
Here we’ve isolated the definition of ‘DP’ as a ‘FORMAT’ element. Note that the function queryMETA() includes the parameter nice which by default is TRUE and attempts to present the data in a nicely formatted manner. However, our query is performed on the actual information in the ‘meta’ region. It is therefore sometimes appropriate to set nice = FALSE so that we can see the raw data. In the above example the angled bracket (‘<’) is omitted from the nice = TRUE representation but is essential to distinguishing the ‘FORMAT’ element from the ‘INFO’ element.

The fix region
The fix region contains information for each variant which is sometimes summarized over all samples. The first eight columns of the fixed region are titled CHROM, POS, ID, REF, ALT, QUAL, FILTER and INFO. This is per variant information which is ‘fixed’, or the same, over all samples. The first two columns indicate the location of the variant by chromosome and position within that chromosome. Here, the ID field has not been used, so it consists of missing data (NA). The REF and ALT columns indicate the reference and alternate allelic states for a diploid sample. When multiple alternate allelic states are present they are delimited with commas. The QUAL column attempts to summarize the quality of each variant over all samples. The FILTER field is not used here but could contain information on whether a variant has passed some form of quality assessment.

head(getFIX(vcf))
##      CHROM              POS   ID REF ALT QUAL     FILTER
## [1,] "Supercontig_1.50" "2"   NA "T" "A" "44.44"  NA    
## [2,] "Supercontig_1.50" "246" NA "C" "G" "144.21" NA    
## [3,] "Supercontig_1.50" "549" NA "A" "C" "68.49"  NA    
## [4,] "Supercontig_1.50" "668" NA "G" "C" "108.07" NA    
## [5,] "Supercontig_1.50" "765" NA "A" "C" "92.78"  NA    
## [6,] "Supercontig_1.50" "780" NA "G" "T" "58.38"  NA
The eigth column, titled INFO, is a semicolon delimited list of information. It can be rather long and cumbersome. The function getFIX() will suppress this column by default. Each abbreviation in the INFO column should be defined in the meta section. We can validate this by querying the meta portion, as we did in the ‘meta’ section above.

The gt region
The gt (genotype) region contains information about each variant for each sample. The values for each variant and each sample are colon delimited. Multiple types of data for each genotype may be stored in this manner. The format of the data is specified by the FORMAT column (column nine). Here we see that we have information for GT, AD, DP, GQ and PL. The definition of these acronyms can be referenced by querying the the meta region, as demonstrated previously. Every variant does not necessarily have the same information (e.g., SNPs and indels may be handled differently), so the rows are best treated independently. Different variant callers may include different information in this region.

vcf@gt[1:6, 1:4]
##      FORMAT           BL2009P4_us23              
## [1,] "GT:AD:DP:GQ:PL" "0|0:62,0:62:99:0,190,2835"
## [2,] "GT:AD:DP:GQ:PL" "1|0:5,5:10:99:111,0,114"  
## [3,] "GT:AD:DP:GQ:PL" NA                         
## [4,] "GT:AD:DP:GQ:PL" "0|0:1,0:1:3:0,3,44"       
## [5,] "GT:AD:DP:GQ:PL" "0|0:2,0:2:6:0,6,49"       
## [6,] "GT:AD:DP:GQ:PL" "0|0:2,0:2:6:0,6,49"       
##      DDR7602                   IN2009T1_us22              
## [1,] "0|0:12,0:12:39:0,39,585" "0|0:37,0:37:99:0,114,1709"
## [2,] NA                        "0|1:2,1:3:16:16,0,48"     
## [3,] NA                        "0|0:2,0:2:6:0,6,51"       
## [4,] NA                        "1|1:0,1:1:3:25,3,0"       
## [5,] "0|0:1,0:1:3:0,3,34"      "0|0:1,0:1:3:0,3,31"       
## [6,] "0|0:1,0:1:3:0,3,34"      "0|0:3,0:3:9:0,9,85"
vcfR
Using the R package vcfR, we can read VCF format files into memory using the function read.vcfR(). Once in memory we can use the head() method to summarize the information in the three VCF regions.

vcf <- read.vcfR("pinfsc50_filtered.vcf.gz")
## File attributes:
##   meta lines: 29
##   header line: 30
##   variant count: 2190
##   column count: 27
## 
Meta line 29 read in.
## All meta lines processed.
## In function read_body_gz.
##   stats(0): 29
##   stats(1): 30
##   stats(2): 2190
##   stats(3): 27
## Initializing gt matrix.
##   nrows: 2190
##   cols.size(): 27
## gt matrix initialized.
## Character matrix gt created.
##   Character matrix gt rows: 2190
##   Character matrix gt cols: 27
##   skip: 0
##   nrows: 2190
##   row_num: 0
## 
## 
Processed variant 1000
Processed variant 2000
Processed variant: 2190
## All variants processed
head(vcf)
## [1] "***** Object of class 'vcfR' *****"
## [1] "***** Meta section *****"
## [1] "##fileformat=VCFv4.1"
## [1] "##source=\"GATK haplotype Caller, phased with beagle4\""
## [1] "##FILTER=<ID=LowQual,Description=\"Low quality\">"
## [1] "##FORMAT=<ID=AD,Number=.,Type=Integer,Description=\"Allelic depths fo [Truncated]"
## [1] "##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Approximate read  [Truncated]"
## [1] "##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\">"
## [1] "First 6 rows."
## [1] 
## [1] "***** Fixed section *****"
##      CHROM              POS     ID REF ALT       QUAL      FILTER
## [1,] "Supercontig_1.50" "80058" NA "T" "G,TACTG" "3480.23" NA    
## [2,] "Supercontig_1.50" "80063" NA "C" "T"       "3016.89" NA    
## [3,] "Supercontig_1.50" "80067" NA "A" "C"       "3555.08" NA    
## [4,] "Supercontig_1.50" "80073" NA "C" "A"       "104.72"  NA    
## [5,] "Supercontig_1.50" "80074" NA "A" "G"       "2877.74" NA    
## [6,] "Supercontig_1.50" "80089" NA "A" "ACG"     "2250.92" NA    
## [1] 
## [1] "***** Genotype section *****"
##      FORMAT           BL2009P4_us23                             
## [1,] "GT:AD:DP:GQ:PL" "1|0:25,3,0:28:45:45,0,1120,129,1134,1300"
## [2,] "GT:AD:DP:GQ:PL" "1|0:29,3:32:30:30,0,1335"                
## [3,] "GT:AD:DP:GQ:PL" "1|0:31,3:34:27:27,0,1372"                
## [4,] "GT:AD:DP:GQ:PL" "0|0:30,0:30:99:0,102,1530"               
## [5,] "GT:AD:DP:GQ:PL" "0|0:30,0:30:93:0,93,1395"                
## [6,] "GT:AD:DP:GQ:PL" "0|0:33,0:33:99:0,99,1485"                
##      DDR7602                                  
## [1,] "1|0:19,7,0:26:99:237,0,777,300,804,1181"
## [2,] "1|0:20,7:27:99:234,0,819"               
## [3,] "1|0:19,6:25:99:189,0,864"               
## [4,] "0|0:26,0:26:87:0,87,1305"               
## [5,] "1|0:21,4:25:99:147,0,867"               
## [6,] "1|0:20,2:22:18:18,0,918"                
##      IN2009T1_us22                              
## [1,] "0|1:29,6,0:35:99:162,0,1229,252,1252,1512"
## [2,] "0|1:27,7:34:99:204,0,1232"                
## [3,] "0|1:27,6:33:99:210,0,1155"                
## [4,] "0|0:33,0:33:99:0,99,1485"                 
## [5,] "0|1:27,6:33:99:171,0,1116"                
## [6,] "0|1:27,7:34:99:213,0,1113"                
##      LBUS5                                    
## [1,] "0|1:19,7,0:26:99:237,0,777,300,804,1181"
## [2,] "0|1:20,7:27:99:234,0,819"               
## [3,] "0|1:19,6:25:99:189,0,864"               
## [4,] "0|0:26,0:26:87:0,87,1305"               
## [5,] "0|1:21,4:25:99:147,0,867"               
## [6,] "0|1:20,2:22:18:18,0,918"                
##      NL07434                                     
## [1,] "0|1:45,19,0:64:99:643,0,1782,793,1866,2825"
## [2,] "0|1:42,18:60:99:655,0,1748"                
## [3,] "0|1:41,16:57:99:584,0,1737"                
## [4,] "0|0:56,0:56:99:0,172,2565"                 
## [5,] "0|1:39,16:55:99:629,0,1709"                
## [6,] "0|1:34,12:46:99:393,0,1518"                
## [1] "First 6 columns only."
## [1] 
## [1] "Unique GT formats:"
## [1] "GT:AD:DP:GQ:PL"
## [1]
After we have made any manipulations of the file we can save it as a VCF file with the function write.vcf().

write.vcf(vcf, "myVCFdata_filtered.vcf.gz")
write.vcf()will write a file to your active directory. We now have a summary of our VCF file which we can use to help understand what forms of information are contained within it. This information can be further explored with plotting functions and used to filter the VCF file for high quality variants as we will see in the next section.

References
Danecek P., Auton A., Abecasis G., Albers CA., Banks E., DePristo MA., Handsaker RE., Lunter G., Marth GT., Sherry ST. et al. 2011. The variant call format and VCFtools. Bioinformatics 27:2156–2158. Available at: https://doi.org/10.1093/bioinformatics/btr330

Knaus BJ., Grünwald NJ. 2017. VcfrVcfr: A package to manipulate and visualize variant call format data in R. Molecular Ecology Resources 17:44–53. Available at: http://dx.doi.org/10.1111/1755-0998.12549

Population genetics and genomics in R
TABLE OF CONTENTS
PART I 
PART II 
PART III 
WORKSHOP 
ABOUT 
APPENDICES 
Quality control
BJ Knaus, JF Tabima and NJ Grünwald
Quality control of data
All data sets are not perfect. A critical step in and research project is to ensure the quality of the data and to develop mitigatation strategies to handle low quality data. A favorite statistics text poses the question ‘garbage in, roses out?’ (Tabachnick, Fidell & Osterlind, 2001). While we all know the answer to this question, when we are presented with a data and are anxious to see what answers our analyses may present us with, its frequently easy to forget to ask this critical question. Many of the existing variant callers state that they aggressively call variants with the hope that these variants are filtered for quality. In this section we discuss methods to characterize quality issues and propose methods to handle these issues.

Missing data
As the size of our dataset grow, in terms of samples and variants, the size of our data matrix grows. As the size of our data matrix grows, it also increases the opportunity to have missing data. Also, some of our quality filtering steps increased the degree of missingness in our data matrix by setting values that we determined to be of unusual quality to NA. One way of managing missing data is to use imputation, a set of methods that attempts to infer what the most likely genotype should be and replaces the missing genotype with the interpolated genotype. However, if your data has a large degree of missingness you may want to attempt to mitigate missingness instead of interpolation Or you may want to implement a mitigation step prior to interpolation in the hope that this will improve the performance of the interpolation. Missing data can frequently be due to samples (columns) or variants (rows) of low quality. Here we demonstrate how to identify samples and variants in the data set that have a high degree of missingness. In another section we’ll show how to omit them.

Overall missingness
A first perspective on how complete our dataset is can be provided by the show method for the vcfR object. When you invoke the name of an object with no arguments it invokes the show method.

vcf
## ***** Object of Class vcfR *****
## 18 samples
## 1 CHROMs
## 2,533 variants
## Object size: 2.9 Mb
## 8.497 percent missing data
## *****        *****         *****
The show method for an object typically reports a summary of what is contained in the object. Here we see the number of samples and variants in our data. We also see a report of what the percentage of missing data is in our object. In the context of vcfR this is the proportion of variants scored as NA. Note that if a variant includes some data associated with a missing genotype it will not be recognized as missing. For example, a missing genotype could be associated with a depth information as follows.

GT:DP ./.:1
Because this variant does include some data it will not be recognized as missing until the genotypes are extracted and queried for missingness. This means that the degree of missingness reported by the show method may be an under representation. It does provide an easily accessed first perspective on the proportion of missing data. To determine the cause of this missing data (e.g., are there particular samples or variants of poor quality) we will look further.

Quantifying missingness, one sample
To quantify missingness for a single sample we can use the function is.na(). This function uses a vector as an argument and returns a logical vector (TRUE and FALSE) indicating which values are NA. If we remind ourselves that TRUEs and FALSEs are numerically encoded as 1 and 0 it reminds us we can take a sum of this logical vector to determine the degree of missingness.

as.numeric(c(FALSE, TRUE))
## [1] 0 1
sum(as.numeric(c(FALSE, TRUE)))
## [1] 1
Now that we’ve reminded ourselves of how to count missing values we can apply this knowledge to query one of our samples. We’ll extract a matrix of variant depths (DP) from the VCF data. We can remind ourselves of what DP is in this VCF data by using the queryMETA() function. (Recall that we have already read in our VCF data to create a vcfR object in the section ‘Reading VCF data’.)

queryMETA(vcf, "DP")
## [[1]]
## [1] "FORMAT=ID=DP"                                                                         
## [2] "Number=1"                                                                             
## [3] "Type=Integer"                                                                         
## [4] "Description=Approximate read depth (reads with MQ=255 or with bad mates are filtered)"
## 
## [[2]]
## [1] "INFO=ID=DP"                                                           
## [2] "Number=1"                                                             
## [3] "Type=Integer"                                                         
## [4] "Description=Approximate read depth; some reads may have been filtered"
Here we’ll want the first definition (FORMAT=ID=DP). Now we can extract a matrix of depths and query the first sample for missingness.

dp <- extract.gt(vcf, element = "DP", as.numeric=TRUE)
sum(is.na(dp[,1]))
## [1] 41
This reports the number of missing variants in the first sample. We could similarly count the number of missing samples from a variant by accessing a row instead of a column. We could also convert this to a percentage by using length() to determine the total number of values in either the column or row and use this as a denominator.

Quantifying missingness, all samples
This is illustrative of what we can accomplish for a single sample or variant. We typically have many samples an tens of thousand (or more) variants. We can extend the functionality of the above example to many columns or rows by using the apply() function. See the section on apply if you are unfamiliar with this function. Because we will be summarizing many samples we will use a barplot to visualize the results as opposed to trying to scrutinize the numerical information.

myMiss <- apply(dp, MARGIN = 2, function(x){ sum(is.na(x)) })
myMiss <- myMiss/nrow(vcf)

library(RColorBrewer)
palette(brewer.pal(n=12, name = 'Set3'))

par(mar = c(12,4,4,2))
barplot(myMiss, las = 2, col = 1:12)
title(ylab = "Missingness (%)")


par(mar = c(5,4,4,2))
This allows us to visualize the degree of missingness on a per sample basis. We see that the sample P7272 has a particularly high amount of missing genotypes. This is because this sample is a different species than the reference it was mapped to. One decision could be to omit this sample. However, if the goal is to make comparisons among these species we may instead search for variants that are present in both taxa.

We can do something similar to query the variants (rows) for missingness. However, when we have a large number of variants we wouldn’t want to visualize this with a barchart. It would require a barchart with 60 thousand bars. Instead of using a barchart we’ll use a histogram.

myMiss <- apply(dp, MARGIN = 1, function(x){ sum(is.na(x)) })
myMiss <- myMiss/ncol(vcf@gt[,-1])

hist(myMiss, col = "#8DD3C7", xlab = "Missingness (%)", main = "")


We’ve now seen how we can create summaries of our data matrix over both rows and columns. Once we have this knowledge in hand we may develop a plan for managing this data.

Sequence depth
Once a sequencing run is complete the data are typically mapped to a reference genome, variants are called and these variant are stored in a VCF file. At this point one of the first questions asked is how well did the sequencing go? One measure of sequencing quality is sequence depth or how many times each position was sequenced. In VCF data only information on the variable positions are reported. But this can be used as a convenient subset of an entire genome. Many variant callers report this information, but not all. Make sure to check your variant caller documentation if you do not see this information in your file, there is a chance it was an option that was not selected. Here we visualize the depth information from a VCF file to provide a perspective on sequence quality.

We can extract the depth data (DP) with the function extract.gt(). This results in a matrix of depth data. We can take a quick peak at this to remind us what it looks like.

library(vcfR)
#vcf <- read.vcfR('TASSEL_GBS0077.vcf.gz')
data(vcfR_example)
dp <- extract.gt(vcf, element = "DP", as.numeric=TRUE)
dp[1:4,1:5]
##                      BL2009P4_us23 DDR7602 IN2009T1_us22 LBUS5 NL07434
## Supercontig_1.50_2              62      12            37    12      NA
## Supercontig_1.50_246            10      NA             3    NA      NA
## Supercontig_1.50_549            NA      NA             2    NA      NA
## Supercontig_1.50_668             1      NA             1    NA       1
We see that it is a large matrix containing lots of numbers, so viewing it directly is of little use. Here we’ll demonstrate the use of barplots and violin plots to visualize this data.

Base barplot
Base R includes a great selection of graphical tools. One is barplot(). A few things nice about this function is that it is designed to work on matrices, such as the one we have, and that it is quick to execute.

par(mar=c(12,4,4,2))
boxplot(dp, col=2:8, las=3)
title(ylab = "Depth (DP)")


par(mar=c(5,4,4,2))
We’ve generated a box and whisker plot for each sample where 50% of the data is contained within each colored box and outliers are presented as circles. One issue with this plot is that all of the data seem squished at the bottom of the plot. A log transformation could help this plot. Remember that log of zero is undefined, so if you persue a log transformation you may need to handle these cases.

Violin plot
Violin plots are similar to boxplots except that they attempt to present the distribution of the data that would otherwise be represented by a box. We’ll neeed to load a few packages to add functions to help us. I think you’ll see that violin plots are a little more involved to create, and they are slower to render, but many find them to be more informative and more aesthetically pleasing.

library(reshape2)
library(ggplot2) 
library(cowplot)

# Melt our matrix into a long form data.frame.
dpf <- melt(dp, varnames=c('Index', 'Sample'), value.name = 'Depth', na.rm=TRUE)
dpf <- dpf[ dpf$Depth > 0,]

# Create a row designator.
# You may want to adjust this
#samps_per_row <- 20
samps_per_row <- 18
myRows <- ceiling(length(levels(dpf$Sample))/samps_per_row)
myList <- vector(mode = "list", length = myRows)

for(i in 1:myRows){
  myIndex <- c(i*samps_per_row - samps_per_row + 1):c(i*samps_per_row)
  myIndex <- myIndex[myIndex <= length(levels(dpf$Sample))]
  myLevels <- levels(dpf$Sample)[myIndex]
  myRegex <- paste(myLevels, collapse = "$|^")
  myRegex <- paste("^", myRegex, "$", sep = "")
  myList[[i]] <- dpf[grep(myRegex, dpf$Sample),]
  myList[[i]]$Sample <- factor(myList[[i]]$Sample)
}

# Create the plot.
myPlots <- vector(mode = "list", length = myRows)
for(i in 1:myRows){
  myPlots[[i]] <- ggplot(myList[[i]], aes(x=Sample, y=Depth)) + 
                  geom_violin(fill="#8dd3c7", adjust=1.0, scale = "count", trim=TRUE)

  myPlots[[i]] <- myPlots[[i]] + theme_bw()
  myPlots[[i]] <- myPlots[[i]] + theme(axis.title.x = element_blank(), 
                  axis.text.x = element_text(angle = 60, hjust = 1))
  myPlots[[i]] <- myPlots[[i]] + scale_y_continuous(trans=scales::log2_trans(), 
                  breaks=c(1, 10, 100, 800),
                  minor_breaks=c(1:10, 2:10*10, 2:8*100))
  myPlots[[i]] <- myPlots[[i]] + theme( panel.grid.major.y=element_line(color = "#A9A9A9", size=0.6) )
  myPlots[[i]] <- myPlots[[i]] + theme( panel.grid.minor.y=element_line(color = "#C0C0C0", size=0.2) )
}
# Plot the plot.
plot_grid(plotlist = myPlots, nrow = myRows)


We can see that the log transformation stretches out the smaller values and compresses the larger values. This allows us to focus on where our data has the greatest density. Things to look for in these plots are whether all samples are equally represented, whether there are a few samples of low quality that may need to be omitted from downstream analyses, or if therre are a small number of jackpot samples that received all the reads at the expense of the other samples. Here the samples appear to be fairly eqaully represented.

Censoring data
From the section where we created depth plots we see that there is a considerable amount of variation in depth within each sample. For example, if we sequenced a genome at 10X coverage we would expect most of our variants to be sequenced at this depth. Instead we see a range of values both exceptionally high and low. Variants sequenced at low coverage may only observe one of two alleles in a diploid. Because of this, we may want to omit variants of low coverage. Variants sequenced at high coverage may be from repetitive regions that were assembled in our reference as a single region. This means that different alleles may be from different copied (loci), so we may want to omit these. Here we’ll censor the variants that we do not feel are of ‘typical’ coverage. When we censor variants we’ll score them as missing (NA). By censoring instead of removing we preserve the geometry of the matrix. This helps facilitate multiple manipulations of the matrix if desired. Let’s begin by reminding us how abundant NAs are in our dataset.

vcf
## ***** Object of Class vcfR *****
## 18 samples
## 1 CHROMs
## 2,533 variants
## Object size: 2.9 Mb
## 8.497 percent missing data
## *****        *****         *****
A number of methods can be used to create intervals that you may consider acceptable. The use of quantiles may be considered desirable because they are non-parametric. We can fit different intervals to different samples using the function apply().

quants <- apply(dp, MARGIN=2, quantile, probs=c(0.1, 0.8), na.rm=TRUE)
quants[,1:6]
##     BL2009P4_us23 DDR7602 IN2009T1_us22 LBUS5 NL07434 P10127
## 10%            15       6            13     6      12      7
## 80%            35      40            32    40      51     24
We can create a second matrix of depths where we subtract the lower threshold of each sample from its depth using the function sweep(). Now all depths in the matrix that are below zero are below our threshold. We can use this information to set these cell to NA in the original matrix. We can similarly subtract the upper threshold from our samples to create a second matrix. Now everything above zero is above our threshold and can be set to NA.

dp2 <- sweep(dp, MARGIN=2, FUN = "-", quants[1,])
dp[dp2 < 0] <- NA

dp2 <- sweep(dp, MARGIN=2, FUN = "-", quants[2,])
dp[dp2 > 0] <- NA

dp[dp < 4] <- NA
Now that we know which cells we want to censor as NA we can use this information to update the vcfR object. Don’t forget that the first column of the gt matrix is the ‘FORMAT’ column. We can omit this from our selection by using -1.

vcf@gt[,-1][ is.na(dp) == TRUE ] <- NA
Now we can use the show method to see how this action has affected missingness in our vcfR object.

vcf
## ***** Object of Class vcfR *****
## 18 samples
## 1 CHROMs
## 2,533 variants
## Object size: 2.5 Mb
## 35.83 percent missing data
## *****        *****         *****
We’ve used depth information to censor variants that we feel are of unusual sequence depth. We’ve also used this information to update our vcfR object so that our data can remain in a constant format. This has resulted in a vcfR object with a greater degree of missing data. However, if our choice of which variants to censor has been good then then the remaining variants may be of a higher quality then the entire set we started with. We’ll deal with mitigating missing data in another section. A similar approach can be used if there are parameters other than depth that a researcher would like to filter on. We now have a vcfR object that has variants that we feel are of higher quality than our original file.

Omitting data
In the section on depth we learned how we can visualize variant depth, or any other numeric value provided in the gt portion of VCF data. In the section on censoring data we learned how to rescore variants that were outside our acceptance threshold as missing. And in the section on missing data we learned how to quantify and visualize missingness in our dataset. Here we put all of these skills together in order to omit samples and variants that have been set as missing (NA).

library(vcfR)
library(pinfsc50)
#data(vcfR_example)
vcf <- system.file("extdata", "pinf_sc50.vcf.gz", package = "pinfsc50")
vcf <- vcfR::read.vcfR(vcf)
dp <- extract.gt(vcf, element = "DP", as.numeric=TRUE)
vcf
## ***** Object of Class vcfR *****
## 18 samples
## 1 CHROMs
## 22,031 variants
## Object size: 20.9 Mb
## 7.929 percent missing data
## *****        *****         *****
Because part of this exercise involves setting cells in our data matrix as NA we should begin by reminding ourselves of how abundant they are. By using the show method we see that we have 7.93 percent missing data. We can now use what we learned previously to set variants that are outside our per sample inclusion threshold as NA.

quants <- apply(dp, MARGIN=2, quantile, probs=c(0.1, 0.9), na.rm=TRUE)
dp2 <- sweep(dp, MARGIN=2, FUN = "-", quants[1,])
dp[dp2 < 0] <- NA
dp2 <- sweep(dp, MARGIN=2, FUN = "-", quants[2,])
dp[dp2 > 0] <- NA
dp[dp < 4] <- NA
vcf@gt[,-1][ is.na(dp) == TRUE ] <- NA
vcf
## ***** Object of Class vcfR *****
## 18 samples
## 1 CHROMs
## 22,031 variants
## Object size: 19.2 Mb
## 27.7 percent missing data
## *****        *****         *****
We see that this censoring has increased the degree of missingness in our matrix to 27.7 percent. Ideally we should visualize the results of this action. For brevity, we will not here. But you can return to the section on depth and reuse the code presented there to visualize how this change has affected the distribution of the data. One way to visualize these data is to use a heatmap.

heatmap.bp(dp[1:1000,], rlabels = FALSE)


Omitting samples
We can see that some samples have a high degree of missingness. By omitting these samples we may reduce the overall missingness in the data set. We accomplish this by using the function is.na() in conjunction with apply() to determine each samples degree of missingness and use this information to omit samples below a threshold. We can then visualize this change with another heatmap.

myMiss <- apply(dp, MARGIN = 2, function(x){ sum( is.na(x) ) } )
myMiss <- myMiss / nrow(dp)
barplot(myMiss, las = 3)


vcf@gt <- vcf@gt[, c(TRUE, myMiss < 0.6)]
vcf
## ***** Object of Class vcfR *****
## 18 samples
## 1 CHROMs
## 22,031 variants
## Object size: 19.2 Mb
## 27.7 percent missing data
## *****        *****         *****
dp <- extract.gt(vcf, element = "DP", as.numeric=TRUE)
heatmap.bp(dp[1:1000,], rlabels = FALSE)


Omitting variants
Previously we have seen how to quantify and visualize missingness for variants in our dataset. We can use this information to omit variants that have a high degree of missingness similar to how we treated the samples.

myMiss <- apply(dp, MARGIN = 1, function(x){ sum( is.na(x) ) } )
myMiss <- myMiss / ncol(dp)
#vcf <- vcf[myMiss < 0.2, ]
vcf <- vcf[myMiss < 0.05, ]
vcf
## ***** Object of Class vcfR *****
## 18 samples
## 1 CHROMs
## 2,190 variants
## Object size: 2.7 Mb
## 0 percent missing data
## *****        *****         *****
dp <- extract.gt(vcf, element = "DP", as.numeric=TRUE)
heatmap.bp(dp, rlabels = FALSE)


When we are happy with our changes we can save our production data using the function write.vcf.

write.vcf(vcf, file = "pinfsc50_filtered.vcf.gz")
#write.vcf(vcf, "pinfsc50_qc.vcf.gz")
Summary.
Through omitting samples and variants with a high degree of missingness we have taken a dataset that had a large fraction of questionable data and subset it on the fraction of the data we feel is of high quality. How important any particular sample or variant is will have to be determined base on the specifics of any particular project. Through exploring thresholds that are different from those implemented here one may be able to improve on this more. We can now proceed to downstream analyses of this dataset with greater confidence that the variants we are analyzing are of high quality.

References
Tabachnick BG., Fidell LS., Osterlind SJ. 2001. Using multivariate statistics.

Population genetics and genomics in R
TABLE OF CONTENTS
PART I 
PART II 
PART III 
WORKSHOP 
ABOUT 
APPENDICES 
Analysis of genome data
BJ Knaus, JF Tabima, and NJ Grünwald
Introduction
Opening and examining the dataset
Converting VCF data to a genlight object
chromR objects
Genetic differentiation
References
Introduction
Analysis of genome data for populations can be seen as similar to the analyses of other marker systems discussed in previous chapters of this book, except that genome data analyses include larger quantities of data. For example, VCF data (discussed in ‘reading VCF data’) can be read into R using vcfR (Knaus & Grünwald, 2017) to create a vcfR object. This object can be converted into a genlight object (Jombart, 2008) and then a snpclone object (Kamvar, Tabima & Grünwald, 2014, Kamvar, Brooks & Grünwald (2015)) if deemed necessary. Analysis on these objects has been covered in previous sections. Genome scale data provides additional analytical options as well. For example, when assumptions about the neutrality of the majority of the genome are appropriate, this can be used as a null hypothesis and used to help identify markers that differentiate from this assumption. Here we’ll provide examples of how genomic data may be analyzed.

For genomics examples we’ll use the pinfsc50 dataset. The pinfsc50 dataset is from a number of published P. infestans genomics projects where the data has been subset here to supercontig_1.50. This dataset is available as a stand alone R package (Knaus & Grünwald, 2017). By subsetting the data to one supercontig it creates a dataset of a size that can be conveniently used for examples. This dataset illustrates some important strengths and weaknesses of these studies. A strength is the amount of data we have for each individual. Among the weaknesses are that the samples are ‘opportunistic’ in that we have no control over the design of the experiment. Also, because of the large investment in data per sample, there is a relatively small number of samples.

Opening and examining the dataset
We’ll read our VCF data into R using the function read.vcfR(). This is data from the pinfsc50 data set that we filtered for quality in the section reading VCF data. Once the file is read in we can validate its contents using the show method which is implemented by executing the object’s name at the prompt.

library('vcfR')
## 
##    *****       ***   vcfR   ***       *****
##    This is vcfR 1.5.0.9000 
##      browseVignettes('vcfR') # Documentation
##      citation('vcfR') # Citation
##    *****       *****      *****       *****
vcf <- read.vcfR("pinfsc50_filtered.vcf.gz")
vcf
## ***** Object of Class vcfR *****
## 18 samples
## 1 CHROMs
## 2,190 variants
## Object size: 2.7 Mb
## 0 percent missing data
## *****        *****         *****
The show method reports that we have 18 samples and 2,190 variants. If this matches our expectation then we can proceed.

Converting VCF data to a genlight object
Different R packages have created different data structures to hold your data when it is imported into R. This is analagous to the different file formats you may have used to analyze your data in software outside of R. We’ve tried to engineer a suite of functions to convert data structures among the various R packages we typically use. The R package adegenet is a popular R package used for population genetic analysis and it works on data structures called ‘genlight’ objects. Here we use the function vcfR2genlight() to convert our vcfR object to a genlight object. This makes our VCF data available to the analyses in adegenet.

x <- vcfR2genlight(vcf)
## Warning in vcfR2genlight(vcf): Found 44 loci with more than two alleles.
## Objects of class genlight only support loci with two alleles.
## 44 loci will be omitted from the genlight object.
## Loading required namespace: adegenet
## Loading required package: parallel
x
##  /// GENLIGHT OBJECT /////////
## 
##  // 18 genotypes,  2,146 binary SNPs, size: 222 Kb
##  0 (0 %) missing data
## 
##  // Basic content
##    @gen: list of 18 SNPbin
## 
##  // Optional content
##    @ind.names:  18 individual labels
##    @loc.names:  2146 locus labels
##    @chromosome: factor storing chromosomes of the SNPs
##    @position: integer storing positions of the SNPs
##    @other: a list containing: elements without names
A genlight object only supports biallelic, or binary, variants. That is, variants with no more than two alleles. However, variant call format data can include multiple alleles. When we created our genlight object we recieved a warning message indicating that our vcfR object had variants with more than two alleles and that it was being subset to only biallelic variants. This is one of several important differences in how data is handled in VCF data versus genlight objects.

Another important difference among VCF and genlight data is how the genotypes are stored. In VCF data the alleles are delimited by either a pipe or a forward slash (‘|’, ‘/’ respectively). Because genlight objects only use biallelic loci the genotypes can be recoded as 0, 1 and 2. These correspond to homozygous for the reference or zero allele, heterozygote or homozygous for the first alternate allele. We can validate this by checking a few select genotypes from both the vcfR object and the genlight object.

# vcfR
gt <- extract.gt(vcf, element = "GT")
gt[c(2,6,18), 1:3]
##                        BL2009P4_us23 DDR7602 IN2009T1_us22
## Supercontig_1.50_80063 "1|0"         "1|0"   "0|1"        
## Supercontig_1.50_80089 "0|0"         "1|0"   "0|1"        
## Supercontig_1.50_94108 "0|1"         "0|1"   "1|1"
# genlight
t(as.matrix(x))[c(1,5,17), 1:3]
##                        BL2009P4_us23 DDR7602 IN2009T1_us22
## Supercontig_1.50_80063             1       1             1
## Supercontig_1.50_80089             0       1             1
## Supercontig_1.50_94108             1       1             2
Note that in VCF data the samples are in columns and the variants are in rows. In genlight objects, and many other R objects, the samples are in rows while the variants are in columns. We can use the transpose function (t()) to convert between these two states.

Yet another difference among VCF data and genlight objects is that in VCF data there is no concept of ‘population.’ The package adegenet was designed specifically for the analysis of population data, so its genlight object has a place (a ‘slot’) to hold this information. Because there is no population data in VCF data, if we want population data we’ll have to set it ourselves.

library(adegenet)
## Loading required package: methods
## Loading required package: ade4
## 
##    /// adegenet 2.0.1 is loaded ////////////
## 
##    > overview: '?adegenet'
##    > tutorials/doc/questions: 'adegenetWeb()' 
##    > bug reports/feature requests: adegenetIssues()
pop(x) <- as.factor(c("us", "eu", "us", "af", "eu", "us", "mx", "eu", "eu", "sa", "mx", "sa", "us", "sa", "Pmir", "us", "eu", "eu"))
popNames(x)
## [1] "af"   "eu"   "mx"   "Pmir" "sa"   "us"
Our population designation consists of a vector, that is the same length as the number of samples we have, where each element indicates which population each sample belongs to. By using the as.factor() function we transform the “vector” into a “factor”. A factor understands that all of the elements that are named “us” or “eu” are all part of the same group. This is why when we ask for the popNames we get a vector where each population is represented only once.

Yet another difference among VCF data and genlight objects is the concept of ploidy. In VCF data each variant is treated independently. This means that in theory VCF data may contain data that is of mixed ploidy. In a genlight object different samples may be of different ploidy levels, but within each sample all of its loci must be of the same ploidy level. Here we’ll set the ploidy of all the samples in the genlight object to the same ploidy.

ploidy(x) <- 2
Distance matrices
Let’s create a pairwise genetic distance matrix for individuals or populations (i.e., groups of individuals).

Note: There isn’t actually a function that creates distance matrices from genlight objects in adegenet. Instead, the authors of adegenet created an as.matrix() function that converts a genlight object to a matrix. This is clever because the function dist() in the package stats tries to convert whatever object it is given into a matrix. The result is that when you call dist() on a genlight object it uses the dist() function to create a distance matrix. The reason this is clever is because it uses pre-existing code. The downside is that because there is no function to specifically create distance matrices from genlight objects in adegenet, there is no documentation in genlight for how this is done. And because the author of dist() never anticipated it could be used on genlight objects, there is no documentation for it there either.
To summarize, we can create a distance matrix from a genlight object using dist():

x.dist <- dist(x)
Note, that we have not specified what the variable x is. We can find documentation for this function with ?dist.

There are also functions to create distance matrices from genlight objects that exist in other packages. The function bitwise.dist() in the package poppr is an example. We can find documentation for this function with ?poppr::bitwise.dist. Again, you need to know where to look for this information or you may not find it. We can use this function as follows.

x.dist <- poppr::bitwise.dist(x)
Note, that the variable x has not yet been specified. Lastly, because you can use as.matrix() on your genlight object, and most distance algorithms can use this matrix as input, you can use this as an intermediate step to create a matrix from your genlight object and pass it to your distance algorithm of choice. Options include ade4, vegdist() in vegan, or daisy() in cluster. Note that it is up to you to determine which distance metric is best for your particular analysis. A number of options therefore exist for creating distance matrices from genlight objects.

chromR objects
Using chromR to locate unusual features in a genome
Genomic projects frequently incorporate several types of data. For example, the reference sequence may be stored as a FASTA format file, variants (SNPs, indels, etc.) may be stored in a variant call format (VCF) file while annotations may be stored as a GFF or BED format (tablular data). Genome browsers can be used to integrate these different data types. However, genome browsers typically lack a manipulation environment, they simply display existing files. The R environment includes a tremendous amount of statistical support that is both specific to genetics and genomics as well as more general tools (e.g., the linear model and its extensions). The R package vcfR provides a link between VCF data and the R environment and it includes a simple genome browser to help visualize the effect of manipulations. Here we explore how we can use vcfR to survey genomic data for interesting features.

Creating chromR objects
In this example we will begin by locating the example data from the pinfsc50 package. This is a separate package from vcfR that you will need to install. If you haven’t installed it already, you can install it with install.packages('pinfsc50'). For data from your own research activities you may wany to omit the system.file() steps and directly use your filenames in the input steps.

library(vcfR)

# Find the files.
vcf_file <- system.file("extdata", "pinf_sc50.vcf.gz", package = "pinfsc50")
dna_file <- system.file("extdata", "pinf_sc50.fasta", package = "pinfsc50")
gff_file <- system.file("extdata", "pinf_sc50.gff", package = "pinfsc50")

# Input the files.
vcf <- read.vcfR(vcf_file, verbose = FALSE)
dna <- ape::read.dna(dna_file, format = "fasta")
gff <- read.table(gff_file, sep="\t", quote="")

# Create a chromR object.
chrom <- create.chromR(name="Supercontig", vcf=vcf, seq=dna, ann=gff, verbose=TRUE)
## Names in vcf:
##   Supercontig_1.50
## Names of sequences:
##   Supercontig_1.50 of Phytophthora infestans T30-4
## Warning in create.chromR(name = "Supercontig", vcf = vcf, seq = dna, ann = gff, : 
##         Names in variant data and sequence data do not match perfectly.
##         If you choose to proceed, we'll do our best to match the data.
##         But prepare yourself for unexpected results.
## Names in annotation:
##   Supercontig_1.50
## Initializing var.info slot.
## var.info slot initialized.
Note that a warning message indicates that the names in all of the data sources do not match pefectly. It has been my experience that this is a frequent occurrence in genome projects. Instead of asking the user to create duplicate files that have the same data but standardized names, vcfR allows the user to exercise some judgement. If you see this message and feel the names are correct you can ignore this and proceed. In this case we see that a chromosome is named ‘Supercontig_1.50’ in the VCF data but named ‘Supercontig_1.50 of Phytophthora infestans T30-4’ in the FASTA (sequence) file. Because we know that for this specific project these are synonyms we can safely ignore the warning and proceed.

Once we have created our chromR object we can verify that its contents are what we expect. By executing the object’s name at the console, with no other arguments, we invoke the object’s ‘show’ method. The show method for chromR objects presents a summary of the object’s contents.

chrom
## *****   Class chromR, method Show   *****
## Name: Supercontig 
## Chromosome length: 1,042,442 bp
##   Chromosome labels: Supercontig_1.50 of Phytophthora infestans T30-4
## Annotation (@ann) count: 223 
##   Annotation chromosome names: Supercontig_1.50
## Variant (@vcf) count: 22,031 
##   Variant (@vcf) chromosome names: Supercontig_1.50
## Object size: 22.5 Mb
## Use head(object) for more details.
## *****      End Show (chromR)        *****
There at least two ways to graphically view the chromR object. The first is plot() which plots histograms of some of data summaries.

plot(chrom)


The read depth here is a sum over all samples. We see a peak that represents the depth where most of our genomes were sequenced at. Low regions of sequence depth may indicate variants where we may be concerned that there may not be enough information to call a genotype. Variants of high coverage may represent repetetive regions of genomes where the reference may not contain all the copies so the reads pile up on the fraction of repeats that were successfully assembled. These regions may violate the ploidy assumptions made by variant callers and therefore may be considered a target for quality filtering. Mapping quality is very peaked at 60 but also contains variants that deviate from this common value. Quality (QUAL) is less easily interpreted. It appears that most of our variants are of a low quality with very few of them being of high quality. It is important to remember that while everyone would like high quality, quality is frequently difficult to measure. The simplest interpretation here is that QUAL may not be a good parameter to use to judge your variants. The last panel for SNP densities is empty because this data is created during the processing of chromR objects, which we will discuss below.

chromoqc(chrom, dp.alpha = 66)


Our second plot, called chromo plot, displays the same information as the plot method only it distributes the data along its chomosomal coordinates. It also includes a representation of the annotation data. The contents of this plot are somewhat flexible in that it depends on what data is present in the chromR object.

Processing chromR objects
Creation and processing of a chromR object has been divided into separate tasks. Creation loads the data into the chromR object and should typically only be required once. Processing the chromR object generates summaries of the data. Some of these summaries will need to be updated as the chromR object is updated. For example, if the size of the sliding window used to summarize variant density and GC content is changed the chromR object will need to be processed to update this information.

chrom <- proc.chromR(chrom, verbose = TRUE)
## Nucleotide regions complete.
##   elapsed time:  0.538
## N regions complete.
##   elapsed time:  0.28
## Population summary complete.
##   elapsed time:  0.357
## window_init complete.
##   elapsed time:  0.001
## windowize_fasta complete.
##   elapsed time:  0.123
## windowize_annotations complete.
##   elapsed time:  0.014
## windowize_variants complete.
##   elapsed time:  0.001
plot(chrom)


Subsequent to processing, our plot function is identical to its previous presentation except that we now have variant densities. When we observe the chromoqc plot we see that we now have variant densities, nucleotide content as well as a representation of where in our reference we have nucleotides (A, C, G or T) or where we have ambiguous nucleotides.

chromoqc(chrom, dp.alpha = 66)


The above data is an example of visualizing raw data that has come from a variant caller and other automated sources. In our section on quality control we presented methods on how to filter variants on various parameters as an attempt to omit low quality variants. We can use this data to create a chromR object and compare it to the above data.

#vcf <- read.vcfR("pinfsc50_qc.vcf.gz", verbose = FALSE)
vcf <- read.vcfR("pinfsc50_filtered.vcf.gz", verbose = FALSE)
chrom <- create.chromR(name="Supercontig", vcf=vcf, seq=dna, ann=gff, verbose=FALSE)
chrom <- proc.chromR(chrom, verbose = FALSE)
chromoqc(chrom, dp.alpha = 66)


We have a smaller quantity of data after our quality control steps. However, there do appear to be a few improvements. First, the read depth is now fairly uniform and lacks the large variation in depth we saw in the raw data. In genomics projects our naive assumption is that we would sequence all regions of the genome at the same depth. So this change in the data allows it to approach our expectation. Second, the mapping quality appear relatively constant and the variants with low mapping quality have been omitted. If we feel that ‘mapping quality’ is a reasonable assessment of quality, we may interpret this as an improvement. These are methods we feel improve the quality of our datasets prior to analysis.

Tabular summaries
When we process a chromR object, two forms of tabular data are created. First, summaries are made on a per variant basis. This includes sample size (minus missing data), allele counts, heterozygosity and effective size. Second, summaries are made on a per window basis. Window size can be changed with the win.size parameter of the function proc.chromR(). Window based summaries include nucleotide content per window (including missing data so you can adjust window size for analyses if necessary), the number of genic sites per window (when annotation information was provided) and the number of variants per window.

head(chrom@var.info)
##              CHROM   POS    MQ  DP mask  n Allele_counts         He
## 1 Supercontig_1.50 80058 58.96 508 TRUE 18       25,10,1 0.64364712
## 2 Supercontig_1.50 80063 58.95 514 TRUE 18         25,11 0.42438272
## 3 Supercontig_1.50 80067 58.88 499 TRUE 18         23,13 0.46141975
## 4 Supercontig_1.50 80073 58.77 490 TRUE 18          35,1 0.05401235
## 5 Supercontig_1.50 80074 58.75 482 TRUE 18         26,10 0.40123457
## 6 Supercontig_1.50 80089 58.80 481 TRUE 18         25,11 0.42438272
##         Ne
## 1 2.806207
## 2 1.737265
## 3 1.856734
## 4 1.057096
## 5 1.670103
## 6 1.737265
head(chrom@win.info)
##              CHROM window start  end length   A   C   G   T    N other
## 1 Supercontig_1.50      1     1 1000   1000 267 213 293 227    0     0
## 2 Supercontig_1.50      2  1001 2000   1000 283 206 309 202    0     0
## 3 Supercontig_1.50      3  2001 3000   1000 229 213 235 177  146     0
## 4 Supercontig_1.50      4  3001 4000   1000   0   0   0   0 1000     0
## 5 Supercontig_1.50      5  4001 5000   1000   0   0   0   0 1000     0
## 6 Supercontig_1.50      6  5001 6000   1000   0   0   0   0 1000     0
##   genic variants
## 1     0        0
## 2     0        0
## 3     0        0
## 4     0        0
## 5     0        0
## 6     0        0
While loading entire genomes into memory may not be practical due to resource limitations, it is frequently practical to break a genome up into fractions that can be processed given the resources available on any system. By processing a genome by chromosomes, or some other fraction, and saving this tabular data to file you can perform genome scans in an attempt to identify interesting features.

Genetic differentiation
A fundamental question to most population studies is whether populations are diverse and whether this diversity is shared among the populations? To address the question of within population diversity geneticists typically report heterozygosity. This is the probability that two alleles randomly chosen from a population will be different (Nei, 1973). Ecologists may know this as Simpson’s Index (Simpson, 1949). To address differentiation population geneticists typically utilize FSTFST or one of its analogues. Population differentiation measured by FSTFST was originally proposed by Sewall Wright (Wright, 1949). This was later extended to a method based on diversity by Masatoshi Nei (Nei, 1973). As researchers applied these metrics to microsatellites, genetic markers with a large number of alleles, it became clear that Nei’s measure would not correctly range from zero to one, so Philip Hedrick proposed a correction (Hedrick, 2005). More recently, Lou Jost proposed another alternative (Jost, 2008). You can tell a topic is popular when so many variants of it are generated. And there are more variants than mentioned here. A nice discussion as to which measure may be appropriate for your data was posteed to teh Molecular Ecologist blog titled should I use FSTFST, GSTGST or DD?.

In vcfR, the function genetic_diff() was implemented to measure population diversity and differentiation. Because VCF data typically do not include population information we’ll have to supply it as a factor. The method ‘nei’ employed here is based on the methods reported by Hedrick (Hedrick, 2005). The exception is that the heterozygosities are weighted by the number of alleles observed in each population. This was inspired by  hierfstat::pairwise.fst() which uses the number of individuals observed in each population to weight the heterozygosities. By using the number of alleles observed instead of the number of individuals we remove an assumption about how many alleles each individual may contribute. That is, we should be able to accomodate samples of mixed ploidy.

library(vcfR)
data(vcfR_example)
pop <- as.factor(c("us", "eu", "us", "af", "eu", "us", "mx", "eu", "eu", "sa", "mx", "sa", "us", "sa", "Pmir", "us", "eu", "eu"))
myDiff <- genetic_diff(vcf, pops = pop, method = 'nei')
knitr::kable(head(myDiff[,1:15]))
CHROM	POS	Hs_af	Hs_eu	Hs_mx	Hs_Pmir	Hs_sa	Hs_us	Ht	n_af	n_eu	n_mx	n_Pmir	n_sa	n_us
Supercontig_1.50	2	0	0.0	0.000	0.5	0.000	0.00	0.0798611	2	4	4	2	4	8
Supercontig_1.50	246	NaN	0.0	0.375	NaN	0.000	0.50	0.3512397	0	4	4	0	6	8
Supercontig_1.50	549	NaN	0.0	NaN	NaN	NaN	0.50	0.4444444	0	2	0	0	0	4
Supercontig_1.50	668	NaN	0.5	0.000	NaN	0.000	0.50	0.5000000	0	4	2	0	2	8
Supercontig_1.50	765	0	0.0	0.000	0.0	0.000	0.00	0.1107266	2	12	4	2	4	10
Supercontig_1.50	780	0	0.0	0.000	0.0	0.375	0.18	0.1244444	2	8	4	2	4	10
The function returns the chromosome and position of each variant as provided in the VCF data. This should allow you to align its output with the VCF data. The heterozygosities for each population are reported as well as the total heterozygosity, followed by the number of alleles observed in each population. Note that in some populations zero alleles were observed. Populations with zero alleles reported heterozygosities of ‘NaN’ because of this absence of data.

knitr::kable(head(myDiff[,16:19]))
Gst	Htmax	Gstmax	Gprimest
0.4782609	0.7951389	0.9475983	0.5047085
NaN	0.8057851	NaN	NaN
NaN	0.6666667	NaN	NaN
NaN	0.8125000	NaN	NaN
1.0000000	0.7543253	1.0000000	1.0000000
0.1160714	0.8000000	0.8625000	0.1345756
The remaining columns contain GSTGST, the maximum heterozygosity, the maximum GSTGST and finally G′STGST′. The maximum heterozygosity and the maximum GSTGST are intermediary values used to calculate G′STGST′. They are typically not reported but provide values to help validate that G′STGST′ was calculated correctly. Note that the populations that had zero alleles, and therefore a heterozygosity of ‘NaN’, contributed to GSTGSTs that were also ‘NaN’. To avoid this you may want to consider omitting populations with a small sample size or that contain a large amount of missing data.

We now have information for each variant in the VCF data. Because this is typically a large quantity of information, we’ll want to summarize it. One way is to take averages of the data.

knitr::kable(round(colMeans(myDiff[,c(3:9,16,19)], na.rm = TRUE), digits = 3))
Hs_af	0.176
Hs_eu	0.188
Hs_mx	0.168
Hs_Pmir	0.052
Hs_sa	0.198
Hs_us	0.155
Ht	0.247
Gst	0.595
Gprimest	0.632
Another way to summarize data is to use violin plots.

library(reshape2)
library(ggplot2)

dpf <- melt(myDiff[,c(3:8,19)], varnames=c('Index', 'Sample'), value.name = 'Depth', na.rm=TRUE)
## No id variables; using all as measure variables
p <- ggplot(dpf, aes(x=variable, y=Depth)) + geom_violin(fill="#2ca25f", adjust = 1.2)
p <- p + xlab("")
p <- p + ylab("")
p <- p + theme_bw()
p


References
Hedrick PW. 2005. A standardized genetic differentiation measure. Evolution 59:1633–1638. Available at: http://dx.doi.org/10.1111/j.0014-3820.2005.tb01814.x

Jombart T. 2008. adegenetadegenet: A R package for the multivariate analysis of genetic markers. Bioinformatics 24:1403–1405. Available at: https://doi.org/10.1093/bioinformatics/btn129

Jost L. 2008. GSTGST And its relatives do not measure differentiation. Molecular Ecology 17:4015–4026. Available at: http://dx.doi.org/10.1111/j.1365-294X.2008.03887.x

Kamvar ZN., Brooks JC., Grünwald NJ. 2015. Novel R tools for analysis of genome-wide population genetic data with emphasis on clonality. Name: Frontiers in Genetics 6:208. Available at: http://dx.doi.org/10.3389/fgene.2015.00208

Kamvar ZN., Tabima JF., Grünwald NJ. 2014. PopprPoppr: An R package for genetic analysis of populations with clonal, partially clonal, and/or sexual reproduction. PeerJ 2:e281. Available at: http://dx.doi.org/10.7717/peerj.281

Knaus BJ., Grünwald NJ. 2017. VcfrVcfr: A package to manipulate and visualize variant call format data in R. Molecular Ecology Resources 17:44–53. Available at: http://dx.doi.org/10.1111/1755-0998.12549

Nei M. 1973. Analysis of gene diversity in subdivided populations. Proceedings of the National Academy of Sciences 70:3321–3323. Available at: http://www.pnas.org/content/70/12/3321.abstract

Simpson EH. 1949. Measurement of diversity. Nature 163:688. Available at: http://dx.doi.org/10.1038/163688a0

Wright S. 1949. The genetical structure of populations. Annals of Eugenics 15:323–354. Available at: http://dx.doi.org/10.1111/j.1469-1809.1949.tb02451.x

Population genetics and genomics in R
TABLE OF CONTENTS
PART I 
PART II 
PART III 
WORKSHOP 
ABOUT 
APPENDICES 
GBS analysis
JF Tabima, BJ Knaus, and NJ Grünwald
Introduction
Opening and examining the vcf file
Converting the dataset to a genlight object
Population genetic analyses for GBS data
Subsetting a vcfR object to 200 random variants
Conclusion
References
Introduction
This tutorial focuses on large SNP data sets such as those obtained from genotyping-by-sequencing (GBS) for population genetic analysis in R. GBS is one of several techniques used to genotype populations using high throughput sequencing (HTS). In GBS, the genome is reduced in representation by using restriction enzymes, and then sequencing these products using HTS. For more information on these techniques we recommend reading Baird et al. (2008), Rowe, Renaut & Guggisberg (2011), or Poland et al. (2012).

We will use a data set of 94 samples of the red raspberry pathogen Phytophthora rubi (Tabima et al., In Prep). This pathogen is diploid and a fungal like Oomycete. Populations were obtained by sampling individual pathogen strains from roots of infected red raspberry in the states of California (CA), Oregon (OR), and Washington (WA). A total of 94 samples of P. rubi were sequenced using the Illumina HiSeq 3000 technology with 150 bp paired end reads and a target insert size of 500 bp. Currently, there is little information about the population structure of P. rubi in the western USA. We are interested in studying the population structure of P. rubi populations in the western US. The VCF data for this population can be downloaded from: prubi_gbs.VCF.gz.

To obtain variant calls in form of VCF data, the FASTQ reads from HTS were mapped to the reference genome of P. rubi (Tabima et al., in prep) using bowtie2 (Langmead & Salzberg, 2012). Variants were called using the GATK HaplotypeCaller (McKenna et al., 2010). This data was further filtered in vcfR using read depths (DP) and mapping qualities (MQ). Data was filtered as follows:

A minimum DP of 5x.
Variants in the top 5% of the DP distribution were removed.
Only variants with a MQ greater than 40 were retained.
Variants with more than 60% missing data were removed.
In addition to the VCF data, we have included the file population_data.gbs.txt, a tab-delimited text file that includes the name of the sample, country of origin, and the population from where it was sampled. The file is available for download at: population_data.gbs.txt. This link will likely open the data in a browser. Save the data onto your hard-drive as an ASCII text file with the same name.

Opening and examining the vcf file
Let’s first load the libraries needed for analysis:

library(vcfR)
library(poppr)
library(ape)
library(RColorBrewer)
Make sure you are in the right folder with the downloaded files available. Some of these packages will print a message when they are loaded. Here we suppressed this information. When you load these packages, you may see more output than presented here.

Next, let’s open the VCF file using vcfR and check that we have 94 samples and 615 SNPs:

rubi.VCF <- read.vcfR("prubi_gbs.vcf.gz")
Once we’ve loaded the data into R, we can validate it by entering our object’s name in the console:

rubi.VCF
## ***** Object of Class vcfR *****
## 94 samples
## 321 CHROMs
## 615 variants
## Object size: 2.9 Mb
## 4.589 percent missing data
## *****        *****         *****
VCF data does not typically include any sort of population information. We have to load this data separately from the text-delimited file we downloaded above that includes the ID of samples and the state where the samples were obtained from. Let’s load the  population_data.gbs.txt file into memory by using the read.table() function:

pop.data <- read.table("population_data.gbs.txt", sep = "\t", header = TRUE)
We can now check that all the samples in the VCF and the population data frame are included:

colnames(rubi.VCF@gt)[-1] == pop.data$AccessID
##  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
## [15] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
## [29] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
## [43] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
## [57] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
## [71] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
## [85] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
Remember, the first column for the vcfR object’s GT slot is a FORMAT column, hence we removed it from the name comparison step (i.e., by removing the first element in the column names vector, the FORMAT column name). If we keep the FORMAT column name we will not be able to compare between the names of the samples in the vcfR object. All of the 94 samples in the population data frame loaded in the vcfR object are in the same order.

Converting the dataset to a genlight object
The next step is to convert the data set into an object that is usable by poppr, adegenet, or any of the other population genetics packages in R. The vcfR package contains multiple functions to convert data into other formats (see the converting_data vignette of vcfR):  vignette('converting_data'). For our particular purpose we want to convert the vcfR object into a genlight object. We can use the  vcfR2genlight function for this:

gl.rubi <- vcfR2genlight(rubi.VCF)
## Warning in vcfR2genlight(rubi.VCF): Found 7 loci with more than two alleles.
## Objects of class genlight only support loci with two alleles.
## 7 loci will be omitted from the genlight object.
A warning is shown while transforming the object, telling us that there are seven loci with more than two alleles. Many of the functions we will be using in these tutorials have been written under the assumption of a bi-allelic model. This bi-allelic model restricts all loci to only two alleles in order to simplify some calculations. The genlight object supports only loci with no more than two alleles. The vcfR2genlight function subsets the data to filter loci that are not bi-allelic, returning an object that contains only loci with two alleles. The warning is to make sure we are aware that this action has taken place.

Additionally, we are required to specify the ploidy of the organism in order to calculate some population genetic metrics. P. rubi is a diploid organism, so we will specify a ploidy of two. All genlight objects have ploidy slots, in which the user can specify the ploidy of each individual sample, or once for the entire population. We can assume that every sample of P. rubi is diploid and we will specify a ploidy of 2 for the entire population. Note that while a genlight object can support individuals of different ploidy, within each individual all loci must be of the same ploidy.

ploidy(gl.rubi) <- 2
Our biological question requires predetermined populations. We can add them to the genlight object as part of the pop (population) slot. In order to specify the population, we added the State column from our pop.data data frame to the pop slot of our genlight object:

pop(gl.rubi) <- pop.data$State
We now end up with a genlight object of filtered VCF data:

gl.rubi
##  /// GENLIGHT OBJECT /////////
## 
##  // 94 genotypes,  608 binary SNPs, size: 241.9 Kb
##  2607 (4.56 %) missing data
## 
##  // Basic content
##    @gen: list of 94 SNPbin
##    @ploidy: ploidy of each individual  (range: 2-2)
## 
##  // Optional content
##    @ind.names:  94 individual labels
##    @loc.names:  608 locus labels
##    @chromosome: factor storing chromosomes of the SNPs
##    @position: integer storing positions of the SNPs
##    @pop: population of each individual (group size range: 23-39)
##    @other: a list containing: elements without names
Next, let’s get started with our first analyses.

Population genetic analyses for GBS data
Distance matrices
Let’s create a pairwise genetic distance matrix for individuals or populations (i.e., groups of individuals).

Note: There isn’t actually a function that creates distance matrices from genlight objects in adegenet. Instead, the authors of adegenet created an as.matrix() function that converts a genlight object to a matrix. This is clever because the function dist() in the package stats tries to convert whatever object it is given into a matrix. The result is that when you call dist() on a genlight object it uses the dist() function to create a distance matrix. The reason this is clever is because it uses pre-existing code. The downside is that because there is no function to specifically create distance matrices from genlight objects in adegenet, there is no documentation in genlight for how this is done. And because the author of dist() never anticipated it could be used on genlight objects, there is no documentation for it there either.
To summarize, we can create a distance matrix from a genlight object using dist():

x.dist <- dist(x)
Note, that we have not specified what the variable x is. We can find documentation for this function with ?dist.

There are also functions to create distance matrices from genlight objects that exist in other packages. The function bitwise.dist() in the package poppr is an example. We can find documentation for this function with ?poppr::bitwise.dist. Again, you need to know where to look for this information or you may not find it. We can use this function as follows.

x.dist <- poppr::bitwise.dist(x)
Note, that the variable x has not yet been specified. Lastly, because you can use as.matrix() on your genlight object, and most distance algorithms can use this matrix as input, you can use this as an intermediate step to create a matrix from your genlight object and pass it to your distance algorithm of choice. Options include ade4, vegdist() in vegan, or daisy() in cluster. Note that it is up to you to determine which distance metric is best for your particular analysis. A number of options therefore exist for creating distance matrices from genlight objects.

Distance tree
Let’s start our analysis by building a genetic distance tree that represents the genetic relatedness of the samples. The similarity between samples and groups of samples is represented by the branch length. In most trees, the branch length is represented by the number of substitutions per site for a cluster or a sample. When samples are very similar, they are grouped by short branches. The longer the branch, the higher the number of substitutions and the higher the genetic distance is between samples or clusters.

For this tutorial, we will build a distance tree to obtain an initial assessment of the population structure of the P. rubi samples in the western US. We will reconstruct a distance tree based on the UPGMA algorithm, with 100 bootstrap replicates to assess branch support:

tree <- aboot(gl.rubi, tree = "upgma", distance = bitwise.dist, sample = 100, showtree = F, cutoff = 50, quiet = T)
Next, we will color the tips of the tree based on the population of origin of the samples, and draw conclusions from what we observe in the tree:

cols <- brewer.pal(n = nPop(gl.rubi), name = "Dark2")
plot.phylo(tree, cex = 0.8, font = 2, adj = 0, tip.color =  cols[pop(gl.rubi)])
nodelabels(tree$node.label, adj = c(1.3, -0.5), frame = "n", cex = 0.8,font = 3, xpd = TRUE)
#legend(35,10,c("CA","OR","WA"),cols, border = FALSE, bty = "n")
legend('topleft', legend = c("CA","OR","WA"), fill = cols, border = FALSE, bty = "n", cex = 2)
axis(side = 1)
title(xlab = "Genetic distance (proportion of loci that are different)")


We observe that samples do not cluster exclusively by region (e.g., CA, OR and WA). Instead, we observe a cluster with mainly CA samples (Green), but also containing a few WA (purple) and two OR (red) samples. The second, lower cluster contains predominantly samples for OR (red) and WA (purple).

Minimum spanning networks
Another useful independent analysis to visualize population structure is a minimum spanning network (MSN). MSN clusters multilocus genotypes (MLG) by genetic distances between them. Each MLG is a node, and the genetic distance is represented by the edges. In high throughput sequencing studies, where marker density is high, each sample typically consists of a unique genotype.

QUESTION: Do we show a reticulated MSN? OR a MST?
The nodes will be connected by the minimum distance between samples. This allows for reticulations (e.g., a network connecting nodes with identical genetic distances; contrast this with a tree where pairwise nodes are only connected to one other node with the shortest distances, even if several nodes share the same minimum genetic distance).

To reconstruct a MSN we require a genlight object and a distance matrix that represents the genetic distance between samples. To calculate this distance we will use the bitwise.dist() function from poppr. The bitwise.dist() function is a fast method to calculate the number of allelic differences between two samples:

library(igraph)

rubi.dist <- bitwise.dist(gl.rubi)
rubi.msn <- poppr.msn(gl.rubi, rubi.dist, showplot = FALSE, include.ties = T)

node.size <- rep(2, times = nInd(gl.rubi))
names(node.size) <- indNames(gl.rubi)
vertex.attributes(rubi.msn$graph)$size <- node.size

set.seed(9)
plot_poppr_msn(gl.rubi, rubi.msn , palette = brewer.pal(n = nPop(gl.rubi), name = "Dark2"), gadj = 70)


Note that we have used the command set.seed(9) to set a random number seed. Because there is some stochasticity in the presentation of a MSN, this is required to reproduce the exact same plot. In practice, you should explore using different seeds to obtain different representations of the same data.

The plot is similar to the tree we rendered above and shows that the populations are not fully differentiated geographically. There are two main groups in the plot, one that clusters the OR samples with some WA samples (upper left), and a second that clusters the CA samples with some WA samples (lower right). This provides additional evidence for the two clusters observed in the genetic distance tree. Next, we perform a principal components analysis.

Principal components analysis
A principal components analysis (PCA) converts the observed SNP data into a set of values of linearly uncorrelated variables called  principal components that summarize the variation between samples. We can perform a PCA on our genlight object by using the glPCA function.

rubi.pca <- glPca(gl.rubi, nf = 3)
barplot(rubi.pca$eig, col = heat.colors(50), main="PCA Eigenvalues")
title(ylab="Proportion of variance explained")
title(xlab="Eigenvalue")


The barplot indicates that we will need to only retain the first 3 PCAs, which cumulatively explain explain 63.183 percent of the variance of the data.

To view the results of the PCA we can use the package ggplot2. We need to convert the data frame that contains the principal components (rubi.pca$scores) into the new object rubi.pca.scores. In addition, we will add the population values as a new column in our  rubi.pca.scores object, in order to be able to color samples by population.

ggplot2 will plot the PCA, color the samples by population, and create ellipses that include 95% of the data for each the population:

rubi.pca.scores <- as.data.frame(rubi.pca$scores)
rubi.pca.scores$pop <- pop(gl.rubi)

library(ggplot2)
set.seed(9)
p <- ggplot(rubi.pca.scores, aes(x=PC1, y=PC2, colour=pop)) 
p <- p + geom_point(size=2)
p <- p + stat_ellipse(level = 0.95, size = 1)
p <- p + scale_color_manual(values = cols) 
p <- p + geom_hline(yintercept = 0) 
p <- p + geom_vline(xintercept = 0) 
p <- p + theme_bw()

p


The PCA produces a pattern similar our previous results. We observe that PC1 distinguishes samples from WA and OR, (right) from a cluster predominantly made up of CA and WA samples (left). The CA samples form a tight cluster with a narrow ellipse in green.

We can further explore population assignments using a discriminant analysis of principal components (DAPC).

DAPC
The DAPC is a multivariate statistical approach that uses populations defined a priori to maximize the variance among populations in the sample by partitioning it into between-population and within-population components. DAPC thus maximizes the discrimination between groups. DAPC is explained in depth in the DAPC chapter on Part II of this tutorial and in the DAPC adegenet vignette.

DAPC requires a genlight object with populations defined a priori. We already have this genlight object from the above steps. Usually, we use the number of principal components and discriminant axes that maximize the variance between populations; but our objective here is to calculate the population assignments based on the results of the PCA. We will use the same parameters as in the PCA to make the results comparable between both methods. These parameters (n.pca=3 and n.da=2) will be used to reconstruct the DAPC, obtain the assignment of the samples to each population, and suggest admixture between geographical states in the western USA.

pnw.dapc <- dapc(gl.rubi, n.pca = 3, n.da = 2)
To confirm that the DAPC is similar to the PCA we can plot the data in a scatter plot.

scatter(pnw.dapc, col = cols, cex = 2, legend = TRUE, clabel = F, posi.leg = "bottomleft", scree.pca = TRUE,
        posi.pca = "topleft", cleg = 0.75)


We see that the results of the PCA and DAPC are very similar. The DAPC object we created includes the population membership probability for each sample to each of the predetermined populations. To visualize the posterior assignments of each sample, we use a composite stacked bar plot (compoplot). A compoplot illustrates the probability of population membership on the y-axis. Each sample is a bin on the x-axis, and the assigned probability of population membership is shown as a stacked bar chart with clusters or populations shown in color.

#compoplot(pnw.dapc,col = function(x) cols, posi = 'top')
compoplot(pnw.dapc,col = cols, posi = 'top')


These plots are hard to interpret and we will thus separate the samples by population.
ggplot2 can be used to reconstruct these plots, but we need to convert the data into a ggplot2 friendly object. We will extract the DAPC calculated population membership assignments (pnw.dapc$posterior) into a new data frame (dapc.results), include the original population assignment as a new column in the data frame (dapc.results$pop), and add a column that includes the sample names (dapc.results$indNames).

dapc.results <- as.data.frame(pnw.dapc$posterior)
dapc.results$pop <- pop(gl.rubi)
dapc.results$indNames <- rownames(dapc.results)
ggplot2 has specific requirements for the structure of the data frame format, as it requires each observation in rows, and all different values of these observations in columns (i.e., a long format data frame). To transform the data frame we use the function melt from the package  reshape2. melt reorganizes the data frame into the required data frame format, where each membership probability observation for a given population is a row with the sample name, original population, and assigned population as columns.

library(reshape2)
dapc.results <- melt(dapc.results)
Ignore the prompt for now. Then, we rename the columns into more familiar terms:

colnames(dapc.results) <- c("Original_Pop","Sample","Assigned_Pop","Posterior_membership_probability")
ggplot2 will plot the dapc.results data frame we reorganized using melt, using the samples on the X-axis and membership probabilities on the Y-axis. The fill color will indicate the original population assignments. Each facet represents the original population assignment for each sample:

p <- ggplot(dapc.results, aes(x=Sample, y=Posterior_membership_probability, fill=Assigned_Pop))
p <- p + geom_bar(stat='identity') 
p <- p + scale_fill_manual(values = cols) 
p <- p + facet_grid(~Original_Pop, scales = "free")
p <- p + theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8))
p


This bar plot shows us a more organized perspective of our data set by contrasting the population membership probability assignments against their original populations.

In general, the composite plot shows that all western states of the USA exhibit admixture with other states, with WA showing the most admixture.

Subsetting a vcfR object to 200 random variants
One of the main limitations encountered when performing a population genetic analysis using large SNP data sets is the size of the data set. Large datasets require large amounts of computing resources, presenting a challenge to individuals who are not familiar with managing their computing resources. These large data sets are becoming very common thanks to the current developments on genomics and computational biology, where instead on performing analyses on one or a few loci current analyses focus in entire genomes of large populations.

GBS data sets are good examples of large data sets used in population genetics. The large number of samples, and the high number of variants obtained, can generate gigabytes of data that are difficult to analyse with a desktop computer. Many of the variants obtained from GBS analyses are removed via filtering (for a deeper explanation of filtering genomic variant data review the quality control chapter of this workshop). Nonetheless, these filtered data sets may have hundreds to thousands of variants, making their analysis very tedious for the average user.

One way of solving the issues of dealing with large genomic data sets is to subset the data set to smaller groups of random variants across the genome. Multiple subsets of a data set can be analysed in order to determine if the different subsets reveal similar overall results. These independent analyses will provide support to the results, while reducing the effective computational time of the analysis.

We will illustrate the process of subsetting datasets using the P. rubi GBS data. As a reminder, this dataset has 138 samples spanning 615 variants across the genome. The number of variants in this analysis does not reflect a real large data set, but we will use it as it will allow us to compare the results of the subsets against the total analysis. This comparison will demonstrate that subsets of large data sets can provide information that is similar to the total data set while reducing the computational intensity of the calculations.

In this example we will show how to subset the data to 200 random variants across the genome. We will use the rubi.VCF object we created at the beginning of this tutorial. To subset, we use square brackets to select portions of an object we would like to retain. The square brackets represent the values to be subset from the original data set, for example: if we have a vector with the four first letters of the alphabet (“A”,“B”,“C”,“D”), and we want to subset only the first two occurrences we can do this:

alphabet <- c("A","B","C","D")
alphabet[c(1,2)]
## [1] "A" "B"
We see that when we use the alphabet[c(1,2)] we are calling the first two occurrences of the vector of interest. We can do something very similar with the vcfR objects from the vcfR package. Lets say we are interested only in the first 10 variants of our rubi.VCF, we can do something similar:

rubi.VCF
## ***** Object of Class vcfR *****
## 94 samples
## 321 CHROMs
## 615 variants
## Object size: 2.9 Mb
## 4.589 percent missing data
## *****        *****         *****
rubi.VCF[c(1:10),]
## ***** Object of Class vcfR *****
## 94 samples
## 4 CHROMs
## 10 variants
## Object size: 0.9 Mb
## 3.83 percent missing data
## *****        *****         *****
We can see that the second object only has 10 variants. If we look at the summary in detail, we observe that the original sample has 312 chromosomes/scaffolds, 615 variants and a size of 2.9 Mb. The subset object, on the other hand, only has 4 chromosomes/scaffolds, 10 variants and a size of 0.9 Mb. Both objects have the same number of samples. This is a good example of how to subset a vcfR object. However, we want to obtain a subset of 200 random variants. If we do rubi.VCF[c(1:200)] we will only obtain the first 200 variants. These 200 variants will always be the same, and we will have no real replication to determine the strength of our approach.

One way to solve this problem is to generate a random string of 200 numbers that range from 1 to the total number of variants of the object. The R statistical environment has the function sample(). The function sample() generates a random vector of elements (i.e. numbers or elements from a vector). We can use sample() to generate the random vector of 200 numbers that range from 1 to the total number of variants of the object. The total number of variants in a vcfR object is equivalent to the number of rows in the vcfR object. We will use the nrow() method for our vcfR object to obtain the maximum range of values to subset our data set:

subset.1 <- sample(size = 200, x= c(1:nrow(rubi.VCF)))
subset.2 <- sample(size = 200, x= c(1:nrow(rubi.VCF)))
We have to check that the subsets are not identical. We can confirm this by using the function identical().

identical(subset.1, subset.2)
## [1] FALSE
We can see that the vectors have different numbers. The results from identical() do not mean that every number in the vector is different. It just means that some of the values in the vectors are different, but each vector can also share some numbers with the other vector.

Now we know how to generate a vector of 200 random numbers from 1 to the length of the vcfR object. We will use these vectors to subset the  rubi.VCF object. To subset the vcfR object we execute code very similar to the one shown in the subset example. The main difference is that now we can use the subset.1 and subset.2 objects as the subset indices:

rubi.VCF.sub1 <- rubi.VCF[subset.1,]
rubi.VCF.sub2 <- rubi.VCF[subset.2,]

rubi.VCF.sub1
## ***** Object of Class vcfR *****
## 94 samples
## 146 CHROMs
## 200 variants
## Object size: 1.7 Mb
## 4.463 percent missing data
## *****        *****         *****
rubi.VCF.sub2
## ***** Object of Class vcfR *****
## 94 samples
## 150 CHROMs
## 200 variants
## Object size: 1.7 Mb
## 4.473 percent missing data
## *****        *****         *****
When the two rubi.VCF.sub objects are compared we can see that the number of variants (200) and the number of samples (94) are identical. This indicates that the subset was performed correctly. Observe that the chromosome number and percentage of missing data are different. These different numbers show that each subset has selected different variants.

Now we can generate subsets of random variants. We will extend the subset example by creating 50 subsets of 200 variants each, then we will then reconstruct a UPGMA distance tree per subset and overlay all of the reconstructed trees. This tree overlay will allow us to determine if the subsets support our hypothesis of low population differentiation, or if we have unique tree clusters for different subsets. You, the user, don’t have to execute this example, as it is for illustrative purposes only.

# Creating a list object to save our subsets in.
rubi.variant.subset <- list()

# Using a for loop to generate 50 subsets of 200 random variants from the rubi.VCF vcfR object.
for (i in 1:50){
  rubi.variant.subset[[i]] <- rubi.VCF[sample(size = 200, x= c(1:nrow(rubi.VCF)))]
}

# Checking we have 50 vcfR objects:
length(rubi.variant.subset)
## [1] 50
head(rubi.variant.subset, n=2)
## [[1]]
## ***** Object of Class vcfR *****
## 94 samples
## 143 CHROMs
## 200 variants
## Object size: 1.7 Mb
## 4.819 percent missing data
## *****        *****         *****
## 
## [[2]]
## ***** Object of Class vcfR *****
## 94 samples
## 146 CHROMs
## 200 variants
## Object size: 1.7 Mb
## 4.505 percent missing data
## *****        *****         *****
# Creating the GenLight object
rubi.gl.subset <- lapply(rubi.variant.subset, function (x) suppressWarnings(vcfR2genlight(x)))
for (i in 1:length(rubi.gl.subset)){
  ploidy(rubi.gl.subset[[i]]) <- 2
}

# Creating a simple UPGMA tree per object
library(phangorn)
rubi.trees <- lapply(rubi.gl.subset, function (x) upgma(bitwise.dist(x)))
class(rubi.trees) <- "multiPhylo"

# Overlapping the trees
densiTree(rubi.trees, consensus = tree, scaleX = T, show.tip.label = F, alpha = 0.1)
title(xlab = "Proportion of variants different")


We can observe that the results between both subsets are very similar: Clustering between samples from WA and OR, clustering between some samples of CA and WA, and a small number of samples of CA and OR clustering. These results reflect similar patterns to what we have observed with the full data set, indicating that our subset strategy is a suitable approach to analyse our P. rubi GBS data set

Since the DensiTree function does not currently allow us to color the tip labels we will have to color them by hand in a graphics software outside of R. This is the final result:

Alt text: Densitree
Alt text: Densitree

The densiTree reconstruction shows a very similar overall topology to the initial consensus tree: We see 6/7 major clades, and each of the clades is comprised of samples from different populations, indicating low geographic population structure. Furthermore, we observe how multiple subsets can result in similar patterns to the overall data. Subsetting large genomic variants is an easy, time-efficient and computationally smart method.

Conclusion
We were able to determine that the western states of P. rubi do not have population structure based on geographical locations. The results showed that the CA population was less diverse and had a lower degree of admixture than either the WA or OR populations. We can conclude that the geographical locations do not reflect the population structure of samples of P. rubi in the western USA, where the results suggested that different states shared genotypes across the entire geographic gradient resulting in one, panmictic and admixed population of P. rubi.

References
Baird NA., Etter PD., Atwood TS., Currey MC., Shiver AL., Lewis ZA., Selker EU., Cresko WA., Johnson EA. 2008. Rapid SNP discovery and genetic mapping using sequenced RAD markers. PLoS ONE 3:e3376. Available at: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0003376

Langmead B., Salzberg SL. 2012. Fast gapped-read alignment with bowtie 2. Nature methods 9:357–359. Available at: http://dx.doi.org/10.1038/nmeth.1923

McKenna A., Hanna M., Banks E., Sivachenko A., Cibulskis K., Kernytsky A., Garimella K., Altshuler D., Gabriel S., Daly M. et al. 2010. The genome analysis toolkit: A mapReduce framework for analyzing next-generation dNA sequencing data. Genome research 20:1297–1303. Available at: https://dx.doi.org/10.1101%2Fgr.107524.110

Poland J., Endelman J., Dawson J., Rutkoski J., Wu S., Manes Y., Dreisigacker S., Crossa J., Sánchez-Villeda H., Sorrells M. et al. 2012. Genomic selection in wheat breeding using genotyping-by-sequencing. The Plant Genome 5:103–113. Available at: http://dl.sciencesocieties.org/publications/tpg/abstracts/5/3/103

Rowe HC., Renaut S., Guggisberg A. 2011. RAD in the realm of next-generation sequencing technologies. Molecular Ecology 20:3499–3502. Available at: http://www.ncbi.nlm.nih.gov/pubmed/21991593

Tabima JF., Zasada I., Coffey M., Grünwald NJ. In Prep. Population dynamics of Phytopthora rubi indicate high rates of migration between states and nurseries in the pacific north western United States. In prep.

Population genetics and genomics in R
TABLE OF CONTENTS
PART I 
PART II 
PART III 
WORKSHOP 
ABOUT 
APPENDICES 
Analysis of genome data
BJ Knaus, JF Tabima, and NJ Grünwald
Introduction
Opening and examining the dataset
Converting VCF data to a genlight object
chromR objects
Genetic differentiation
References
Introduction
Analysis of genome data for populations can be seen as similar to the analyses of other marker systems discussed in previous chapters of this book, except that genome data analyses include larger quantities of data. For example, VCF data (discussed in ‘reading VCF data’) can be read into R using vcfR (Knaus & Grünwald, 2017) to create a vcfR object. This object can be converted into a genlight object (Jombart, 2008) and then a snpclone object (Kamvar, Tabima & Grünwald, 2014, Kamvar, Brooks & Grünwald (2015)) if deemed necessary. Analysis on these objects has been covered in previous sections. Genome scale data provides additional analytical options as well. For example, when assumptions about the neutrality of the majority of the genome are appropriate, this can be used as a null hypothesis and used to help identify markers that differentiate from this assumption. Here we’ll provide examples of how genomic data may be analyzed.

For genomics examples we’ll use the pinfsc50 dataset. The pinfsc50 dataset is from a number of published P. infestans genomics projects where the data has been subset here to supercontig_1.50. This dataset is available as a stand alone R package (Knaus & Grünwald, 2017). By subsetting the data to one supercontig it creates a dataset of a size that can be conveniently used for examples. This dataset illustrates some important strengths and weaknesses of these studies. A strength is the amount of data we have for each individual. Among the weaknesses are that the samples are ‘opportunistic’ in that we have no control over the design of the experiment. Also, because of the large investment in data per sample, there is a relatively small number of samples.

Opening and examining the dataset
We’ll read our VCF data into R using the function read.vcfR(). This is data from the pinfsc50 data set that we filtered for quality in the section reading VCF data. Once the file is read in we can validate its contents using the show method which is implemented by executing the object’s name at the prompt.

library('vcfR')
## 
##    *****       ***   vcfR   ***       *****
##    This is vcfR 1.5.0.9000 
##      browseVignettes('vcfR') # Documentation
##      citation('vcfR') # Citation
##    *****       *****      *****       *****
vcf <- read.vcfR("pinfsc50_filtered.vcf.gz")
vcf
## ***** Object of Class vcfR *****
## 18 samples
## 1 CHROMs
## 2,190 variants
## Object size: 2.7 Mb
## 0 percent missing data
## *****        *****         *****
The show method reports that we have 18 samples and 2,190 variants. If this matches our expectation then we can proceed.

Converting VCF data to a genlight object
Different R packages have created different data structures to hold your data when it is imported into R. This is analagous to the different file formats you may have used to analyze your data in software outside of R. We’ve tried to engineer a suite of functions to convert data structures among the various R packages we typically use. The R package adegenet is a popular R package used for population genetic analysis and it works on data structures called ‘genlight’ objects. Here we use the function vcfR2genlight() to convert our vcfR object to a genlight object. This makes our VCF data available to the analyses in adegenet.

x <- vcfR2genlight(vcf)
## Warning in vcfR2genlight(vcf): Found 44 loci with more than two alleles.
## Objects of class genlight only support loci with two alleles.
## 44 loci will be omitted from the genlight object.
## Loading required namespace: adegenet
## Loading required package: parallel
x
##  /// GENLIGHT OBJECT /////////
## 
##  // 18 genotypes,  2,146 binary SNPs, size: 222 Kb
##  0 (0 %) missing data
## 
##  // Basic content
##    @gen: list of 18 SNPbin
## 
##  // Optional content
##    @ind.names:  18 individual labels
##    @loc.names:  2146 locus labels
##    @chromosome: factor storing chromosomes of the SNPs
##    @position: integer storing positions of the SNPs
##    @other: a list containing: elements without names
A genlight object only supports biallelic, or binary, variants. That is, variants with no more than two alleles. However, variant call format data can include multiple alleles. When we created our genlight object we recieved a warning message indicating that our vcfR object had variants with more than two alleles and that it was being subset to only biallelic variants. This is one of several important differences in how data is handled in VCF data versus genlight objects.

Another important difference among VCF and genlight data is how the genotypes are stored. In VCF data the alleles are delimited by either a pipe or a forward slash (‘|’, ‘/’ respectively). Because genlight objects only use biallelic loci the genotypes can be recoded as 0, 1 and 2. These correspond to homozygous for the reference or zero allele, heterozygote or homozygous for the first alternate allele. We can validate this by checking a few select genotypes from both the vcfR object and the genlight object.

# vcfR
gt <- extract.gt(vcf, element = "GT")
gt[c(2,6,18), 1:3]
##                        BL2009P4_us23 DDR7602 IN2009T1_us22
## Supercontig_1.50_80063 "1|0"         "1|0"   "0|1"        
## Supercontig_1.50_80089 "0|0"         "1|0"   "0|1"        
## Supercontig_1.50_94108 "0|1"         "0|1"   "1|1"
# genlight
t(as.matrix(x))[c(1,5,17), 1:3]
##                        BL2009P4_us23 DDR7602 IN2009T1_us22
## Supercontig_1.50_80063             1       1             1
## Supercontig_1.50_80089             0       1             1
## Supercontig_1.50_94108             1       1             2
Note that in VCF data the samples are in columns and the variants are in rows. In genlight objects, and many other R objects, the samples are in rows while the variants are in columns. We can use the transpose function (t()) to convert between these two states.

Yet another difference among VCF data and genlight objects is that in VCF data there is no concept of ‘population.’ The package adegenet was designed specifically for the analysis of population data, so its genlight object has a place (a ‘slot’) to hold this information. Because there is no population data in VCF data, if we want population data we’ll have to set it ourselves.

library(adegenet)
## Loading required package: methods
## Loading required package: ade4
## 
##    /// adegenet 2.0.1 is loaded ////////////
## 
##    > overview: '?adegenet'
##    > tutorials/doc/questions: 'adegenetWeb()' 
##    > bug reports/feature requests: adegenetIssues()
pop(x) <- as.factor(c("us", "eu", "us", "af", "eu", "us", "mx", "eu", "eu", "sa", "mx", "sa", "us", "sa", "Pmir", "us", "eu", "eu"))
popNames(x)
## [1] "af"   "eu"   "mx"   "Pmir" "sa"   "us"
Our population designation consists of a vector, that is the same length as the number of samples we have, where each element indicates which population each sample belongs to. By using the as.factor() function we transform the “vector” into a “factor”. A factor understands that all of the elements that are named “us” or “eu” are all part of the same group. This is why when we ask for the popNames we get a vector where each population is represented only once.

Yet another difference among VCF data and genlight objects is the concept of ploidy. In VCF data each variant is treated independently. This means that in theory VCF data may contain data that is of mixed ploidy. In a genlight object different samples may be of different ploidy levels, but within each sample all of its loci must be of the same ploidy level. Here we’ll set the ploidy of all the samples in the genlight object to the same ploidy.

ploidy(x) <- 2
Distance matrices
Let’s create a pairwise genetic distance matrix for individuals or populations (i.e., groups of individuals).

Note: There isn’t actually a function that creates distance matrices from genlight objects in adegenet. Instead, the authors of adegenet created an as.matrix() function that converts a genlight object to a matrix. This is clever because the function dist() in the package stats tries to convert whatever object it is given into a matrix. The result is that when you call dist() on a genlight object it uses the dist() function to create a distance matrix. The reason this is clever is because it uses pre-existing code. The downside is that because there is no function to specifically create distance matrices from genlight objects in adegenet, there is no documentation in genlight for how this is done. And because the author of dist() never anticipated it could be used on genlight objects, there is no documentation for it there either.
To summarize, we can create a distance matrix from a genlight object using dist():

x.dist <- dist(x)
Note, that we have not specified what the variable x is. We can find documentation for this function with ?dist.

There are also functions to create distance matrices from genlight objects that exist in other packages. The function bitwise.dist() in the package poppr is an example. We can find documentation for this function with ?poppr::bitwise.dist. Again, you need to know where to look for this information or you may not find it. We can use this function as follows.

x.dist <- poppr::bitwise.dist(x)
Note, that the variable x has not yet been specified. Lastly, because you can use as.matrix() on your genlight object, and most distance algorithms can use this matrix as input, you can use this as an intermediate step to create a matrix from your genlight object and pass it to your distance algorithm of choice. Options include ade4, vegdist() in vegan, or daisy() in cluster. Note that it is up to you to determine which distance metric is best for your particular analysis. A number of options therefore exist for creating distance matrices from genlight objects.

chromR objects
Using chromR to locate unusual features in a genome
Genomic projects frequently incorporate several types of data. For example, the reference sequence may be stored as a FASTA format file, variants (SNPs, indels, etc.) may be stored in a variant call format (VCF) file while annotations may be stored as a GFF or BED format (tablular data). Genome browsers can be used to integrate these different data types. However, genome browsers typically lack a manipulation environment, they simply display existing files. The R environment includes a tremendous amount of statistical support that is both specific to genetics and genomics as well as more general tools (e.g., the linear model and its extensions). The R package vcfR provides a link between VCF data and the R environment and it includes a simple genome browser to help visualize the effect of manipulations. Here we explore how we can use vcfR to survey genomic data for interesting features.

Creating chromR objects
In this example we will begin by locating the example data from the pinfsc50 package. This is a separate package from vcfR that you will need to install. If you haven’t installed it already, you can install it with install.packages('pinfsc50'). For data from your own research activities you may wany to omit the system.file() steps and directly use your filenames in the input steps.

library(vcfR)

# Find the files.
vcf_file <- system.file("extdata", "pinf_sc50.vcf.gz", package = "pinfsc50")
dna_file <- system.file("extdata", "pinf_sc50.fasta", package = "pinfsc50")
gff_file <- system.file("extdata", "pinf_sc50.gff", package = "pinfsc50")

# Input the files.
vcf <- read.vcfR(vcf_file, verbose = FALSE)
dna <- ape::read.dna(dna_file, format = "fasta")
gff <- read.table(gff_file, sep="\t", quote="")

# Create a chromR object.
chrom <- create.chromR(name="Supercontig", vcf=vcf, seq=dna, ann=gff, verbose=TRUE)
## Names in vcf:
##   Supercontig_1.50
## Names of sequences:
##   Supercontig_1.50 of Phytophthora infestans T30-4
## Warning in create.chromR(name = "Supercontig", vcf = vcf, seq = dna, ann = gff, : 
##         Names in variant data and sequence data do not match perfectly.
##         If you choose to proceed, we'll do our best to match the data.
##         But prepare yourself for unexpected results.
## Names in annotation:
##   Supercontig_1.50
## Initializing var.info slot.
## var.info slot initialized.
Note that a warning message indicates that the names in all of the data sources do not match pefectly. It has been my experience that this is a frequent occurrence in genome projects. Instead of asking the user to create duplicate files that have the same data but standardized names, vcfR allows the user to exercise some judgement. If you see this message and feel the names are correct you can ignore this and proceed. In this case we see that a chromosome is named ‘Supercontig_1.50’ in the VCF data but named ‘Supercontig_1.50 of Phytophthora infestans T30-4’ in the FASTA (sequence) file. Because we know that for this specific project these are synonyms we can safely ignore the warning and proceed.

Once we have created our chromR object we can verify that its contents are what we expect. By executing the object’s name at the console, with no other arguments, we invoke the object’s ‘show’ method. The show method for chromR objects presents a summary of the object’s contents.

chrom
## *****   Class chromR, method Show   *****
## Name: Supercontig 
## Chromosome length: 1,042,442 bp
##   Chromosome labels: Supercontig_1.50 of Phytophthora infestans T30-4
## Annotation (@ann) count: 223 
##   Annotation chromosome names: Supercontig_1.50
## Variant (@vcf) count: 22,031 
##   Variant (@vcf) chromosome names: Supercontig_1.50
## Object size: 22.5 Mb
## Use head(object) for more details.
## *****      End Show (chromR)        *****
There at least two ways to graphically view the chromR object. The first is plot() which plots histograms of some of data summaries.

plot(chrom)


The read depth here is a sum over all samples. We see a peak that represents the depth where most of our genomes were sequenced at. Low regions of sequence depth may indicate variants where we may be concerned that there may not be enough information to call a genotype. Variants of high coverage may represent repetetive regions of genomes where the reference may not contain all the copies so the reads pile up on the fraction of repeats that were successfully assembled. These regions may violate the ploidy assumptions made by variant callers and therefore may be considered a target for quality filtering. Mapping quality is very peaked at 60 but also contains variants that deviate from this common value. Quality (QUAL) is less easily interpreted. It appears that most of our variants are of a low quality with very few of them being of high quality. It is important to remember that while everyone would like high quality, quality is frequently difficult to measure. The simplest interpretation here is that QUAL may not be a good parameter to use to judge your variants. The last panel for SNP densities is empty because this data is created during the processing of chromR objects, which we will discuss below.

chromoqc(chrom, dp.alpha = 66)


Our second plot, called chromo plot, displays the same information as the plot method only it distributes the data along its chomosomal coordinates. It also includes a representation of the annotation data. The contents of this plot are somewhat flexible in that it depends on what data is present in the chromR object.

Processing chromR objects
Creation and processing of a chromR object has been divided into separate tasks. Creation loads the data into the chromR object and should typically only be required once. Processing the chromR object generates summaries of the data. Some of these summaries will need to be updated as the chromR object is updated. For example, if the size of the sliding window used to summarize variant density and GC content is changed the chromR object will need to be processed to update this information.

chrom <- proc.chromR(chrom, verbose = TRUE)
## Nucleotide regions complete.
##   elapsed time:  0.538
## N regions complete.
##   elapsed time:  0.28
## Population summary complete.
##   elapsed time:  0.357
## window_init complete.
##   elapsed time:  0.001
## windowize_fasta complete.
##   elapsed time:  0.123
## windowize_annotations complete.
##   elapsed time:  0.014
## windowize_variants complete.
##   elapsed time:  0.001
plot(chrom)


Subsequent to processing, our plot function is identical to its previous presentation except that we now have variant densities. When we observe the chromoqc plot we see that we now have variant densities, nucleotide content as well as a representation of where in our reference we have nucleotides (A, C, G or T) or where we have ambiguous nucleotides.

chromoqc(chrom, dp.alpha = 66)


The above data is an example of visualizing raw data that has come from a variant caller and other automated sources. In our section on quality control we presented methods on how to filter variants on various parameters as an attempt to omit low quality variants. We can use this data to create a chromR object and compare it to the above data.

#vcf <- read.vcfR("pinfsc50_qc.vcf.gz", verbose = FALSE)
vcf <- read.vcfR("pinfsc50_filtered.vcf.gz", verbose = FALSE)
chrom <- create.chromR(name="Supercontig", vcf=vcf, seq=dna, ann=gff, verbose=FALSE)
chrom <- proc.chromR(chrom, verbose = FALSE)
chromoqc(chrom, dp.alpha = 66)


We have a smaller quantity of data after our quality control steps. However, there do appear to be a few improvements. First, the read depth is now fairly uniform and lacks the large variation in depth we saw in the raw data. In genomics projects our naive assumption is that we would sequence all regions of the genome at the same depth. So this change in the data allows it to approach our expectation. Second, the mapping quality appear relatively constant and the variants with low mapping quality have been omitted. If we feel that ‘mapping quality’ is a reasonable assessment of quality, we may interpret this as an improvement. These are methods we feel improve the quality of our datasets prior to analysis.

Tabular summaries
When we process a chromR object, two forms of tabular data are created. First, summaries are made on a per variant basis. This includes sample size (minus missing data), allele counts, heterozygosity and effective size. Second, summaries are made on a per window basis. Window size can be changed with the win.size parameter of the function proc.chromR(). Window based summaries include nucleotide content per window (including missing data so you can adjust window size for analyses if necessary), the number of genic sites per window (when annotation information was provided) and the number of variants per window.

head(chrom@var.info)
##              CHROM   POS    MQ  DP mask  n Allele_counts         He
## 1 Supercontig_1.50 80058 58.96 508 TRUE 18       25,10,1 0.64364712
## 2 Supercontig_1.50 80063 58.95 514 TRUE 18         25,11 0.42438272
## 3 Supercontig_1.50 80067 58.88 499 TRUE 18         23,13 0.46141975
## 4 Supercontig_1.50 80073 58.77 490 TRUE 18          35,1 0.05401235
## 5 Supercontig_1.50 80074 58.75 482 TRUE 18         26,10 0.40123457
## 6 Supercontig_1.50 80089 58.80 481 TRUE 18         25,11 0.42438272
##         Ne
## 1 2.806207
## 2 1.737265
## 3 1.856734
## 4 1.057096
## 5 1.670103
## 6 1.737265
head(chrom@win.info)
##              CHROM window start  end length   A   C   G   T    N other
## 1 Supercontig_1.50      1     1 1000   1000 267 213 293 227    0     0
## 2 Supercontig_1.50      2  1001 2000   1000 283 206 309 202    0     0
## 3 Supercontig_1.50      3  2001 3000   1000 229 213 235 177  146     0
## 4 Supercontig_1.50      4  3001 4000   1000   0   0   0   0 1000     0
## 5 Supercontig_1.50      5  4001 5000   1000   0   0   0   0 1000     0
## 6 Supercontig_1.50      6  5001 6000   1000   0   0   0   0 1000     0
##   genic variants
## 1     0        0
## 2     0        0
## 3     0        0
## 4     0        0
## 5     0        0
## 6     0        0
While loading entire genomes into memory may not be practical due to resource limitations, it is frequently practical to break a genome up into fractions that can be processed given the resources available on any system. By processing a genome by chromosomes, or some other fraction, and saving this tabular data to file you can perform genome scans in an attempt to identify interesting features.

Genetic differentiation
A fundamental question to most population studies is whether populations are diverse and whether this diversity is shared among the populations? To address the question of within population diversity geneticists typically report heterozygosity. This is the probability that two alleles randomly chosen from a population will be different (Nei, 1973). Ecologists may know this as Simpson’s Index (Simpson, 1949). To address differentiation population geneticists typically utilize FSTFST or one of its analogues. Population differentiation measured by FSTFST was originally proposed by Sewall Wright (Wright, 1949). This was later extended to a method based on diversity by Masatoshi Nei (Nei, 1973). As researchers applied these metrics to microsatellites, genetic markers with a large number of alleles, it became clear that Nei’s measure would not correctly range from zero to one, so Philip Hedrick proposed a correction (Hedrick, 2005). More recently, Lou Jost proposed another alternative (Jost, 2008). You can tell a topic is popular when so many variants of it are generated. And there are more variants than mentioned here. A nice discussion as to which measure may be appropriate for your data was posteed to teh Molecular Ecologist blog titled should I use FSTFST, GSTGST or DD?.

In vcfR, the function genetic_diff() was implemented to measure population diversity and differentiation. Because VCF data typically do not include population information we’ll have to supply it as a factor. The method ‘nei’ employed here is based on the methods reported by Hedrick (Hedrick, 2005). The exception is that the heterozygosities are weighted by the number of alleles observed in each population. This was inspired by  hierfstat::pairwise.fst() which uses the number of individuals observed in each population to weight the heterozygosities. By using the number of alleles observed instead of the number of individuals we remove an assumption about how many alleles each individual may contribute. That is, we should be able to accomodate samples of mixed ploidy.

library(vcfR)
data(vcfR_example)
pop <- as.factor(c("us", "eu", "us", "af", "eu", "us", "mx", "eu", "eu", "sa", "mx", "sa", "us", "sa", "Pmir", "us", "eu", "eu"))
myDiff <- genetic_diff(vcf, pops = pop, method = 'nei')
knitr::kable(head(myDiff[,1:15]))
CHROM	POS	Hs_af	Hs_eu	Hs_mx	Hs_Pmir	Hs_sa	Hs_us	Ht	n_af	n_eu	n_mx	n_Pmir	n_sa	n_us
Supercontig_1.50	2	0	0.0	0.000	0.5	0.000	0.00	0.0798611	2	4	4	2	4	8
Supercontig_1.50	246	NaN	0.0	0.375	NaN	0.000	0.50	0.3512397	0	4	4	0	6	8
Supercontig_1.50	549	NaN	0.0	NaN	NaN	NaN	0.50	0.4444444	0	2	0	0	0	4
Supercontig_1.50	668	NaN	0.5	0.000	NaN	0.000	0.50	0.5000000	0	4	2	0	2	8
Supercontig_1.50	765	0	0.0	0.000	0.0	0.000	0.00	0.1107266	2	12	4	2	4	10
Supercontig_1.50	780	0	0.0	0.000	0.0	0.375	0.18	0.1244444	2	8	4	2	4	10
The function returns the chromosome and position of each variant as provided in the VCF data. This should allow you to align its output with the VCF data. The heterozygosities for each population are reported as well as the total heterozygosity, followed by the number of alleles observed in each population. Note that in some populations zero alleles were observed. Populations with zero alleles reported heterozygosities of ‘NaN’ because of this absence of data.

knitr::kable(head(myDiff[,16:19]))
Gst	Htmax	Gstmax	Gprimest
0.4782609	0.7951389	0.9475983	0.5047085
NaN	0.8057851	NaN	NaN
NaN	0.6666667	NaN	NaN
NaN	0.8125000	NaN	NaN
1.0000000	0.7543253	1.0000000	1.0000000
0.1160714	0.8000000	0.8625000	0.1345756
The remaining columns contain GSTGST, the maximum heterozygosity, the maximum GSTGST and finally G′STGST′. The maximum heterozygosity and the maximum GSTGST are intermediary values used to calculate G′STGST′. They are typically not reported but provide values to help validate that G′STGST′ was calculated correctly. Note that the populations that had zero alleles, and therefore a heterozygosity of ‘NaN’, contributed to GSTGSTs that were also ‘NaN’. To avoid this you may want to consider omitting populations with a small sample size or that contain a large amount of missing data.

We now have information for each variant in the VCF data. Because this is typically a large quantity of information, we’ll want to summarize it. One way is to take averages of the data.

knitr::kable(round(colMeans(myDiff[,c(3:9,16,19)], na.rm = TRUE), digits = 3))
Hs_af	0.176
Hs_eu	0.188
Hs_mx	0.168
Hs_Pmir	0.052
Hs_sa	0.198
Hs_us	0.155
Ht	0.247
Gst	0.595
Gprimest	0.632
Another way to summarize data is to use violin plots.

library(reshape2)
library(ggplot2)

dpf <- melt(myDiff[,c(3:8,19)], varnames=c('Index', 'Sample'), value.name = 'Depth', na.rm=TRUE)
## No id variables; using all as measure variables
p <- ggplot(dpf, aes(x=variable, y=Depth)) + geom_violin(fill="#2ca25f", adjust = 1.2)
p <- p + xlab("")
p <- p + ylab("")
p <- p + theme_bw()
p


References
Hedrick PW. 2005. A standardized genetic differentiation measure. Evolution 59:1633–1638. Available at: http://dx.doi.org/10.1111/j.0014-3820.2005.tb01814.x

Jombart T. 2008. adegenetadegenet: A R package for the multivariate analysis of genetic markers. Bioinformatics 24:1403–1405. Available at: https://doi.org/10.1093/bioinformatics/btn129

Jost L. 2008. GSTGST And its relatives do not measure differentiation. Molecular Ecology 17:4015–4026. Available at: http://dx.doi.org/10.1111/j.1365-294X.2008.03887.x

Kamvar ZN., Brooks JC., Grünwald NJ. 2015. Novel R tools for analysis of genome-wide population genetic data with emphasis on clonality. Name: Frontiers in Genetics 6:208. Available at: http://dx.doi.org/10.3389/fgene.2015.00208

Kamvar ZN., Tabima JF., Grünwald NJ. 2014. PopprPoppr: An R package for genetic analysis of populations with clonal, partially clonal, and/or sexual reproduction. PeerJ 2:e281. Available at: http://dx.doi.org/10.7717/peerj.281

Knaus BJ., Grünwald NJ. 2017. VcfrVcfr: A package to manipulate and visualize variant call format data in R. Molecular Ecology Resources 17:44–53. Available at: http://dx.doi.org/10.1111/1755-0998.12549

Nei M. 1973. Analysis of gene diversity in subdivided populations. Proceedings of the National Academy of Sciences 70:3321–3323. Available at: http://www.pnas.org/content/70/12/3321.abstract

Simpson EH. 1949. Measurement of diversity. Nature 163:688. Available at: http://dx.doi.org/10.1038/163688a0

Wright S. 1949. The genetical structure of populations. Annals of Eugenics 15:323–354. Available at: http://dx.doi.org/10.1111/j.1469-1809.1949.tb02451.x

Population genetics and genomics in R
TABLE OF CONTENTS
PART I 
PART II 
PART III 
WORKSHOP 
ABOUT 
APPENDICES 
GBS analysis
JF Tabima, BJ Knaus, and NJ Grünwald
Introduction
Opening and examining the vcf file
Converting the dataset to a genlight object
Population genetic analyses for GBS data
Subsetting a vcfR object to 200 random variants
Conclusion
References
Introduction
This tutorial focuses on large SNP data sets such as those obtained from genotyping-by-sequencing (GBS) for population genetic analysis in R. GBS is one of several techniques used to genotype populations using high throughput sequencing (HTS). In GBS, the genome is reduced in representation by using restriction enzymes, and then sequencing these products using HTS. For more information on these techniques we recommend reading Baird et al. (2008), Rowe, Renaut & Guggisberg (2011), or Poland et al. (2012).

We will use a data set of 94 samples of the red raspberry pathogen Phytophthora rubi (Tabima et al., In Prep). This pathogen is diploid and a fungal like Oomycete. Populations were obtained by sampling individual pathogen strains from roots of infected red raspberry in the states of California (CA), Oregon (OR), and Washington (WA). A total of 94 samples of P. rubi were sequenced using the Illumina HiSeq 3000 technology with 150 bp paired end reads and a target insert size of 500 bp. Currently, there is little information about the population structure of P. rubi in the western USA. We are interested in studying the population structure of P. rubi populations in the western US. The VCF data for this population can be downloaded from: prubi_gbs.VCF.gz.

To obtain variant calls in form of VCF data, the FASTQ reads from HTS were mapped to the reference genome of P. rubi (Tabima et al., in prep) using bowtie2 (Langmead & Salzberg, 2012). Variants were called using the GATK HaplotypeCaller (McKenna et al., 2010). This data was further filtered in vcfR using read depths (DP) and mapping qualities (MQ). Data was filtered as follows:

A minimum DP of 5x.
Variants in the top 5% of the DP distribution were removed.
Only variants with a MQ greater than 40 were retained.
Variants with more than 60% missing data were removed.
In addition to the VCF data, we have included the file population_data.gbs.txt, a tab-delimited text file that includes the name of the sample, country of origin, and the population from where it was sampled. The file is available for download at: population_data.gbs.txt. This link will likely open the data in a browser. Save the data onto your hard-drive as an ASCII text file with the same name.

Opening and examining the vcf file
Let’s first load the libraries needed for analysis:

library(vcfR)
library(poppr)
library(ape)
library(RColorBrewer)
Make sure you are in the right folder with the downloaded files available. Some of these packages will print a message when they are loaded. Here we suppressed this information. When you load these packages, you may see more output than presented here.

Next, let’s open the VCF file using vcfR and check that we have 94 samples and 615 SNPs:

rubi.VCF <- read.vcfR("prubi_gbs.vcf.gz")
Once we’ve loaded the data into R, we can validate it by entering our object’s name in the console:

rubi.VCF
## ***** Object of Class vcfR *****
## 94 samples
## 321 CHROMs
## 615 variants
## Object size: 2.9 Mb
## 4.589 percent missing data
## *****        *****         *****
VCF data does not typically include any sort of population information. We have to load this data separately from the text-delimited file we downloaded above that includes the ID of samples and the state where the samples were obtained from. Let’s load the  population_data.gbs.txt file into memory by using the read.table() function:

pop.data <- read.table("population_data.gbs.txt", sep = "\t", header = TRUE)
We can now check that all the samples in the VCF and the population data frame are included:

colnames(rubi.VCF@gt)[-1] == pop.data$AccessID
##  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
## [15] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
## [29] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
## [43] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
## [57] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
## [71] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
## [85] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
Remember, the first column for the vcfR object’s GT slot is a FORMAT column, hence we removed it from the name comparison step (i.e., by removing the first element in the column names vector, the FORMAT column name). If we keep the FORMAT column name we will not be able to compare between the names of the samples in the vcfR object. All of the 94 samples in the population data frame loaded in the vcfR object are in the same order.

Converting the dataset to a genlight object
The next step is to convert the data set into an object that is usable by poppr, adegenet, or any of the other population genetics packages in R. The vcfR package contains multiple functions to convert data into other formats (see the converting_data vignette of vcfR):  vignette('converting_data'). For our particular purpose we want to convert the vcfR object into a genlight object. We can use the  vcfR2genlight function for this:

gl.rubi <- vcfR2genlight(rubi.VCF)
## Warning in vcfR2genlight(rubi.VCF): Found 7 loci with more than two alleles.
## Objects of class genlight only support loci with two alleles.
## 7 loci will be omitted from the genlight object.
A warning is shown while transforming the object, telling us that there are seven loci with more than two alleles. Many of the functions we will be using in these tutorials have been written under the assumption of a bi-allelic model. This bi-allelic model restricts all loci to only two alleles in order to simplify some calculations. The genlight object supports only loci with no more than two alleles. The vcfR2genlight function subsets the data to filter loci that are not bi-allelic, returning an object that contains only loci with two alleles. The warning is to make sure we are aware that this action has taken place.

Additionally, we are required to specify the ploidy of the organism in order to calculate some population genetic metrics. P. rubi is a diploid organism, so we will specify a ploidy of two. All genlight objects have ploidy slots, in which the user can specify the ploidy of each individual sample, or once for the entire population. We can assume that every sample of P. rubi is diploid and we will specify a ploidy of 2 for the entire population. Note that while a genlight object can support individuals of different ploidy, within each individual all loci must be of the same ploidy.

ploidy(gl.rubi) <- 2
Our biological question requires predetermined populations. We can add them to the genlight object as part of the pop (population) slot. In order to specify the population, we added the State column from our pop.data data frame to the pop slot of our genlight object:

pop(gl.rubi) <- pop.data$State
We now end up with a genlight object of filtered VCF data:

gl.rubi
##  /// GENLIGHT OBJECT /////////
## 
##  // 94 genotypes,  608 binary SNPs, size: 241.9 Kb
##  2607 (4.56 %) missing data
## 
##  // Basic content
##    @gen: list of 94 SNPbin
##    @ploidy: ploidy of each individual  (range: 2-2)
## 
##  // Optional content
##    @ind.names:  94 individual labels
##    @loc.names:  608 locus labels
##    @chromosome: factor storing chromosomes of the SNPs
##    @position: integer storing positions of the SNPs
##    @pop: population of each individual (group size range: 23-39)
##    @other: a list containing: elements without names
Next, let’s get started with our first analyses.

Population genetic analyses for GBS data
Distance matrices
Let’s create a pairwise genetic distance matrix for individuals or populations (i.e., groups of individuals).

Note: There isn’t actually a function that creates distance matrices from genlight objects in adegenet. Instead, the authors of adegenet created an as.matrix() function that converts a genlight object to a matrix. This is clever because the function dist() in the package stats tries to convert whatever object it is given into a matrix. The result is that when you call dist() on a genlight object it uses the dist() function to create a distance matrix. The reason this is clever is because it uses pre-existing code. The downside is that because there is no function to specifically create distance matrices from genlight objects in adegenet, there is no documentation in genlight for how this is done. And because the author of dist() never anticipated it could be used on genlight objects, there is no documentation for it there either.
To summarize, we can create a distance matrix from a genlight object using dist():

x.dist <- dist(x)
Note, that we have not specified what the variable x is. We can find documentation for this function with ?dist.

There are also functions to create distance matrices from genlight objects that exist in other packages. The function bitwise.dist() in the package poppr is an example. We can find documentation for this function with ?poppr::bitwise.dist. Again, you need to know where to look for this information or you may not find it. We can use this function as follows.

x.dist <- poppr::bitwise.dist(x)
Note, that the variable x has not yet been specified. Lastly, because you can use as.matrix() on your genlight object, and most distance algorithms can use this matrix as input, you can use this as an intermediate step to create a matrix from your genlight object and pass it to your distance algorithm of choice. Options include ade4, vegdist() in vegan, or daisy() in cluster. Note that it is up to you to determine which distance metric is best for your particular analysis. A number of options therefore exist for creating distance matrices from genlight objects.

Distance tree
Let’s start our analysis by building a genetic distance tree that represents the genetic relatedness of the samples. The similarity between samples and groups of samples is represented by the branch length. In most trees, the branch length is represented by the number of substitutions per site for a cluster or a sample. When samples are very similar, they are grouped by short branches. The longer the branch, the higher the number of substitutions and the higher the genetic distance is between samples or clusters.

For this tutorial, we will build a distance tree to obtain an initial assessment of the population structure of the P. rubi samples in the western US. We will reconstruct a distance tree based on the UPGMA algorithm, with 100 bootstrap replicates to assess branch support:

tree <- aboot(gl.rubi, tree = "upgma", distance = bitwise.dist, sample = 100, showtree = F, cutoff = 50, quiet = T)
Next, we will color the tips of the tree based on the population of origin of the samples, and draw conclusions from what we observe in the tree:

cols <- brewer.pal(n = nPop(gl.rubi), name = "Dark2")
plot.phylo(tree, cex = 0.8, font = 2, adj = 0, tip.color =  cols[pop(gl.rubi)])
nodelabels(tree$node.label, adj = c(1.3, -0.5), frame = "n", cex = 0.8,font = 3, xpd = TRUE)
#legend(35,10,c("CA","OR","WA"),cols, border = FALSE, bty = "n")
legend('topleft', legend = c("CA","OR","WA"), fill = cols, border = FALSE, bty = "n", cex = 2)
axis(side = 1)
title(xlab = "Genetic distance (proportion of loci that are different)")


We observe that samples do not cluster exclusively by region (e.g., CA, OR and WA). Instead, we observe a cluster with mainly CA samples (Green), but also containing a few WA (purple) and two OR (red) samples. The second, lower cluster contains predominantly samples for OR (red) and WA (purple).

Minimum spanning networks
Another useful independent analysis to visualize population structure is a minimum spanning network (MSN). MSN clusters multilocus genotypes (MLG) by genetic distances between them. Each MLG is a node, and the genetic distance is represented by the edges. In high throughput sequencing studies, where marker density is high, each sample typically consists of a unique genotype.

QUESTION: Do we show a reticulated MSN? OR a MST?
The nodes will be connected by the minimum distance between samples. This allows for reticulations (e.g., a network connecting nodes with identical genetic distances; contrast this with a tree where pairwise nodes are only connected to one other node with the shortest distances, even if several nodes share the same minimum genetic distance).

To reconstruct a MSN we require a genlight object and a distance matrix that represents the genetic distance between samples. To calculate this distance we will use the bitwise.dist() function from poppr. The bitwise.dist() function is a fast method to calculate the number of allelic differences between two samples:

library(igraph)

rubi.dist <- bitwise.dist(gl.rubi)
rubi.msn <- poppr.msn(gl.rubi, rubi.dist, showplot = FALSE, include.ties = T)

node.size <- rep(2, times = nInd(gl.rubi))
names(node.size) <- indNames(gl.rubi)
vertex.attributes(rubi.msn$graph)$size <- node.size

set.seed(9)
plot_poppr_msn(gl.rubi, rubi.msn , palette = brewer.pal(n = nPop(gl.rubi), name = "Dark2"), gadj = 70)


Note that we have used the command set.seed(9) to set a random number seed. Because there is some stochasticity in the presentation of a MSN, this is required to reproduce the exact same plot. In practice, you should explore using different seeds to obtain different representations of the same data.

The plot is similar to the tree we rendered above and shows that the populations are not fully differentiated geographically. There are two main groups in the plot, one that clusters the OR samples with some WA samples (upper left), and a second that clusters the CA samples with some WA samples (lower right). This provides additional evidence for the two clusters observed in the genetic distance tree. Next, we perform a principal components analysis.

Principal components analysis
A principal components analysis (PCA) converts the observed SNP data into a set of values of linearly uncorrelated variables called  principal components that summarize the variation between samples. We can perform a PCA on our genlight object by using the glPCA function.

rubi.pca <- glPca(gl.rubi, nf = 3)
barplot(rubi.pca$eig, col = heat.colors(50), main="PCA Eigenvalues")
title(ylab="Proportion of variance explained")
title(xlab="Eigenvalue")


The barplot indicates that we will need to only retain the first 3 PCAs, which cumulatively explain explain 63.183 percent of the variance of the data.

To view the results of the PCA we can use the package ggplot2. We need to convert the data frame that contains the principal components (rubi.pca$scores) into the new object rubi.pca.scores. In addition, we will add the population values as a new column in our  rubi.pca.scores object, in order to be able to color samples by population.

ggplot2 will plot the PCA, color the samples by population, and create ellipses that include 95% of the data for each the population:

rubi.pca.scores <- as.data.frame(rubi.pca$scores)
rubi.pca.scores$pop <- pop(gl.rubi)

library(ggplot2)
set.seed(9)
p <- ggplot(rubi.pca.scores, aes(x=PC1, y=PC2, colour=pop)) 
p <- p + geom_point(size=2)
p <- p + stat_ellipse(level = 0.95, size = 1)
p <- p + scale_color_manual(values = cols) 
p <- p + geom_hline(yintercept = 0) 
p <- p + geom_vline(xintercept = 0) 
p <- p + theme_bw()

p


The PCA produces a pattern similar our previous results. We observe that PC1 distinguishes samples from WA and OR, (right) from a cluster predominantly made up of CA and WA samples (left). The CA samples form a tight cluster with a narrow ellipse in green.

We can further explore population assignments using a discriminant analysis of principal components (DAPC).

DAPC
The DAPC is a multivariate statistical approach that uses populations defined a priori to maximize the variance among populations in the sample by partitioning it into between-population and within-population components. DAPC thus maximizes the discrimination between groups. DAPC is explained in depth in the DAPC chapter on Part II of this tutorial and in the DAPC adegenet vignette.

DAPC requires a genlight object with populations defined a priori. We already have this genlight object from the above steps. Usually, we use the number of principal components and discriminant axes that maximize the variance between populations; but our objective here is to calculate the population assignments based on the results of the PCA. We will use the same parameters as in the PCA to make the results comparable between both methods. These parameters (n.pca=3 and n.da=2) will be used to reconstruct the DAPC, obtain the assignment of the samples to each population, and suggest admixture between geographical states in the western USA.

pnw.dapc <- dapc(gl.rubi, n.pca = 3, n.da = 2)
To confirm that the DAPC is similar to the PCA we can plot the data in a scatter plot.

scatter(pnw.dapc, col = cols, cex = 2, legend = TRUE, clabel = F, posi.leg = "bottomleft", scree.pca = TRUE,
        posi.pca = "topleft", cleg = 0.75)


We see that the results of the PCA and DAPC are very similar. The DAPC object we created includes the population membership probability for each sample to each of the predetermined populations. To visualize the posterior assignments of each sample, we use a composite stacked bar plot (compoplot). A compoplot illustrates the probability of population membership on the y-axis. Each sample is a bin on the x-axis, and the assigned probability of population membership is shown as a stacked bar chart with clusters or populations shown in color.

#compoplot(pnw.dapc,col = function(x) cols, posi = 'top')
compoplot(pnw.dapc,col = cols, posi = 'top')


These plots are hard to interpret and we will thus separate the samples by population.
ggplot2 can be used to reconstruct these plots, but we need to convert the data into a ggplot2 friendly object. We will extract the DAPC calculated population membership assignments (pnw.dapc$posterior) into a new data frame (dapc.results), include the original population assignment as a new column in the data frame (dapc.results$pop), and add a column that includes the sample names (dapc.results$indNames).

dapc.results <- as.data.frame(pnw.dapc$posterior)
dapc.results$pop <- pop(gl.rubi)
dapc.results$indNames <- rownames(dapc.results)
ggplot2 has specific requirements for the structure of the data frame format, as it requires each observation in rows, and all different values of these observations in columns (i.e., a long format data frame). To transform the data frame we use the function melt from the package  reshape2. melt reorganizes the data frame into the required data frame format, where each membership probability observation for a given population is a row with the sample name, original population, and assigned population as columns.

library(reshape2)
dapc.results <- melt(dapc.results)
Ignore the prompt for now. Then, we rename the columns into more familiar terms:

colnames(dapc.results) <- c("Original_Pop","Sample","Assigned_Pop","Posterior_membership_probability")
ggplot2 will plot the dapc.results data frame we reorganized using melt, using the samples on the X-axis and membership probabilities on the Y-axis. The fill color will indicate the original population assignments. Each facet represents the original population assignment for each sample:

p <- ggplot(dapc.results, aes(x=Sample, y=Posterior_membership_probability, fill=Assigned_Pop))
p <- p + geom_bar(stat='identity') 
p <- p + scale_fill_manual(values = cols) 
p <- p + facet_grid(~Original_Pop, scales = "free")
p <- p + theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8))
p


This bar plot shows us a more organized perspective of our data set by contrasting the population membership probability assignments against their original populations.

In general, the composite plot shows that all western states of the USA exhibit admixture with other states, with WA showing the most admixture.

Subsetting a vcfR object to 200 random variants
One of the main limitations encountered when performing a population genetic analysis using large SNP data sets is the size of the data set. Large datasets require large amounts of computing resources, presenting a challenge to individuals who are not familiar with managing their computing resources. These large data sets are becoming very common thanks to the current developments on genomics and computational biology, where instead on performing analyses on one or a few loci current analyses focus in entire genomes of large populations.

GBS data sets are good examples of large data sets used in population genetics. The large number of samples, and the high number of variants obtained, can generate gigabytes of data that are difficult to analyse with a desktop computer. Many of the variants obtained from GBS analyses are removed via filtering (for a deeper explanation of filtering genomic variant data review the quality control chapter of this workshop). Nonetheless, these filtered data sets may have hundreds to thousands of variants, making their analysis very tedious for the average user.

One way of solving the issues of dealing with large genomic data sets is to subset the data set to smaller groups of random variants across the genome. Multiple subsets of a data set can be analysed in order to determine if the different subsets reveal similar overall results. These independent analyses will provide support to the results, while reducing the effective computational time of the analysis.

We will illustrate the process of subsetting datasets using the P. rubi GBS data. As a reminder, this dataset has 138 samples spanning 615 variants across the genome. The number of variants in this analysis does not reflect a real large data set, but we will use it as it will allow us to compare the results of the subsets against the total analysis. This comparison will demonstrate that subsets of large data sets can provide information that is similar to the total data set while reducing the computational intensity of the calculations.

In this example we will show how to subset the data to 200 random variants across the genome. We will use the rubi.VCF object we created at the beginning of this tutorial. To subset, we use square brackets to select portions of an object we would like to retain. The square brackets represent the values to be subset from the original data set, for example: if we have a vector with the four first letters of the alphabet (“A”,“B”,“C”,“D”), and we want to subset only the first two occurrences we can do this:

alphabet <- c("A","B","C","D")
alphabet[c(1,2)]
## [1] "A" "B"
We see that when we use the alphabet[c(1,2)] we are calling the first two occurrences of the vector of interest. We can do something very similar with the vcfR objects from the vcfR package. Lets say we are interested only in the first 10 variants of our rubi.VCF, we can do something similar:

rubi.VCF
## ***** Object of Class vcfR *****
## 94 samples
## 321 CHROMs
## 615 variants
## Object size: 2.9 Mb
## 4.589 percent missing data
## *****        *****         *****
rubi.VCF[c(1:10),]
## ***** Object of Class vcfR *****
## 94 samples
## 4 CHROMs
## 10 variants
## Object size: 0.9 Mb
## 3.83 percent missing data
## *****        *****         *****
We can see that the second object only has 10 variants. If we look at the summary in detail, we observe that the original sample has 312 chromosomes/scaffolds, 615 variants and a size of 2.9 Mb. The subset object, on the other hand, only has 4 chromosomes/scaffolds, 10 variants and a size of 0.9 Mb. Both objects have the same number of samples. This is a good example of how to subset a vcfR object. However, we want to obtain a subset of 200 random variants. If we do rubi.VCF[c(1:200)] we will only obtain the first 200 variants. These 200 variants will always be the same, and we will have no real replication to determine the strength of our approach.

One way to solve this problem is to generate a random string of 200 numbers that range from 1 to the total number of variants of the object. The R statistical environment has the function sample(). The function sample() generates a random vector of elements (i.e. numbers or elements from a vector). We can use sample() to generate the random vector of 200 numbers that range from 1 to the total number of variants of the object. The total number of variants in a vcfR object is equivalent to the number of rows in the vcfR object. We will use the nrow() method for our vcfR object to obtain the maximum range of values to subset our data set:

subset.1 <- sample(size = 200, x= c(1:nrow(rubi.VCF)))
subset.2 <- sample(size = 200, x= c(1:nrow(rubi.VCF)))
We have to check that the subsets are not identical. We can confirm this by using the function identical().

identical(subset.1, subset.2)
## [1] FALSE
We can see that the vectors have different numbers. The results from identical() do not mean that every number in the vector is different. It just means that some of the values in the vectors are different, but each vector can also share some numbers with the other vector.

Now we know how to generate a vector of 200 random numbers from 1 to the length of the vcfR object. We will use these vectors to subset the  rubi.VCF object. To subset the vcfR object we execute code very similar to the one shown in the subset example. The main difference is that now we can use the subset.1 and subset.2 objects as the subset indices:

rubi.VCF.sub1 <- rubi.VCF[subset.1,]
rubi.VCF.sub2 <- rubi.VCF[subset.2,]

rubi.VCF.sub1
## ***** Object of Class vcfR *****
## 94 samples
## 146 CHROMs
## 200 variants
## Object size: 1.7 Mb
## 4.463 percent missing data
## *****        *****         *****
rubi.VCF.sub2
## ***** Object of Class vcfR *****
## 94 samples
## 150 CHROMs
## 200 variants
## Object size: 1.7 Mb
## 4.473 percent missing data
## *****        *****         *****
When the two rubi.VCF.sub objects are compared we can see that the number of variants (200) and the number of samples (94) are identical. This indicates that the subset was performed correctly. Observe that the chromosome number and percentage of missing data are different. These different numbers show that each subset has selected different variants.

Now we can generate subsets of random variants. We will extend the subset example by creating 50 subsets of 200 variants each, then we will then reconstruct a UPGMA distance tree per subset and overlay all of the reconstructed trees. This tree overlay will allow us to determine if the subsets support our hypothesis of low population differentiation, or if we have unique tree clusters for different subsets. You, the user, don’t have to execute this example, as it is for illustrative purposes only.

# Creating a list object to save our subsets in.
rubi.variant.subset <- list()

# Using a for loop to generate 50 subsets of 200 random variants from the rubi.VCF vcfR object.
for (i in 1:50){
  rubi.variant.subset[[i]] <- rubi.VCF[sample(size = 200, x= c(1:nrow(rubi.VCF)))]
}

# Checking we have 50 vcfR objects:
length(rubi.variant.subset)
## [1] 50
head(rubi.variant.subset, n=2)
## [[1]]
## ***** Object of Class vcfR *****
## 94 samples
## 143 CHROMs
## 200 variants
## Object size: 1.7 Mb
## 4.819 percent missing data
## *****        *****         *****
## 
## [[2]]
## ***** Object of Class vcfR *****
## 94 samples
## 146 CHROMs
## 200 variants
## Object size: 1.7 Mb
## 4.505 percent missing data
## *****        *****         *****
# Creating the GenLight object
rubi.gl.subset <- lapply(rubi.variant.subset, function (x) suppressWarnings(vcfR2genlight(x)))
for (i in 1:length(rubi.gl.subset)){
  ploidy(rubi.gl.subset[[i]]) <- 2
}

# Creating a simple UPGMA tree per object
library(phangorn)
rubi.trees <- lapply(rubi.gl.subset, function (x) upgma(bitwise.dist(x)))
class(rubi.trees) <- "multiPhylo"

# Overlapping the trees
densiTree(rubi.trees, consensus = tree, scaleX = T, show.tip.label = F, alpha = 0.1)
title(xlab = "Proportion of variants different")


We can observe that the results between both subsets are very similar: Clustering between samples from WA and OR, clustering between some samples of CA and WA, and a small number of samples of CA and OR clustering. These results reflect similar patterns to what we have observed with the full data set, indicating that our subset strategy is a suitable approach to analyse our P. rubi GBS data set

Since the DensiTree function does not currently allow us to color the tip labels we will have to color them by hand in a graphics software outside of R. This is the final result:

Alt text: Densitree
Alt text: Densitree

The densiTree reconstruction shows a very similar overall topology to the initial consensus tree: We see 6/7 major clades, and each of the clades is comprised of samples from different populations, indicating low geographic population structure. Furthermore, we observe how multiple subsets can result in similar patterns to the overall data. Subsetting large genomic variants is an easy, time-efficient and computationally smart method.

Conclusion
We were able to determine that the western states of P. rubi do not have population structure based on geographical locations. The results showed that the CA population was less diverse and had a lower degree of admixture than either the WA or OR populations. We can conclude that the geographical locations do not reflect the population structure of samples of P. rubi in the western USA, where the results suggested that different states shared genotypes across the entire geographic gradient resulting in one, panmictic and admixed population of P. rubi.

References
Baird NA., Etter PD., Atwood TS., Currey MC., Shiver AL., Lewis ZA., Selker EU., Cresko WA., Johnson EA. 2008. Rapid SNP discovery and genetic mapping using sequenced RAD markers. PLoS ONE 3:e3376. Available at: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0003376

Langmead B., Salzberg SL. 2012. Fast gapped-read alignment with bowtie 2. Nature methods 9:357–359. Available at: http://dx.doi.org/10.1038/nmeth.1923

McKenna A., Hanna M., Banks E., Sivachenko A., Cibulskis K., Kernytsky A., Garimella K., Altshuler D., Gabriel S., Daly M. et al. 2010. The genome analysis toolkit: A mapReduce framework for analyzing next-generation dNA sequencing data. Genome research 20:1297–1303. Available at: https://dx.doi.org/10.1101%2Fgr.107524.110

Poland J., Endelman J., Dawson J., Rutkoski J., Wu S., Manes Y., Dreisigacker S., Crossa J., Sánchez-Villeda H., Sorrells M. et al. 2012. Genomic selection in wheat breeding using genotyping-by-sequencing. The Plant Genome 5:103–113. Available at: http://dl.sciencesocieties.org/publications/tpg/abstracts/5/3/103

Rowe HC., Renaut S., Guggisberg A. 2011. RAD in the realm of next-generation sequencing technologies. Molecular Ecology 20:3499–3502. Available at: http://www.ncbi.nlm.nih.gov/pubmed/21991593

Tabima JF., Zasada I., Coffey M., Grünwald NJ. In Prep. Population dynamics of Phytopthora rubi indicate high rates of migration between states and nurseries in the pacific north western United States. In prep.


## Evolution of Genetic Systems

## Phylogenetics

## Game Theory

## Genetic Algorithms

## Evolutionary Algorithms

## Hardy-Weinberg

## Linkage Disequilibrium

